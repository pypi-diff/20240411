# Comparing `tmp/bisheng_pyautogen-0.2.1-py3-none-any.whl.zip` & `tmp/bisheng_pyautogen-0.3.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,52 +1,36 @@
-Zip file size: 149452 bytes, number of entries: 50
--rw-r--r--  2.0 unx      234 b- defN 24-Jan-29 10:49 autogen/__init__.py
--rw-r--r--  2.0 unx     3191 b- defN 24-Jan-29 10:49 autogen/_pydantic.py
--rw-r--r--  2.0 unx     2045 b- defN 24-Jan-29 10:49 autogen/agent_utils.py
--rw-r--r--  2.0 unx    11329 b- defN 24-Jan-29 10:49 autogen/browser_utils.py
--rw-r--r--  2.0 unx    24054 b- defN 24-Jan-29 10:49 autogen/code_utils.py
--rw-r--r--  2.0 unx    12053 b- defN 24-Jan-29 10:49 autogen/function_utils.py
--rw-r--r--  2.0 unx    10051 b- defN 24-Jan-29 10:49 autogen/math_utils.py
--rw-r--r--  2.0 unx    16478 b- defN 24-Jan-29 10:49 autogen/retrieve_utils.py
--rw-r--r--  2.0 unx     7041 b- defN 24-Jan-29 10:49 autogen/token_count_utils.py
--rw-r--r--  2.0 unx       22 b- defN 24-Mar-07 07:55 autogen/version.py
--rw-r--r--  2.0 unx      350 b- defN 24-Jan-29 10:49 autogen/agentchat/__init__.py
--rw-r--r--  2.0 unx     2922 b- defN 24-Jan-29 10:49 autogen/agentchat/agent.py
--rw-r--r--  2.0 unx     4840 b- defN 24-Jan-29 10:49 autogen/agentchat/assistant_agent.py
--rw-r--r--  2.0 unx    68410 b- defN 24-Jan-29 10:49 autogen/agentchat/conversable_agent.py
--rw-r--r--  2.0 unx    12074 b- defN 24-Jan-29 10:49 autogen/agentchat/groupchat.py
--rw-r--r--  2.0 unx     5546 b- defN 24-Jan-29 10:49 autogen/agentchat/user_proxy_agent.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/__init__.py
--rw-r--r--  2.0 unx    32658 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/agent_builder.py
--rw-r--r--  2.0 unx    23884 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/compressible_agent.py
--rw-r--r--  2.0 unx    19861 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/gpt_assistant_agent.py
--rw-r--r--  2.0 unx     5778 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/img_utils.py
--rw-r--r--  2.0 unx     6247 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/llava_agent.py
--rw-r--r--  2.0 unx    19162 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/math_user_proxy_agent.py
--rw-r--r--  2.0 unx     3487 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/multimodal_conversable_agent.py
--rw-r--r--  2.0 unx    15881 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/qdrant_retrieve_user_proxy_agent.py
--rw-r--r--  2.0 unx     1994 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/retrieve_assistant_agent.py
--rw-r--r--  2.0 unx    23552 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/retrieve_user_proxy_agent.py
--rw-r--r--  2.0 unx    21183 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/teachable_agent.py
--rw-r--r--  2.0 unx     4530 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/text_analyzer_agent.py
--rw-r--r--  2.0 unx    16171 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/web_surfer.py
--rw-r--r--  2.0 unx      133 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/capabilities/__init__.py
--rw-r--r--  2.0 unx      545 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/capabilities/agent_capability.py
--rw-r--r--  2.0 unx    18432 b- defN 24-Jan-29 10:49 autogen/agentchat/contrib/capabilities/teachability.py
--rw-r--r--  2.0 unx       46 b- defN 24-Jan-29 10:49 autogen/cache/__init__.py
--rw-r--r--  2.0 unx     2839 b- defN 24-Jan-29 10:49 autogen/cache/abstract_cache_base.py
--rw-r--r--  2.0 unx     4882 b- defN 24-Jan-29 10:49 autogen/cache/cache.py
--rw-r--r--  2.0 unx     1594 b- defN 24-Jan-29 10:49 autogen/cache/cache_factory.py
--rw-r--r--  2.0 unx     2704 b- defN 24-Jan-29 10:49 autogen/cache/disk_cache.py
--rw-r--r--  2.0 unx     3485 b- defN 24-Jan-29 10:49 autogen/cache/redis_cache.py
--rw-r--r--  2.0 unx        0 b- defN 24-Jan-29 10:49 autogen/extensions/__init__.py
--rw-r--r--  2.0 unx      496 b- defN 24-Jan-29 10:49 autogen/oai/__init__.py
--rw-r--r--  2.0 unx    31834 b- defN 24-Jan-29 10:49 autogen/oai/client.py
--rw-r--r--  2.0 unx    52398 b- defN 24-Jan-29 10:49 autogen/oai/completion.py
--rw-r--r--  2.0 unx    15584 b- defN 24-Jan-29 10:49 autogen/oai/openai_utils.py
--rw-r--r--  2.0 unx    18650 b- defN 24-Mar-22 01:47 bisheng_pyautogen-0.2.1.dist-info/LICENSE
--rw-r--r--  2.0 unx     1141 b- defN 24-Mar-22 01:47 bisheng_pyautogen-0.2.1.dist-info/LICENSE-CODE
--rw-r--r--  2.0 unx    13750 b- defN 24-Mar-22 01:47 bisheng_pyautogen-0.2.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-22 01:47 bisheng_pyautogen-0.2.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        8 b- defN 24-Mar-22 01:47 bisheng_pyautogen-0.2.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     4585 b- defN 24-Mar-22 01:47 bisheng_pyautogen-0.2.1.dist-info/RECORD
-50 files, 548226 bytes uncompressed, 142054 bytes compressed:  74.1%
+Zip file size: 97075 bytes, number of entries: 34
+-rw-r--r--  2.0 unx      234 b- defN 24-Apr-11 08:30 autogen/__init__.py
+-rw-r--r--  2.0 unx    24054 b- defN 24-Apr-11 08:30 autogen/code_utils.py
+-rw-r--r--  2.0 unx    10051 b- defN 24-Apr-11 08:30 autogen/math_utils.py
+-rw-r--r--  2.0 unx    16478 b- defN 24-Apr-11 08:30 autogen/retrieve_utils.py
+-rw-r--r--  2.0 unx       22 b- defN 24-Apr-11 08:33 autogen/version.py
+-rw-r--r--  2.0 unx      350 b- defN 24-Apr-11 08:30 autogen/agentchat/__init__.py
+-rw-r--r--  2.0 unx     2922 b- defN 24-Apr-11 08:30 autogen/agentchat/agent.py
+-rw-r--r--  2.0 unx     4810 b- defN 24-Apr-11 08:30 autogen/agentchat/assistant_agent.py
+-rw-r--r--  2.0 unx    68862 b- defN 24-Apr-11 08:30 autogen/agentchat/conversable_agent.py
+-rw-r--r--  2.0 unx    12074 b- defN 24-Apr-11 08:30 autogen/agentchat/groupchat.py
+-rw-r--r--  2.0 unx     5516 b- defN 24-Apr-11 08:30 autogen/agentchat/user_proxy_agent.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-11 08:30 autogen/agentchat/contrib/__init__.py
+-rw-r--r--  2.0 unx    19162 b- defN 24-Apr-11 08:30 autogen/agentchat/contrib/math_user_proxy_agent.py
+-rw-r--r--  2.0 unx    15881 b- defN 24-Apr-11 08:30 autogen/agentchat/contrib/qdrant_retrieve_user_proxy_agent.py
+-rw-r--r--  2.0 unx     1994 b- defN 24-Apr-11 08:30 autogen/agentchat/contrib/retrieve_assistant_agent.py
+-rw-r--r--  2.0 unx    23552 b- defN 24-Apr-11 08:30 autogen/agentchat/contrib/retrieve_user_proxy_agent.py
+-rw-r--r--  2.0 unx    21183 b- defN 24-Apr-11 08:30 autogen/agentchat/contrib/teachable_agent.py
+-rw-r--r--  2.0 unx     4530 b- defN 24-Apr-11 08:30 autogen/agentchat/contrib/text_analyzer_agent.py
+-rw-r--r--  2.0 unx       46 b- defN 24-Apr-11 08:30 autogen/cache/__init__.py
+-rw-r--r--  2.0 unx     2839 b- defN 24-Apr-11 08:30 autogen/cache/abstract_cache_base.py
+-rw-r--r--  2.0 unx     4882 b- defN 24-Apr-11 08:30 autogen/cache/cache.py
+-rw-r--r--  2.0 unx     1594 b- defN 24-Apr-11 08:30 autogen/cache/cache_factory.py
+-rw-r--r--  2.0 unx     2704 b- defN 24-Apr-11 08:30 autogen/cache/disk_cache.py
+-rw-r--r--  2.0 unx     3485 b- defN 24-Apr-11 08:30 autogen/cache/redis_cache.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-11 08:30 autogen/extensions/__init__.py
+-rw-r--r--  2.0 unx      496 b- defN 24-Apr-11 08:30 autogen/oai/__init__.py
+-rw-r--r--  2.0 unx    52262 b- defN 24-Apr-11 08:30 autogen/oai/completion.py
+-rw-r--r--  2.0 unx    19776 b- defN 24-Apr-11 08:30 autogen/oai/openai_utils.py
+-rw-r--r--  2.0 unx    18650 b- defN 24-Apr-11 08:33 bisheng_pyautogen-0.3.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1141 b- defN 24-Apr-11 08:33 bisheng_pyautogen-0.3.0.dist-info/LICENSE-CODE
+-rw-r--r--  2.0 unx    13750 b- defN 24-Apr-11 08:33 bisheng_pyautogen-0.3.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-11 08:33 bisheng_pyautogen-0.3.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 24-Apr-11 08:33 bisheng_pyautogen-0.3.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3054 b- defN 24-Apr-11 08:33 bisheng_pyautogen-0.3.0.dist-info/RECORD
+34 files, 356454 bytes uncompressed, 92119 bytes compressed:  74.2%
```

## zipnote {}

```diff
@@ -1,34 +1,19 @@
 Filename: autogen/__init__.py
 Comment: 
 
-Filename: autogen/_pydantic.py
-Comment: 
-
-Filename: autogen/agent_utils.py
-Comment: 
-
-Filename: autogen/browser_utils.py
-Comment: 
-
 Filename: autogen/code_utils.py
 Comment: 
 
-Filename: autogen/function_utils.py
-Comment: 
-
 Filename: autogen/math_utils.py
 Comment: 
 
 Filename: autogen/retrieve_utils.py
 Comment: 
 
-Filename: autogen/token_count_utils.py
-Comment: 
-
 Filename: autogen/version.py
 Comment: 
 
 Filename: autogen/agentchat/__init__.py
 Comment: 
 
 Filename: autogen/agentchat/agent.py
@@ -45,35 +30,17 @@
 
 Filename: autogen/agentchat/user_proxy_agent.py
 Comment: 
 
 Filename: autogen/agentchat/contrib/__init__.py
 Comment: 
 
-Filename: autogen/agentchat/contrib/agent_builder.py
-Comment: 
-
-Filename: autogen/agentchat/contrib/compressible_agent.py
-Comment: 
-
-Filename: autogen/agentchat/contrib/gpt_assistant_agent.py
-Comment: 
-
-Filename: autogen/agentchat/contrib/img_utils.py
-Comment: 
-
-Filename: autogen/agentchat/contrib/llava_agent.py
-Comment: 
-
 Filename: autogen/agentchat/contrib/math_user_proxy_agent.py
 Comment: 
 
-Filename: autogen/agentchat/contrib/multimodal_conversable_agent.py
-Comment: 
-
 Filename: autogen/agentchat/contrib/qdrant_retrieve_user_proxy_agent.py
 Comment: 
 
 Filename: autogen/agentchat/contrib/retrieve_assistant_agent.py
 Comment: 
 
 Filename: autogen/agentchat/contrib/retrieve_user_proxy_agent.py
@@ -81,26 +48,14 @@
 
 Filename: autogen/agentchat/contrib/teachable_agent.py
 Comment: 
 
 Filename: autogen/agentchat/contrib/text_analyzer_agent.py
 Comment: 
 
-Filename: autogen/agentchat/contrib/web_surfer.py
-Comment: 
-
-Filename: autogen/agentchat/contrib/capabilities/__init__.py
-Comment: 
-
-Filename: autogen/agentchat/contrib/capabilities/agent_capability.py
-Comment: 
-
-Filename: autogen/agentchat/contrib/capabilities/teachability.py
-Comment: 
-
 Filename: autogen/cache/__init__.py
 Comment: 
 
 Filename: autogen/cache/abstract_cache_base.py
 Comment: 
 
 Filename: autogen/cache/cache.py
@@ -117,35 +72,32 @@
 
 Filename: autogen/extensions/__init__.py
 Comment: 
 
 Filename: autogen/oai/__init__.py
 Comment: 
 
-Filename: autogen/oai/client.py
-Comment: 
-
 Filename: autogen/oai/completion.py
 Comment: 
 
 Filename: autogen/oai/openai_utils.py
 Comment: 
 
-Filename: bisheng_pyautogen-0.2.1.dist-info/LICENSE
+Filename: bisheng_pyautogen-0.3.0.dist-info/LICENSE
 Comment: 
 
-Filename: bisheng_pyautogen-0.2.1.dist-info/LICENSE-CODE
+Filename: bisheng_pyautogen-0.3.0.dist-info/LICENSE-CODE
 Comment: 
 
-Filename: bisheng_pyautogen-0.2.1.dist-info/METADATA
+Filename: bisheng_pyautogen-0.3.0.dist-info/METADATA
 Comment: 
 
-Filename: bisheng_pyautogen-0.2.1.dist-info/WHEEL
+Filename: bisheng_pyautogen-0.3.0.dist-info/WHEEL
 Comment: 
 
-Filename: bisheng_pyautogen-0.2.1.dist-info/top_level.txt
+Filename: bisheng_pyautogen-0.3.0.dist-info/top_level.txt
 Comment: 
 
-Filename: bisheng_pyautogen-0.2.1.dist-info/RECORD
+Filename: bisheng_pyautogen-0.3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## autogen/version.py

```diff
@@ -1 +1 @@
-__version__ = "0.2.1"
+__version__ = "0.3.0"
```

## autogen/agentchat/assistant_agent.py

```diff
@@ -28,16 +28,15 @@
 Reply "TERMINATE" in the end when everything is done.
     """
 
     def __init__(
         self,
         name: str,
         system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,
-        llm_config: Optional[Union[Dict, bool]] = None,
-        llm: Optional[BaseLanguageModel] = None,
+        llm_config: Optional[Union[Dict, bool, BaseLanguageModel]] = None,
         is_termination_msg: Optional[Callable[[Dict], bool]] = None,
         max_consecutive_auto_reply: Optional[int] = None,
         human_input_mode: Optional[str] = "NEVER",
         code_execution_config: Optional[Union[Dict, bool]] = False,
         **kwargs,
     ):
         """
```

## autogen/agentchat/conversable_agent.py

```diff
@@ -55,16 +55,15 @@
         name: str,
         system_message: Optional[str] = "You are a helpful AI Assistant.",
         is_termination_msg: Optional[Callable[[Dict], bool]] = None,
         max_consecutive_auto_reply: Optional[int] = None,
         human_input_mode: Optional[str] = "TERMINATE",
         function_map: Optional[Dict[str, Callable]] = None,
         code_execution_config: Optional[Union[Dict, bool]] = None,
-        llm_config: Optional[Union[Dict, bool]] = None,
-        llm: Optional[BaseLanguageModel] = None,
+        llm_config: Optional[Union[Dict, bool, BaseLanguageModel]] = None,
         default_auto_reply: Optional[Union[str, Dict, None]] = "",
     ):
         """
         Args:
             name (str): name of the agent.
             system_message (str): system message for the ChatCompletion inference.
             is_termination_msg (function): a function that takes a message in the form of a dictionary
@@ -109,17 +108,21 @@
         self._oai_messages = defaultdict(list)
         self._oai_system_message = [{"content": system_message, "role": "system"}]
         self._is_termination_msg = (is_termination_msg if is_termination_msg is not None else
                                     (lambda x: x.get("content") == "TERMINATE"))
         if llm_config is False:
             self.llm_config = False
         else:
-            self.llm_config = self.DEFAULT_CONFIG.copy()
             if isinstance(llm_config, dict):
+                self.llm_config = self.DEFAULT_CONFIG.copy()
                 self.llm_config.update(llm_config)
+                self.llm = None
+            else:
+                self.llm_config = {}
+                self.llm = llm_config
 
         self._code_execution_config = {} if code_execution_config is None else code_execution_config
         self.human_input_mode = human_input_mode
         self._max_consecutive_auto_reply = (max_consecutive_auto_reply
                                             if max_consecutive_auto_reply is not None else
                                             self.MAX_CONSECUTIVE_AUTO_REPLY)
         self._consecutive_auto_reply_counter = defaultdict(int)
@@ -735,14 +738,19 @@
         config: Optional[Any] = None,
         **kwargs,
     ) -> Tuple[bool, Union[str, Dict, None]]:
         """Generate a reply using autogen.oai."""
         llm_config = self.llm_config if config is None else config
         if llm_config is False:
             return False, None
+
+        run_manager = kwargs.get("run_manager")
+        llm_config['run_manager'] = run_manager
+        llm_config['llm'] = self.llm  # 直接使用第三方模型
+
         if messages is None:
             messages = self._oai_messages[sender]
 
         # TODO: #1143 handle token limit exceeded error
         response = oai.ChatCompletion.create(context=messages[-1].pop("context", None),
                                              messages=self._oai_system_message + messages,
                                              **llm_config)
@@ -755,21 +763,26 @@
         config: Optional[Any] = None,
         **kwargs,
     ) -> Tuple[bool, Union[str, Dict, None]]:
         """Generate a reply using autogen.oai."""
         llm_config = self.llm_config if config is None else config
         if llm_config is False:
             return False, None
+
+        llm_config['llm'] = self.llm  # 直接使用第三方模型
+        run_manager = kwargs.get("run_manager")
+        llm_config['run_manager'] = run_manager
+
         if messages is None:
             messages = self._oai_messages[sender]
 
         # TODO: #1143 handle token limit exceeded error
-        response = oai.ChatCompletion.create(context=messages[-1].pop("context", None),
-                                             messages=self._oai_system_message + messages,
-                                             **llm_config)
+        response = await oai.ChatCompletion.acreate(context=messages[-1].pop("context", None),
+                                                    messages=self._oai_system_message + messages,
+                                                    **llm_config)
         return True, oai.ChatCompletion.extract_text_or_function_call(response)[0]
 
     def generate_code_execution_reply(
         self,
         messages: Optional[List[Dict]] = None,
         sender: Optional[Agent] = None,
         config: Optional[Any] = None,
```

## autogen/agentchat/user_proxy_agent.py

```diff
@@ -21,16 +21,15 @@
         name: str,
         is_termination_msg: Optional[Callable[[Dict], bool]] = None,
         max_consecutive_auto_reply: Optional[int] = None,
         human_input_mode: Optional[str] = "ALWAYS",
         function_map: Optional[Dict[str, Callable]] = None,
         code_execution_config: Optional[Union[Dict, bool]] = None,
         default_auto_reply: Optional[Union[str, Dict, None]] = "",
-        llm_config: Optional[Union[Dict, bool]] = False,
-        llm: Optional[BaseLanguageModel] = None,
+        llm_config: Optional[Union[Dict, bool, BaseLanguageModel]] = False,
         system_message: Optional[str] = "",
     ):
         """
         Args:
             name (str): name of the agent.
             is_termination_msg (function): a function that takes a message in the form of a dictionary
                 and returns a boolean value indicating if this received message is a termination message.
```

## autogen/oai/completion.py

```diff
@@ -1,32 +1,32 @@
 from time import sleep
 import logging
-import time
 from typing import List, Optional, Dict, Callable, Union
 import sys
 import shutil
 import numpy as np
 from flaml import tune, BlendSearch
 from flaml.tune.space import is_constant
 from flaml.automl.logger import logger_formatter
-from .openai_utils import get_key
+from autogen.oai.openai_utils import create_chat_result, get_key
 from collections import defaultdict
+from langchain_core.language_models import BaseLanguageModel
+from langchain_core.callbacks import AsyncCallbackManagerForLLMRun
 
+from langchain_core.runnables import RunnableConfig
 try:
     import openai
-    from openai.error import (
-        ServiceUnavailableError,
+    from openai import (
         RateLimitError,
         APIError,
-        InvalidRequestError,
         APIConnectionError,
         Timeout,
         AuthenticationError,
     )
-    from openai import Completion as openai_Completion
+    from openai import OpenAI as openai_Completion
     import diskcache
 
     ERROR = None
 except ImportError:
     ERROR = ImportError("please install openai and diskcache to use the autogen.oai subpackage.")
     openai_Completion = object
 logger = logging.getLogger(__name__)
@@ -83,44 +83,47 @@
         "gpt-4-0314": (0.03, 0.06),  # deprecate in Sep
         "gpt-4-32k-0314": (0.06, 0.12),  # deprecate in Sep
         "gpt-4-0613": (0.03, 0.06),
         "gpt-4-32k-0613": (0.06, 0.12),
     }
 
     default_search_space = {
-        "model": tune.choice(
-            [
-                "text-ada-001",
-                "text-babbage-001",
-                "text-davinci-003",
-                "gpt-3.5-turbo",
-                "gpt-4",
-            ]
-        ),
-        "temperature_or_top_p": tune.choice(
-            [
-                {"temperature": tune.uniform(0, 2)},
-                {"top_p": tune.uniform(0, 1)},
-            ]
-        ),
-        "max_tokens": tune.lograndint(50, 1000),
-        "n": tune.randint(1, 100),
-        "prompt": "{prompt}",
+        "model":
+        tune.choice([
+            "text-ada-001",
+            "text-babbage-001",
+            "text-davinci-003",
+            "gpt-3.5-turbo",
+            "gpt-4",
+        ]),
+        "temperature_or_top_p":
+        tune.choice([
+            {
+                "temperature": tune.uniform(0, 2)
+            },
+            {
+                "top_p": tune.uniform(0, 1)
+            },
+        ]),
+        "max_tokens":
+        tune.lograndint(50, 1000),
+        "n":
+        tune.randint(1, 100),
+        "prompt":
+        "{prompt}",
     }
 
     seed = 41
     cache_path = f".cache/{seed}"
     # retry after this many seconds
     retry_wait_time = 10
     # fail a request after hitting RateLimitError for this many seconds
     max_retry_period = 120
     # time out for request to openai server
     request_timeout = 60
-
-    openai_completion_class = not ERROR and openai.Completion
     _total_cost = 0
     optimization_budget = None
 
     _history_dict = _count_create = None
 
     @classmethod
     def set_cache(cls, seed: Optional[int] = 41, cache_path_root: Optional[str] = ".cache"):
@@ -167,131 +170,138 @@
             if "messages" in config:
                 messages = config["messages"]
                 if len(messages) > 1 and messages[-1]["role"] != "assistant":
                     existing_key = get_key(messages[:-1])
                     value = cls._history_dict.pop(existing_key, value)
                 key = get_key(messages + [choice["message"] for choice in response["choices"]])
             else:
-                key = get_key([config["prompt"]] + [choice.get("text") for choice in response["choices"]])
+                key = get_key([config["prompt"]] +
+                              [choice.get("text") for choice in response["choices"]])
             value["created_at"].append(cls._count_create)
             value["cost"].append(response["cost"])
-            value["token_count"].append(
-                {
-                    "model": response["model"],
-                    "prompt_tokens": response["usage"]["prompt_tokens"],
-                    "completion_tokens": response["usage"].get("completion_tokens", 0),
-                    "total_tokens": response["usage"]["total_tokens"],
-                }
-            )
+            value["token_count"].append({
+                "model":
+                response["model"],
+                "prompt_tokens":
+                response["usage"]["prompt_tokens"],
+                "completion_tokens":
+                response["usage"].get("completion_tokens", 0),
+                "total_tokens":
+                response["usage"]["total_tokens"],
+            })
             cls._history_dict[key] = value
             cls._count_create += 1
             return
         cls._history_dict[cls._count_create] = {
             "request": config,
             "response": response.to_dict_recursive(),
         }
         cls._count_create += 1
 
     @classmethod
-    def _get_response(cls, config: Dict, raise_on_ratelimit_or_timeout=False, use_cache=True):
+    def _get_response(cls,
+                      messages: Dict,
+                      llm_config: Dict,
+                      raise_on_ratelimit_or_timeout=False,
+                      use_cache=True):
         """Get the response from the openai api call.
 
         Try cache first. If not found, call the openai api. If the api call fails, retry after retry_wait_time.
         """
-        config = config.copy()
-        openai.api_key_path = config.pop("api_key_path", openai.api_key_path)
+        config = messages.copy()
         key = get_key(config)
         if use_cache:
             response = cls._cache.get(key, None)
             if response is not None and (response != -1 or not raise_on_ratelimit_or_timeout):
                 # print("using cached response")
                 cls._book_keeping(config, response)
                 return response
-        openai_completion = (
-            openai.ChatCompletion
-            if config["model"].replace("gpt-35-turbo", "gpt-3.5-turbo") in cls.chat_models
-            or issubclass(cls, ChatCompletion)
-            else openai.Completion
-        )
-        start_time = time.time()
+
+        client_params = {
+            "api_key": llm_config['api_key'],
+            "base_url": llm_config.get("api_base") or "https://api.openai.com/v1",
+            "http_client": llm_config.get("http_client")
+        }
+
+        openai_completion = openai.OpenAI(**client_params).chat.completions
         request_timeout = cls.request_timeout
         max_retry_period = config.pop("max_retry_period", cls.max_retry_period)
         retry_wait_time = config.pop("retry_wait_time", cls.retry_wait_time)
-        while True:
-            try:
-                if "request_timeout" in config:
-                    response = openai_completion.create(**config)
-                else:
-                    response = openai_completion.create(request_timeout=request_timeout, **config)
-            except (
-                ServiceUnavailableError,
-                APIConnectionError,
-            ):
-                # transient error
-                logger.info(f"retrying in {retry_wait_time} seconds...", exc_info=1)
-                sleep(retry_wait_time)
-            except APIError as err:
-                error_code = err and err.json_body and isinstance(err.json_body, dict) and err.json_body.get("error")
-                error_code = error_code and error_code.get("code")
-                if error_code == "content_filter":
-                    raise
-                # transient error
-                logger.info(f"retrying in {retry_wait_time} seconds...", exc_info=1)
-                sleep(retry_wait_time)
-            except (RateLimitError, Timeout) as err:
-                time_left = max_retry_period - (time.time() - start_time + retry_wait_time)
-                if (
-                    time_left > 0
-                    and isinstance(err, RateLimitError)
-                    or time_left > request_timeout
-                    and isinstance(err, Timeout)
-                    and "request_timeout" not in config
-                ):
-                    if isinstance(err, Timeout):
-                        request_timeout <<= 1
-                    request_timeout = min(request_timeout, time_left)
-                    logger.info(f"retrying in {retry_wait_time} seconds...", exc_info=1)
-                    sleep(retry_wait_time)
-                elif raise_on_ratelimit_or_timeout:
-                    raise
-                else:
-                    response = -1
-                    if use_cache and isinstance(err, Timeout):
-                        cls._cache.set(key, response)
-                    logger.warning(
-                        f"Failed to get response from openai api due to getting RateLimitError or Timeout for {max_retry_period} seconds."
-                    )
-                    return response
-            except InvalidRequestError:
-                if "azure" in config.get("api_type", openai.api_type) and "model" in config:
-                    # azure api uses "engine" instead of "model"
-                    config["engine"] = config.pop("model").replace("gpt-3.5-turbo", "gpt-35-turbo")
-                else:
-                    raise
+
+        try:
+            config['stream'] = False
+            if "request_timeout" in config:
+                config['timeout'] = config.pop("request_timeout")
+                response = openai_completion.create(**config)
             else:
-                if use_cache:
-                    cls._cache.set(key, response)
-                cls._book_keeping(config, response)
-                return response
+                response = openai_completion.create(timeout=request_timeout, **config)
+            response = create_chat_result(response)
+        except (APIConnectionError, ):
+            # transient error
+            logger.info(f"retrying in {retry_wait_time} seconds...", exc_info=1)
+            sleep(retry_wait_time)
+        except APIError as err:
+            error_code = err and err.body and isinstance(err.body, dict) and err.body.get("error")
+            error_code = error_code and error_code.get("code")
+            if error_code == "content_filter":
+                raise
+            # transient error
+            logger.info(f"retrying in {retry_wait_time} seconds...", exc_info=1)
+            sleep(retry_wait_time)
+        except (Exception) as err:
+            logger.error(f"exception err={str(err)}")
+            sleep(retry_wait_time)
+        else:
+            if use_cache:
+                cls._cache.set(key, response)
+            cls._book_keeping(config, response)
+            return response
+        logger.warning(f"Failed to get response from openai api due to Exception")
+        return -1
+
+    @classmethod
+    async def _aget_resonse_llm(cls,
+                                params: Dict,
+                                llm: BaseLanguageModel,
+                                run_manage: Optional[AsyncCallbackManagerForLLMRun] = None):
+        callbacks = run_manage if run_manage else None
+        content = params.get("messages")
+        config = RunnableConfig(callbacks=callbacks, tags=['autogen'], run_name="autogen")
+        response = await llm.ainvoke(content, config=config)
+        await run_manage.on_text(text="", sender="autogen")
+        return create_chat_result(response)
+
+    @classmethod
+    def _get_resonse_llm(cls,
+                         params: Dict,
+                         llm: BaseLanguageModel,
+                         run_manage: Optional[AsyncCallbackManagerForLLMRun] = None):
+        callbacks = None
+        content = params.get("messages")
+        config = RunnableConfig(callbacks=callbacks, tags=['autogen'], run_name="autogen")
+        response = llm.invoke(content, config=config)
+        return create_chat_result(response)
 
     @classmethod
     def _get_max_valid_n(cls, key, max_tokens):
         # find the max value in max_valid_n_per_max_tokens
         # whose key is equal or larger than max_tokens
         return max(
-            (value for k, value in cls._max_valid_n_per_max_tokens.get(key, {}).items() if k >= max_tokens),
+            (value for k, value in cls._max_valid_n_per_max_tokens.get(key, {}).items()
+             if k >= max_tokens),
             default=1,
         )
 
     @classmethod
     def _get_min_invalid_n(cls, key, max_tokens):
         # find the min value in min_invalid_n_per_max_tokens
         # whose key is equal or smaller than max_tokens
         return min(
-            (value for k, value in cls._min_invalid_n_per_max_tokens.get(key, {}).items() if k <= max_tokens),
+            (value for k, value in cls._min_invalid_n_per_max_tokens.get(key, {}).items()
+             if k <= max_tokens),
             default=None,
         )
 
     @classmethod
     def _get_region_key(cls, config):
         # get a key for the valid/invalid region corresponding to the given config
         config = cls._pop_subspace(config, always_copy=False)
@@ -301,17 +311,16 @@
             config.get("stop"),
         )
 
     @classmethod
     def _update_invalid_n(cls, prune, region_key, max_tokens, num_completions):
         if prune:
             # update invalid n and prune this config
-            cls._min_invalid_n_per_max_tokens[region_key] = invalid_n = cls._min_invalid_n_per_max_tokens.get(
-                region_key, {}
-            )
+            cls._min_invalid_n_per_max_tokens[
+                region_key] = invalid_n = cls._min_invalid_n_per_max_tokens.get(region_key, {})
             invalid_n[max_tokens] = min(num_completions, invalid_n.get(max_tokens, np.inf))
 
     @classmethod
     def _pop_subspace(cls, config, always_copy=True):
         if "subspace" in config:
             config = config.copy()
             config.update(config.pop("subspace"))
@@ -356,25 +365,26 @@
         price = cls.price1K.get(model)
         price_input, price_output = price if isinstance(price, tuple) else (price, price)
         inference_budget = getattr(cls, "inference_budget", None)
         prune_hp = getattr(cls, "_prune_hp", "n")
         metric = cls._metric
         config_n = params.get(prune_hp, 1)  # default value in OpenAI is 1
         max_tokens = params.get(
-            "max_tokens", np.inf if model in cls.chat_models or issubclass(cls, ChatCompletion) else 16
-        )
+            "max_tokens",
+            np.inf if model in cls.chat_models or issubclass(cls, ChatCompletion) else 16)
         target_output_tokens = None
         if not cls.avg_input_tokens:
             input_tokens = [None] * data_length
         prune = prune and inference_budget and not eval_only
         if prune:
             region_key = cls._get_region_key(config)
             max_valid_n = cls._get_max_valid_n(region_key, max_tokens)
             if cls.avg_input_tokens:
-                target_output_tokens = (inference_budget * 1000 - cls.avg_input_tokens * price_input) / price_output
+                target_output_tokens = (inference_budget * 1000 -
+                                        cls.avg_input_tokens * price_input) / price_output
                 # max_tokens bounds the maximum tokens
                 # so using it we can calculate a valid n according to the avg # input tokens
                 max_valid_n = max(
                     max_valid_n,
                     int(target_output_tokens // max_tokens),
                 )
             if config_n <= max_valid_n:
@@ -400,15 +410,17 @@
             prev_data_limit = 0
             data_early_stop = False  # whether data early stop happens for this n
             while True:  # data_limit <= data_length
                 # limit the number of data points to avoid rate limit
                 for i in range(prev_data_limit, data_limit):
                     logger.debug(f"num_completions={num_completions}, data instance={i}")
                     data_i = data[i]
-                    response = cls.create(data_i, raise_on_ratelimit_or_timeout=eval_only, **params)
+                    response = cls.create(data_i,
+                                          raise_on_ratelimit_or_timeout=eval_only,
+                                          **params)
                     if response == -1:  # rate limit/timeout error, treat as invalid
                         cls._update_invalid_n(prune, region_key, max_tokens, num_completions)
                         result[metric] = 0
                         result["cost"] = cost
                         return result
                     # evaluate the quality of the responses
                     responses = cls.extract_text_or_function_call(response)
@@ -433,37 +445,34 @@
                         responses_list[i].extend(responses)
                         # Assumption 1: assuming requesting n1, n2 responses separatively then combining them
                         # is the same as requesting (n1+n2) responses together
                     else:
                         n_tokens_list.append(n_output_tokens)
                         responses_list.append(responses)
                 avg_n_tokens = np.mean(n_tokens_list[:data_limit])
-                rho = (
-                    (1 - data_limit / data_length) * (1 + 1 / data_limit)
-                    if data_limit << 1 > data_length
-                    else (1 - (data_limit - 1) / data_length)
-                )
+                rho = ((1 - data_limit / data_length) *
+                       (1 + 1 / data_limit) if data_limit << 1 > data_length else
+                       (1 - (data_limit - 1) / data_length))
                 # Hoeffding-Serfling bound
                 ratio = 0.1 * np.sqrt(rho / data_limit)
-                if target_output_tokens and avg_n_tokens > target_output_tokens * (1 + ratio) and not eval_only:
+                if target_output_tokens and avg_n_tokens > target_output_tokens * (
+                        1 + ratio) and not eval_only:
                     cls._update_invalid_n(prune, region_key, max_tokens, num_completions)
                     result[metric] = 0
                     result["total_cost"] = cls._total_cost
                     result["cost"] = cost
                     return result
-                if (
-                    prune
-                    and target_output_tokens
-                    and avg_n_tokens <= target_output_tokens * (1 - ratio)
-                    and (num_completions < config_n or num_completions == config_n and data_limit == data_length)
-                ):
+                if (prune and target_output_tokens
+                        and avg_n_tokens <= target_output_tokens * (1 - ratio)
+                        and (num_completions < config_n
+                             or num_completions == config_n and data_limit == data_length)):
                     # update valid n
-                    cls._max_valid_n_per_max_tokens[region_key] = valid_n = cls._max_valid_n_per_max_tokens.get(
-                        region_key, {}
-                    )
+                    cls._max_valid_n_per_max_tokens[
+                        region_key] = valid_n = cls._max_valid_n_per_max_tokens.get(
+                            region_key, {})
                     valid_n[max_tokens] = max(num_completions, valid_n.get(max_tokens, 0))
                     if num_completions < config_n:
                         # valid already, skip the rest of the data
                         data_limit = data_length
                         data_early_stop = True
                         break
                 prev_data_limit = data_limit
@@ -487,18 +496,18 @@
                     if isinstance(result[key], (float, int)):
                         result[key] /= data_limit
                 result["total_cost"] = cls._total_cost
                 result["cost"] = cost
                 if not cls.avg_input_tokens:
                     cls.avg_input_tokens = np.mean(input_tokens)
                     if prune:
-                        target_output_tokens = (
-                            inference_budget * 1000 - cls.avg_input_tokens * price_input
-                        ) / price_output
-                result["inference_cost"] = (avg_n_tokens * price_output + cls.avg_input_tokens * price_input) / 1000
+                        target_output_tokens = (inference_budget * 1000 -
+                                                cls.avg_input_tokens * price_input) / price_output
+                result["inference_cost"] = (avg_n_tokens * price_output +
+                                            cls.avg_input_tokens * price_input) / 1000
                 break
             else:
                 if data_early_stop:
                     previous_num_completions = 0
                     n_tokens_list.clear()
                     responses_list.clear()
                 else:
@@ -593,15 +602,16 @@
         cls._max_valid_n_per_max_tokens, cls._min_invalid_n_per_max_tokens = {}, {}
         cls.optimization_budget = optimization_budget
         cls.inference_budget = inference_budget
         cls._prune_hp = "best_of" if space.get("best_of", 1) != 1 else "n"
         cls._prompts = space.get("prompt")
         if cls._prompts is None:
             cls._messages = space.get("messages")
-            if not all((isinstance(cls._messages, list), isinstance(cls._messages[0], (dict, list)))):
+            if not all((isinstance(cls._messages, list), isinstance(cls._messages[0],
+                                                                    (dict, list)))):
                 error_msg = "messages must be a list of dicts or a list of lists."
                 logger.error(error_msg)
                 raise AssertionError(error_msg)
             if isinstance(cls._messages[0], dict):
                 cls._messages = [cls._messages]
             space["messages"] = tune.choice(list(range(len(cls._messages))))
         else:
@@ -784,58 +794,61 @@
             raise ERROR
 
         # Warn if a config list was provided but was empty
         if type(config_list) is list and len(config_list) == 0:
             logger.warning(
                 "Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead."
             )
+        llm = config.pop("llm", None)
+        run_manage = config.pop("run_manager", None)
+        params = cls._construct_params(context,
+                                       config,
+                                       allow_format_str_template=allow_format_str_template)
 
-        if config_list:
-            last = len(config_list) - 1
-            cost = 0
-            for i, each_config in enumerate(config_list):
-                base_config = config.copy()
-                base_config["allow_format_str_template"] = allow_format_str_template
-                base_config.update(each_config)
-                if i < last and filter_func is None and "max_retry_period" not in base_config:
-                    # max_retry_period = 0 to avoid retrying when no filter is given
-                    base_config["max_retry_period"] = 0
-                try:
-                    response = cls.create(
-                        context,
-                        use_cache,
-                        raise_on_ratelimit_or_timeout=i < last or raise_on_ratelimit_or_timeout,
-                        **base_config,
-                    )
-                    if response == -1:
-                        return response
-                    pass_filter = filter_func is None or filter_func(
-                        context=context, base_config=config, response=response
-                    )
-                    if pass_filter or i == last:
-                        response["cost"] = cost + response["cost"]
-                        response["config_id"] = i
-                        response["pass_filter"] = pass_filter
-                        return response
-                    cost += response["cost"]
-                except (AuthenticationError, RateLimitError, Timeout, InvalidRequestError):
-                    logger.debug(f"failed with config {i}", exc_info=1)
-                    if i == last:
-                        raise
-        params = cls._construct_params(context, config, allow_format_str_template=allow_format_str_template)
-        if not use_cache:
-            return cls._get_response(
-                params, raise_on_ratelimit_or_timeout=raise_on_ratelimit_or_timeout, use_cache=False
-            )
-        seed = cls.seed
-        if "seed" in params:
-            cls.set_cache(params.pop("seed"))
-        with diskcache.Cache(cls.cache_path) as cls._cache:
-            cls.set_cache(seed)
-            return cls._get_response(params, raise_on_ratelimit_or_timeout=raise_on_ratelimit_or_timeout)
+        if llm:
+            return cls._get_resonse_llm(params=params, llm=llm, run_manage=run_manage)
+        else:
+            seed = cls.seed
+            if "seed" in params:
+                cls.set_cache(params.pop("seed"))
+            with diskcache.Cache(cls.cache_path) as cls._cache:
+                cls.set_cache(seed)
+                return cls._get_response(
+                    params,
+                    config_list[0],
+                    raise_on_ratelimit_or_timeout=raise_on_ratelimit_or_timeout,
+                    use_cache=use_cache)
+
+    @classmethod
+    async def acreate(cls,
+                      context: Optional[Dict] = None,
+                      use_cache: Optional[bool] = True,
+                      config_list: Optional[List[Dict]] = None,
+                      filter_func: Optional[Callable[[Dict, Dict, Dict], bool]] = None,
+                      raise_on_ratelimit_or_timeout: Optional[bool] = True,
+                      allow_format_str_template: Optional[bool] = False,
+                      **config):
+        llm = config.pop("llm", None)
+        run_manage = config.pop("run_manager", None)
+        params = cls._construct_params(context,
+                                       config,
+                                       allow_format_str_template=allow_format_str_template)
+        if llm:
+            return await cls._aget_resonse_llm(params=params, llm=llm, run_manage=run_manage)
+        else:
+            seed = cls.seed
+            if "seed" in params:
+                cls.set_cache(params.pop("seed"))
+            with diskcache.Cache(cls.cache_path) as cls._cache:
+                cls.set_cache(seed)
+                return cls._get_response(
+                    params,
+                    config_list[0],
+                    raise_on_ratelimit_or_timeout=raise_on_ratelimit_or_timeout,
+                    use_cache=use_cache)
 
     @classmethod
     def instantiate(
         cls,
         template: Union[str, None],
         context: Optional[Dict] = None,
         allow_format_str_template: Optional[bool] = False,
@@ -843,49 +856,44 @@
         if not context or template is None:
             return template
         if isinstance(template, str):
             return template.format(**context) if allow_format_str_template else template
         return template(context)
 
     @classmethod
-    def _construct_params(cls, context, config, prompt=None, messages=None, allow_format_str_template=False):
+    def _construct_params(cls,
+                          context,
+                          config,
+                          prompt=None,
+                          messages=None,
+                          allow_format_str_template=False):
         params = config.copy()
-        model = config["model"]
+        model = config.get("model")
         prompt = config.get("prompt") if prompt is None else prompt
         messages = config.get("messages") if messages is None else messages
         # either "prompt" should be in config (for being compatible with non-chat models)
         # or "messages" should be in config (for tuning chat models only)
         if prompt is None and (model in cls.chat_models or issubclass(cls, ChatCompletion)):
             if messages is None:
                 raise ValueError("Either prompt or messages should be in config for chat models.")
         if prompt is None:
             params["messages"] = (
-                [
-                    {
-                        **m,
-                        "content": cls.instantiate(m["content"], context, allow_format_str_template),
-                    }
-                    if m.get("content")
-                    else m
-                    for m in messages
-                ]
-                if context
-                else messages
-            )
-        elif model in cls.chat_models or issubclass(cls, ChatCompletion):
+                [{
+                    **m,
+                    "content": cls.instantiate(m["content"], context, allow_format_str_template),
+                } if m.get("content") else m for m in messages] if context else messages)
+        else:
             # convert prompt to messages
             params["messages"] = [
                 {
                     "role": "user",
                     "content": cls.instantiate(prompt, context, allow_format_str_template),
                 },
             ]
             params.pop("prompt", None)
-        else:
-            params["prompt"] = cls.instantiate(prompt, context, allow_format_str_template)
         return params
 
     @classmethod
     def test(
         cls,
         data,
         eval_func=None,
@@ -1001,18 +1009,16 @@
                 metric_agg_method = agg_method[key]
                 if not callable(metric_agg_method):
                     error_msg = "please provide a callable for each metric"
                     logger.error(error_msg)
                     raise AssertionError(error_msg)
                 result_agg[key] = metric_agg_method([r[key] for r in result_list])
         else:
-            raise ValueError(
-                "agg_method needs to be a string ('avg' or 'median'),\
-                or a callable, or a dictionary of callable."
-            )
+            raise ValueError("agg_method needs to be a string ('avg' or 'median'),\
+                or a callable, or a dictionary of callable.")
         logger.setLevel(old_level)
         # should we also return the result_list and responses_list or not?
         if "cost" not in result_agg:
             result_agg["cost"] = cost
         if "inference_cost" not in result_agg:
             result_agg["inference_cost"] = cost / len(data)
         if return_responses_and_per_instance_result:
@@ -1067,42 +1073,45 @@
         Returns:
             A list of text or function calls in the responses.
         """
         choices = response["choices"]
         if "text" in choices[0]:
             return [choice["text"] for choice in choices]
         return [
-            choice["message"] if "function_call" in choice["message"] else choice["message"].get("content", "")
-            for choice in choices
+            choice["message"] if choice["message"].get("function_call") else choice["message"].get(
+                "content", "") for choice in choices
         ]
 
     @classmethod
     @property
     def logged_history(cls) -> Dict:
         """Return the book keeping dictionary."""
         return cls._history_dict
 
     @classmethod
     def print_usage_summary(cls) -> Dict:
         """Return the usage summary."""
         if cls._history_dict is None:
             print("No usage summary available.", flush=True)
 
-        token_count_summary = defaultdict(lambda: {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0})
+        token_count_summary = defaultdict(lambda: {
+            "prompt_tokens": 0,
+            "completion_tokens": 0,
+            "total_tokens": 0
+        })
 
         if not cls._history_compact:
             source = cls._history_dict.values()
             total_cost = sum(msg_pair["response"]["cost"] for msg_pair in source)
         else:
             # source = cls._history_dict["token_count"]
             # total_cost = sum(cls._history_dict['cost'])
             total_cost = sum(sum(value_list["cost"]) for value_list in cls._history_dict.values())
-            source = (
-                token_data for value_list in cls._history_dict.values() for token_data in value_list["token_count"]
-            )
+            source = (token_data for value_list in cls._history_dict.values()
+                      for token_data in value_list["token_count"])
 
         for entry in source:
             if not cls._history_compact:
                 model = entry["response"]["model"]
                 token_data = entry["response"]["usage"]
             else:
                 model = entry["model"]
@@ -1116,17 +1125,18 @@
         for model, counts in token_count_summary.items():
             print(
                 f"Token count summary for model {model}: prompt_tokens: {counts['prompt_tokens']}, completion_tokens: {counts['completion_tokens']}, total_tokens: {counts['total_tokens']}",
                 flush=True,
             )
 
     @classmethod
-    def start_logging(
-        cls, history_dict: Optional[Dict] = None, compact: Optional[bool] = True, reset_counter: Optional[bool] = True
-    ):
+    def start_logging(cls,
+                      history_dict: Optional[Dict] = None,
+                      compact: Optional[bool] = True,
+                      reset_counter: Optional[bool] = True):
         """Start book keeping.
 
         Args:
             history_dict (Dict): A dictionary for book keeping.
                 If no provided, a new one will be created.
             compact (bool): Whether to keep the history dictionary compact.
                 Compact history contains one key per conversation, and the value is a dictionary
@@ -1177,8 +1187,7 @@
 
 
 class ChatCompletion(Completion):
     """A class for OpenAI API ChatCompletion. Share the same API as Completion."""
 
     default_search_space = Completion.default_search_space.copy()
     default_search_space["model"] = tune.choice(["gpt-3.5-turbo", "gpt-4"])
-    openai_completion_class = not ERROR and openai.ChatCompletion
```

## autogen/oai/openai_utils.py

```diff
@@ -1,15 +1,25 @@
 import os
 import json
 import tempfile
 from pathlib import Path
-from typing import List, Optional, Dict, Set, Union
+from typing import Any, List, Mapping, Optional, Dict, Set, Union, cast
 import logging
 from dotenv import find_dotenv, load_dotenv
-
+from langchain_core.messages import (
+    AIMessage,
+    BaseMessage,
+    ChatMessage,
+    FunctionMessage,
+    HumanMessage,
+    SystemMessage,
+    ToolMessage,
+)
+from langchain_core.outputs import ChatResult
+import openai
 
 NON_CACHE_KEY = ["api_key", "api_base", "api_type", "api_version"]
 
 
 def get_key(config):
     """Get a unique identifier of a configuration.
 
@@ -28,17 +38,18 @@
     #     return tuple(get_key(x) for x in sorted(config.items()))
     # if isinstance(config, list):
     #     return tuple(get_key(x) for x in config)
     # return config
     return json.dumps(config, sort_keys=True)
 
 
-def get_config_list(
-    api_keys: List, api_bases: Optional[List] = None, api_type: Optional[str] = None, api_version: Optional[str] = None
-) -> List[Dict]:
+def get_config_list(api_keys: List,
+                    api_bases: Optional[List] = None,
+                    api_type: Optional[str] = None,
+                    api_version: Optional[str] = None) -> List[Dict]:
     """Get a list of configs for openai api calls.
 
     Args:
         api_keys (list): The api keys for openai api calls.
         api_bases (list, optional): The api bases for openai api calls.
         api_type (str, optional): The api type for openai api calls.
         api_version (str, optional): The api version for openai api calls.
@@ -108,28 +119,22 @@
         get_config_list(
             # Assuming Azure OpenAI api keys in os.environ["AZURE_OPENAI_API_KEY"], in separated lines
             api_keys=os.environ.get("AZURE_OPENAI_API_KEY", "").split("\n"),
             # Assuming Azure OpenAI api bases in os.environ["AZURE_OPENAI_API_BASE"], in separated lines
             api_bases=os.environ.get("AZURE_OPENAI_API_BASE", "").split("\n"),
             api_type="azure",
             api_version="2023-07-01-preview",  # change if necessary
-        )
-        if exclude != "aoai"
-        else []
-    )
+        ) if exclude != "aoai" else [])
     openai_config = (
         get_config_list(
             # Assuming OpenAI API_KEY in os.environ["OPENAI_API_KEY"]
             api_keys=os.environ.get("OPENAI_API_KEY", "").split("\n"),
             # "api_type": "open_ai",
             # "api_base": "https://api.openai.com/v1",
-        )
-        if exclude != "openai"
-        else []
-    )
+        ) if exclude != "openai" else [])
     config_list = openai_config + aoai_config
     return config_list
 
 
 def config_list_from_models(
     key_file_path: Optional[str] = ".",
     openai_api_key_file: Optional[str] = "key_openai.txt",
@@ -155,15 +160,17 @@
         key_file_path,
         openai_api_key_file,
         aoai_api_key_file,
         aoai_api_base_file,
         exclude,
     )
     if model_list:
-        config_list = [{**config, "model": model} for model in model_list for config in config_list]
+        config_list = [{
+            **config, "model": model
+        } for model in model_list for config in config_list]
     return config_list
 
 
 def config_list_gpt4_gpt35(
     key_file_path: Optional[str] = ".",
     openai_api_key_file: Optional[str] = "key_openai.txt",
     aoai_api_key_file: Optional[str] = "key_aoai.txt",
@@ -201,15 +208,16 @@
             and values corresponding to lists of acceptable values for each key.
 
     Returns:
         list: The filtered config list.
     """
     if filter_dict:
         config_list = [
-            config for config in config_list if all(config.get(key) in value for key, value in filter_dict.items())
+            config for config in config_list if all(
+                config.get(key) in value for key, value in filter_dict.items())
         ]
     return config_list
 
 
 def config_list_from_json(
     env_or_file: str,
     file_location: Optional[str] = "",
@@ -243,17 +251,18 @@
                 config_list = json.load(json_file)
         except FileNotFoundError:
             logging.warning(f"The specified config_list file '{config_list_path}' does not exist.")
             return []
     return filter_config(config_list, filter_dict)
 
 
-def get_config(
-    api_key: str, api_base: Optional[str] = None, api_type: Optional[str] = None, api_version: Optional[str] = None
-) -> Dict:
+def get_config(api_key: str,
+               api_base: Optional[str] = None,
+               api_type: Optional[str] = None,
+               api_version: Optional[str] = None) -> Dict:
     """
     Construct a configuration dictionary with the provided API configurations.
     Appending the additional configurations to the config only if they're set
 
     example:
     >> model_api_key_map={
         "gpt-4": "OPENAI_API_KEY",
@@ -280,16 +289,17 @@
         config["api_type"] = api_type
     if api_version:
         config["api_version"] = api_version
     return config
 
 
 def config_list_from_dotenv(
-    dotenv_file_path: Optional[str] = None, model_api_key_map: Optional[dict] = None, filter_dict: Optional[dict] = None
-) -> List[Dict[str, Union[str, Set[str]]]]:
+        dotenv_file_path: Optional[str] = None,
+        model_api_key_map: Optional[dict] = None,
+        filter_dict: Optional[dict] = None) -> List[Dict[str, Union[str, Set[str]]]]:
     """
     Load API configurations from a specified .env file or environment variables and construct a list of configurations.
 
     This function will:
     - Load API keys from a provided .env file or from existing environment variables.
     - Create a configuration dictionary for each model using the API keys and additional configurations.
     - Filter and return the configurations based on provided filters.
@@ -320,15 +330,16 @@
         if dotenv_path.exists():
             load_dotenv(dotenv_path)
         else:
             logging.warning(f"The specified .env file {dotenv_path} does not exist.")
     else:
         dotenv_path = find_dotenv()
         if not dotenv_path:
-            logging.warning("No .env file found. Loading configurations from environment variables.")
+            logging.warning(
+                "No .env file found. Loading configurations from environment variables.")
         load_dotenv(dotenv_path)
 
     # Ensure the model_api_key_map is not None to prevent TypeErrors during key assignment.
     model_api_key_map = model_api_key_map or {}
 
     # Ensure default models are always considered
     default_models = ["gpt-4", "gpt-3.5-turbo"]
@@ -377,7 +388,101 @@
 
     if len(config_list) == 0:
         logging.error("No configurations loaded.")
         return []
 
     logging.info(f"Models available: {[config['model'] for config in config_list]}")
     return config_list
+
+
+def _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:
+    """Convert a dictionary to a LangChain message.
+
+    Args:
+        _dict: The dictionary.
+
+    Returns:
+        The LangChain message.
+    """
+    role = _dict.get("role")
+    name = _dict.get("name")
+    id_ = _dict.get("id")
+    if role == "user":
+        return HumanMessage(content=_dict.get("content", ""), id=id_, name=name)
+    elif role == "assistant":
+        # Fix for azure
+        # Also OpenAI returns None for tool invocations
+        content = _dict.get("content", "") or ""
+        additional_kwargs: Dict = {}
+        if function_call := _dict.get("function_call"):
+            additional_kwargs["function_call"] = dict(function_call)
+        if tool_calls := _dict.get("tool_calls"):
+            additional_kwargs["tool_calls"] = tool_calls
+        return AIMessage(content=content, additional_kwargs=additional_kwargs, name=name, id=id_)
+    elif role == "system":
+        return SystemMessage(content=_dict.get("content", ""), name=name, id=id_)
+    elif role == "function":
+        return FunctionMessage(content=_dict.get("content", ""),
+                               name=cast(str, _dict.get("name")),
+                               id=id_)
+    elif role == "tool":
+        additional_kwargs = {}
+        if "name" in _dict:
+            additional_kwargs["name"] = _dict["name"]
+        return ToolMessage(
+            content=_dict.get("content", ""),
+            tool_call_id=cast(str, _dict.get("tool_call_id")),
+            additional_kwargs=additional_kwargs,
+            name=name,
+            id=id_,
+        )
+    else:
+        return ChatMessage(content=_dict.get("content", ""), role=role, id=id_)
+
+
+def _convert_message_to_dict(_msg: BaseMessage) -> Dict[str, Any]:
+    message = {}
+    message['choices'] = []
+    choice = {}
+    if isinstance(_msg, HumanMessage):
+        choice['message'] = {"content": _msg.content, 'role': "user", "name": _msg.name}
+    elif isinstance(_msg, AIMessage):
+        choice['message'] = {'content': _msg.content, 'role': 'assistant', "name": _msg.name}
+        choice['message'].update(_msg.additional_kwargs)
+    elif isinstance(_msg, SystemMessage):
+        choice['message'] = {'content': _msg.content, 'role': 'system', "name": _msg.name}
+    elif isinstance(_msg, FunctionMessage):
+        choice['message'] = {'content': _msg.content, 'role': 'function', "name": _msg.name}
+    elif isinstance(_msg, ToolMessage):
+        choice['message'] = {'content': _msg.content, 'role': 'tool', "name": _msg.name}
+        choice['message']['tool_call_id'] = _msg.tool_call_id
+        choice['message'].update(_msg.additional_kwargs)
+    else:
+        choice['message'] = {"content": _msg.content, 'role': _msg.role, "name": _msg.name}
+    message['choices'].append(choice)
+    return message
+
+
+def create_chat_result(response: Union[dict, openai.BaseModel, ChatResult]) -> Dict:
+    if isinstance(response, openai.BaseModel):
+        response = response.model_dump()
+    else:
+        # 将llm 返回的message，转为choies dict
+        response = _convert_message_to_dict(response)
+
+    # for res in response["choices"]:
+    #     message = _convert_dict_to_message(res["message"])
+    #     generation_info = dict(finish_reason=res.get("finish_reason"))
+    #     if "logprobs" in res:
+    #         generation_info["logprobs"] = res["logprobs"]
+    #     gen = ChatGeneration(
+    #         message=message,
+    #         generation_info=generation_info,
+    #     )
+    #     generations.append(gen)
+    # token_usage = response.get("usage", {})
+    # llm_output = {
+    #     "token_usage": token_usage,
+    #     "system_fingerprint": response.get("system_fingerprint", ""),
+    # }
+    # res = ChatResult(generations=generations, llm_output=llm_output)
+    return response
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## Comparing `bisheng_pyautogen-0.2.1.dist-info/LICENSE` & `bisheng_pyautogen-0.3.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `bisheng_pyautogen-0.2.1.dist-info/LICENSE-CODE` & `bisheng_pyautogen-0.3.0.dist-info/LICENSE-CODE`

 * *Files identical despite different names*

## Comparing `bisheng_pyautogen-0.2.1.dist-info/METADATA` & `bisheng_pyautogen-0.3.0.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: bisheng-pyautogen
-Version: 0.2.1
+Version: 0.3.0
 Summary: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework
 Home-page: https://github.com/microsoft/autogen
 Author: AutoGen
 Author-email: auto-gen@outlook.com
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
```

## Comparing `bisheng_pyautogen-0.2.1.dist-info/RECORD` & `bisheng_pyautogen-0.3.0.dist-info/RECORD`

 * *Files 21% similar despite different names*

```diff
@@ -1,50 +1,34 @@
 autogen/__init__.py,sha256=if3YKscyXplpt1IEwVwjg_1rD0TwfPUkaG3wG4-A-Q0,234
-autogen/_pydantic.py,sha256=pOvbPALQurXDwd_Mo6nkfYqp9rLWkiuB4uITeEOg8ts,3191
-autogen/agent_utils.py,sha256=PPOfab7MGe8j6NDi6ZWe9Rz7irU4aUSai0y2Cc01EuA,2045
-autogen/browser_utils.py,sha256=-DuI2ntAhDlqVsUhdGgrL65L49HV68Dy8WHMR103fvI,11329
 autogen/code_utils.py,sha256=uKaDOJie98O0Hj5OA34wFCG3DDiLUyMAmNPlkTnYbF4,24054
-autogen/function_utils.py,sha256=X09xsKPz629oQufc_ygWweNuQLe7Db2q77W4vjSiBQs,12053
 autogen/math_utils.py,sha256=0qwuRC0fCfRr5lbfSC9XHB1WqeMrZl_aT7MGJrB2esw,10051
 autogen/retrieve_utils.py,sha256=nxwLpvTVsabEtOEF0ipJHZb6X3FkEuM8hFIV8Mxint0,16478
-autogen/token_count_utils.py,sha256=j6gAYAhQPeohS2c5VLyrpOa6qZIY5Y8h-6OOMCc_gyM,7041
-autogen/version.py,sha256=HfjVOrpTnmZ-xVFCYSVmX50EXaBQeJteUHG-PD6iQs8,22
+autogen/version.py,sha256=VrXpHDu3erkzwl_WXrqINBm9xWkcyUy53IQOj042dOs,22
 autogen/agentchat/__init__.py,sha256=rwvv1DdQMvNMsWDQ7u8M4HQsKhOvFjoZ2xlPvnW6QkQ,350
 autogen/agentchat/agent.py,sha256=EuT35_oLESBltnKGMw0N1gGf5TgvFCTaaMT_aCGZ3_s,2922
-autogen/agentchat/assistant_agent.py,sha256=PqxPYpz8R3nhIFDuprOLGMCdIL6fGxrf7eEoJlTOY2k,4840
-autogen/agentchat/conversable_agent.py,sha256=_WuSnKEfa45flFqnqziId-QnGdmotQYeXk_Ki_RHymE,68410
+autogen/agentchat/assistant_agent.py,sha256=9qLk1a7q593dIzRPSSzWrHB95BsOl2mUGqT8k3475lk,4810
+autogen/agentchat/conversable_agent.py,sha256=UGprkGtFtplyp6-ttze13e60i4HIrq27ANOFz0_i3_8,68862
 autogen/agentchat/groupchat.py,sha256=584cd3fCBqyfNcbLU3sD__pwaVq38Sxju71gty6sr4o,12074
-autogen/agentchat/user_proxy_agent.py,sha256=tjB1cej9PdQcXn_RuakbxRekcBLBccaJaUxDqkcb39g,5546
+autogen/agentchat/user_proxy_agent.py,sha256=sZZZuZh-nsT1UjYWW7vjEUTw4Fb_cThta6mzb3FIg9Q,5516
 autogen/agentchat/contrib/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-autogen/agentchat/contrib/agent_builder.py,sha256=3yY4bMT4IEuH4dDH9oG1BtAPvo31Z0ucI9sI6-pPdRY,32658
-autogen/agentchat/contrib/compressible_agent.py,sha256=iVsqmjFrOZ1UPZ59bM7UV2Bybn8wddbZo-npoNyQVxE,23884
-autogen/agentchat/contrib/gpt_assistant_agent.py,sha256=tR37aKTiPDPLDcCrIELiNSq1HYs8ndkfQQlDoIsIaF0,19861
-autogen/agentchat/contrib/img_utils.py,sha256=sB87Baz6gBIH0gL4ny8M2wS7Ed6NvAm5Bm6PU8tsbn0,5778
-autogen/agentchat/contrib/llava_agent.py,sha256=UvAD_ulgDBD36EgK3JCiERS_G9suFoQWr7SxcdA41d8,6247
 autogen/agentchat/contrib/math_user_proxy_agent.py,sha256=MHYNX00C35nFB54CTLDrgC0vwO4BxlRZvE-USLA1ios,19162
-autogen/agentchat/contrib/multimodal_conversable_agent.py,sha256=LL4Kgz2DmINMvJ4pG3DT3NOAErrN3c93k9dC-OoGoz0,3487
 autogen/agentchat/contrib/qdrant_retrieve_user_proxy_agent.py,sha256=LkwGuCL_SgVkqSJCM6E9udmD1cZT2fh3kG4ODV5UgKY,15881
 autogen/agentchat/contrib/retrieve_assistant_agent.py,sha256=Em49Zy99Z-GT_Do7hH4sEdc2LZtr3Sb2DmLS1cS6Se0,1994
 autogen/agentchat/contrib/retrieve_user_proxy_agent.py,sha256=3Y21IP-b4h6wWSHS1Ft9-zjIw7G2NUDUgiWpPhw4rIk,23552
 autogen/agentchat/contrib/teachable_agent.py,sha256=bZd1T4n2a_-h-I4xyd12rNCw73BPqbLLV4h96oUwsEI,21183
 autogen/agentchat/contrib/text_analyzer_agent.py,sha256=RCFpfO50Y-p1MIQ0Z1T3jdAM7Lj7qb2zWqeL7Wp5-3s,4530
-autogen/agentchat/contrib/web_surfer.py,sha256=X12TrAHByEJJud7JydFnVErhzh6H4cWZ7rkz3PWnSGg,16171
-autogen/agentchat/contrib/capabilities/__init__.py,sha256=ajM8sOD-s5FBdWT-EvTmocXXnnzxtBkpk2AvaJ7dIJo,133
-autogen/agentchat/contrib/capabilities/agent_capability.py,sha256=sSPiaKT2QY-T7B67SF_v7KOfakCg6GjqB5sjb46LOTA,545
-autogen/agentchat/contrib/capabilities/teachability.py,sha256=_rms3IaIYZYkydNPyN2PJS32-ppeKIXUs6UE38mMRqU,18432
 autogen/cache/__init__.py,sha256=h8oFajRV5LJYKgBQVDYsc17-7a_QAMDkq6DQEmE0mWg,46
 autogen/cache/abstract_cache_base.py,sha256=kVBfTZxHcIP1LW2TBizcL4IDDpsG6QDhJ3CVrC1p5Kw,2839
 autogen/cache/cache.py,sha256=d0hKtAF8zY_d449qtdH8BbrXM7qng6Pkyoil80eT1JI,4882
 autogen/cache/cache_factory.py,sha256=SMPlbvRMtTS5ouYtEqOZDu3ykwIKnb7KIxZW7DbU3dM,1594
 autogen/cache/disk_cache.py,sha256=ECXd8Yj_NDU_kltZvfHxZLeomAozv_EI7WPLvUZ8Q80,2704
 autogen/cache/redis_cache.py,sha256=STxA3Ej3_L4Ct3xAkDUH2uxppMoXSV3ahssKukByLdA,3485
 autogen/extensions/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autogen/oai/__init__.py,sha256=1TWpfh5SGy3Jw__AC7_b3gf0FSdNdqnn5skmwbH8nbs,496
-autogen/oai/client.py,sha256=LsIOnyEcsew0wK2IRy2uPn933WEFKWjRfSUJxL-LJM0,31834
-autogen/oai/completion.py,sha256=gwf-FKxjLlZGRS15HGsoupip_ox5fzyFaTl2RNUhFHs,52398
-autogen/oai/openai_utils.py,sha256=3A7R1UhFB1KxelYgUZI7YzJQU3xBYnS9lm3OFYzVsi8,15584
-bisheng_pyautogen-0.2.1.dist-info/LICENSE,sha256=fnFw486_iKn2DHuEIUGDI8CTBNoa9NXpD02h3ByKJmE,18650
-bisheng_pyautogen-0.2.1.dist-info/LICENSE-CODE,sha256=ws_MuBL-SCEBqPBFl9_FqZkaaydIJmxHrJG2parhU4M,1141
-bisheng_pyautogen-0.2.1.dist-info/METADATA,sha256=5yyKCti4LlpexSc8dCKQUAvf83MKX3mMt8puc-mlljA,13750
-bisheng_pyautogen-0.2.1.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-bisheng_pyautogen-0.2.1.dist-info/top_level.txt,sha256=qRZ2MW8yuy0LtOLpeZndl4H1buusjxhwD6c9AXbVqKQ,8
-bisheng_pyautogen-0.2.1.dist-info/RECORD,,
+autogen/oai/completion.py,sha256=9FFNHhy9EQFbG54AqeB022iJKmxps0asKufxpU3gLQg,52262
+autogen/oai/openai_utils.py,sha256=5hkc9f9sv0wUY6EgS_3s-XhHr8tZ3jyBu-0bzzipqjE,19776
+bisheng_pyautogen-0.3.0.dist-info/LICENSE,sha256=fnFw486_iKn2DHuEIUGDI8CTBNoa9NXpD02h3ByKJmE,18650
+bisheng_pyautogen-0.3.0.dist-info/LICENSE-CODE,sha256=ws_MuBL-SCEBqPBFl9_FqZkaaydIJmxHrJG2parhU4M,1141
+bisheng_pyautogen-0.3.0.dist-info/METADATA,sha256=ET_VNIoX1uuCEByvWXjHeecrglbtcQxFzdno6VI2TBA,13750
+bisheng_pyautogen-0.3.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+bisheng_pyautogen-0.3.0.dist-info/top_level.txt,sha256=qRZ2MW8yuy0LtOLpeZndl4H1buusjxhwD6c9AXbVqKQ,8
+bisheng_pyautogen-0.3.0.dist-info/RECORD,,
```

