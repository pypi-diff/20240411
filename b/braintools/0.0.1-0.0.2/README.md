# Comparing `tmp/braintools-0.0.1-py2.py3-none-any.whl.zip` & `tmp/braintools-0.0.2-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,23 +1,40 @@
-Zip file size: 46467 bytes, number of entries: 21
--rw-rw-rw-  2.0 fat      177 b- defN 24-Mar-24 03:11 braintools/__init__.py
--rw-rw-rw-  2.0 fat      302 b- defN 24-Mar-21 08:46 braintools/functional/__init__.py
--rw-rw-rw-  2.0 fat    21865 b- defN 24-Mar-24 03:17 braintools/functional/_activations.py
--rw-rw-rw-  2.0 fat     1000 b- defN 24-Mar-21 12:07 braintools/functional/_others.py
--rw-rw-rw-  2.0 fat     1834 b- defN 24-Mar-15 14:10 braintools/functional/_spikes.py
--rw-rw-rw-  2.0 fat      391 b- defN 24-Mar-21 07:38 braintools/init/__init__.py
--rw-rw-rw-  2.0 fat      134 b- defN 24-Mar-19 13:59 braintools/init/_base.py
--rw-rw-rw-  2.0 fat     4720 b- defN 24-Mar-22 09:56 braintools/init/_generic.py
--rw-rw-rw-  2.0 fat    14455 b- defN 24-Mar-21 07:42 braintools/init/_random_inits.py
--rw-rw-rw-  2.0 fat     2273 b- defN 24-Mar-21 07:42 braintools/init/_regular_inits.py
--rw-rw-rw-  2.0 fat      254 b- defN 24-Mar-24 03:11 braintools/inputs/__init__.py
--rw-rw-rw-  2.0 fat    10468 b- defN 24-Mar-24 03:10 braintools/inputs/currents.py
--rw-rw-rw-  2.0 fat     3408 b- defN 24-Mar-24 03:10 braintools/inputs/tests/test_currents.py
--rw-rw-rw-  2.0 fat      215 b- defN 24-Mar-22 15:10 braintools/optim/__init__.py
--rw-rw-rw-  2.0 fat    14499 b- defN 24-Mar-22 06:25 braintools/optim/_lr_scheduler.py
--rw-rw-rw-  2.0 fat    44614 b- defN 24-Mar-22 15:10 braintools/optim/_sgd_optimizer.py
--rw-rw-rw-  2.0 fat    35773 b- defN 24-Mar-24 03:18 braintools-0.0.1.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     3652 b- defN 24-Mar-24 03:18 braintools-0.0.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat      110 b- defN 24-Mar-24 03:18 braintools-0.0.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 24-Mar-24 03:18 braintools-0.0.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     1806 b- defN 24-Mar-24 03:18 braintools-0.0.1.dist-info/RECORD
-21 files, 161961 bytes uncompressed, 43517 bytes compressed:  73.1%
+Zip file size: 79791 bytes, number of entries: 38
+-rw-rw-rw-  2.0 fat      914 b- defN 24-Apr-06 07:27 braintools/__init__.py
+-rw-rw-rw-  2.0 fat     1025 b- defN 24-Apr-06 08:01 braintools/functional/__init__.py
+-rw-rw-rw-  2.0 fat    18207 b- defN 24-Apr-11 11:10 braintools/functional/_activations.py
+-rw-rw-rw-  2.0 fat     1745 b- defN 24-Apr-11 11:10 braintools/functional/_normalization.py
+-rw-rw-rw-  2.0 fat     2581 b- defN 24-Apr-11 11:10 braintools/functional/_spikes.py
+-rw-rw-rw-  2.0 fat     1102 b- defN 24-Apr-06 07:27 braintools/init/__init__.py
+-rw-rw-rw-  2.0 fat     1153 b- defN 24-Apr-06 07:27 braintools/init/_base.py
+-rw-rw-rw-  2.0 fat     6427 b- defN 24-Apr-06 07:27 braintools/init/_generic.py
+-rw-rw-rw-  2.0 fat    15361 b- defN 24-Apr-06 07:27 braintools/init/_random_inits.py
+-rw-rw-rw-  2.0 fat     3071 b- defN 24-Apr-06 07:27 braintools/init/_regular_inits.py
+-rw-rw-rw-  2.0 fat      963 b- defN 24-Apr-06 07:27 braintools/input/__init__.py
+-rw-rw-rw-  2.0 fat    11180 b- defN 24-Apr-11 11:10 braintools/input/currents.py
+-rw-rw-rw-  2.0 fat     4168 b- defN 24-Apr-11 11:10 braintools/input/currents_test.py
+-rw-rw-rw-  2.0 fat     1687 b- defN 24-Apr-06 07:27 braintools/metric/__init__.py
+-rw-rw-rw-  2.0 fat    22249 b- defN 24-Apr-11 11:10 braintools/metric/_classification.py
+-rw-rw-rw-  2.0 fat    22447 b- defN 24-Apr-11 11:10 braintools/metric/_classification_test.py
+-rw-rw-rw-  2.0 fat     9962 b- defN 24-Apr-06 07:27 braintools/metric/_correlation.py
+-rw-rw-rw-  2.0 fat     3940 b- defN 24-Apr-06 07:27 braintools/metric/_correlation_test.py
+-rw-rw-rw-  2.0 fat     2158 b- defN 24-Apr-06 07:27 braintools/metric/_fenchel_young.py
+-rw-rw-rw-  2.0 fat     2530 b- defN 24-Apr-11 11:10 braintools/metric/_fenchel_young_test.py
+-rw-rw-rw-  2.0 fat     2456 b- defN 24-Apr-06 07:27 braintools/metric/_firings.py
+-rw-rw-rw-  2.0 fat     1359 b- defN 24-Apr-06 07:27 braintools/metric/_firings_test.py
+-rw-rw-rw-  2.0 fat     4967 b- defN 24-Apr-11 11:10 braintools/metric/_lfp.py
+-rw-rw-rw-  2.0 fat     6193 b- defN 24-Apr-06 07:27 braintools/metric/_ranking.py
+-rw-rw-rw-  2.0 fat     8907 b- defN 24-Apr-11 11:10 braintools/metric/_ranking_test.py
+-rw-rw-rw-  2.0 fat    16755 b- defN 24-Apr-11 11:10 braintools/metric/_regression.py
+-rw-rw-rw-  2.0 fat     4945 b- defN 24-Apr-11 11:10 braintools/metric/_regression_test.py
+-rw-rw-rw-  2.0 fat     1583 b- defN 24-Apr-06 07:27 braintools/metric/_smoothing.py
+-rw-rw-rw-  2.0 fat     2012 b- defN 24-Apr-11 11:10 braintools/metric/_smoothing_test.py
+-rw-rw-rw-  2.0 fat     1036 b- defN 24-Apr-06 08:01 braintools/metric/_util.py
+-rw-rw-rw-  2.0 fat      924 b- defN 24-Apr-06 07:27 braintools/optim/__init__.py
+-rw-rw-rw-  2.0 fat    15582 b- defN 24-Apr-06 07:27 braintools/optim/_lr_scheduler.py
+-rw-rw-rw-  2.0 fat    45895 b- defN 24-Apr-06 07:27 braintools/optim/_sgd_optimizer.py
+-rw-rw-rw-  2.0 fat    11554 b- defN 24-Apr-11 11:21 braintools-0.0.2.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     3970 b- defN 24-Apr-11 11:21 braintools-0.0.2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      110 b- defN 24-Apr-11 11:21 braintools-0.0.2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 24-Apr-11 11:21 braintools-0.0.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     3344 b- defN 24-Apr-11 11:21 braintools-0.0.2.dist-info/RECORD
+38 files, 264473 bytes uncompressed, 74423 bytes compressed:  71.9%
```

## zipnote {}

```diff
@@ -3,15 +3,15 @@
 
 Filename: braintools/functional/__init__.py
 Comment: 
 
 Filename: braintools/functional/_activations.py
 Comment: 
 
-Filename: braintools/functional/_others.py
+Filename: braintools/functional/_normalization.py
 Comment: 
 
 Filename: braintools/functional/_spikes.py
 Comment: 
 
 Filename: braintools/init/__init__.py
 Comment: 
@@ -24,41 +24,92 @@
 
 Filename: braintools/init/_random_inits.py
 Comment: 
 
 Filename: braintools/init/_regular_inits.py
 Comment: 
 
-Filename: braintools/inputs/__init__.py
+Filename: braintools/input/__init__.py
 Comment: 
 
-Filename: braintools/inputs/currents.py
+Filename: braintools/input/currents.py
 Comment: 
 
-Filename: braintools/inputs/tests/test_currents.py
+Filename: braintools/input/currents_test.py
+Comment: 
+
+Filename: braintools/metric/__init__.py
+Comment: 
+
+Filename: braintools/metric/_classification.py
+Comment: 
+
+Filename: braintools/metric/_classification_test.py
+Comment: 
+
+Filename: braintools/metric/_correlation.py
+Comment: 
+
+Filename: braintools/metric/_correlation_test.py
+Comment: 
+
+Filename: braintools/metric/_fenchel_young.py
+Comment: 
+
+Filename: braintools/metric/_fenchel_young_test.py
+Comment: 
+
+Filename: braintools/metric/_firings.py
+Comment: 
+
+Filename: braintools/metric/_firings_test.py
+Comment: 
+
+Filename: braintools/metric/_lfp.py
+Comment: 
+
+Filename: braintools/metric/_ranking.py
+Comment: 
+
+Filename: braintools/metric/_ranking_test.py
+Comment: 
+
+Filename: braintools/metric/_regression.py
+Comment: 
+
+Filename: braintools/metric/_regression_test.py
+Comment: 
+
+Filename: braintools/metric/_smoothing.py
+Comment: 
+
+Filename: braintools/metric/_smoothing_test.py
+Comment: 
+
+Filename: braintools/metric/_util.py
 Comment: 
 
 Filename: braintools/optim/__init__.py
 Comment: 
 
 Filename: braintools/optim/_lr_scheduler.py
 Comment: 
 
 Filename: braintools/optim/_sgd_optimizer.py
 Comment: 
 
-Filename: braintools-0.0.1.dist-info/LICENSE
+Filename: braintools-0.0.2.dist-info/LICENSE
 Comment: 
 
-Filename: braintools-0.0.1.dist-info/METADATA
+Filename: braintools-0.0.2.dist-info/METADATA
 Comment: 
 
-Filename: braintools-0.0.1.dist-info/WHEEL
+Filename: braintools-0.0.2.dist-info/WHEEL
 Comment: 
 
-Filename: braintools-0.0.1.dist-info/top_level.txt
+Filename: braintools-0.0.2.dist-info/top_level.txt
 Comment: 
 
-Filename: braintools-0.0.1.dist-info/RECORD
+Filename: braintools-0.0.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## braintools/__init__.py

```diff
@@ -1,12 +1,27 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 
 
-__version__ = "0.0.1"
 
-from . import inputs
+__version__ = "0.0.2"
+
+from . import metric
+from . import input
 from . import init
 from . import optim
 from . import functional
 
-__all__ = ['inputs', 'init', 'optim', 'functional']
-
+__all__ = ['input', 'init', 'optim', 'functional', 'metric']
```

## braintools/functional/__init__.py

```diff
@@ -1,10 +1,25 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
 
 from ._activations import *
 from ._activations import __all__ as __activations_all__
-from ._others import *
-from ._others import __all__ as __others_all__
+from ._normalization import *
+from ._normalization import __all__ as __others_all__
 from ._spikes import *
 from ._spikes import __all__ as __spikes_all__
 
 __all__ = __spikes_all__ + __others_all__ + __activations_all__
```

## braintools/functional/_activations.py

```diff
@@ -1,20 +1,34 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+
 """Shared neural network activations and other functions."""
 
-import operator
-from functools import partial
+
+from __future__ import annotations
 from typing import Any, Union, Sequence
 
 import jax
 import jax.numpy as jnp
-import numpy as np
-from jax import custom_jvp
-from jax import lax
 from jax.scipy.special import logsumexp
 from jax.typing import ArrayLike
+import braincore as bc
 
 __all__ = [
   "relu",
   "squareplus",
   "softplus",
   "soft_sign",
   "sigmoid",
@@ -44,15 +58,16 @@
   'prelu',
   'tanh_shrink',
   'softmin',
 ]
 
 
 def softmin(x, axis=-1):
-  r"""Applies the Softmin function to an n-dimensional input Tensor
+  r"""
+  Applies the Softmin function to an n-dimensional input Tensor
   rescaling them so that the elements of the n-dimensional output Tensor
   lie in the range `[0, 1]` and sum to 1.
 
   Softmin is defined as:
 
   .. math::
       \text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}
@@ -67,24 +82,26 @@
           along dim will sum to 1).
   """
   unnormalized = jnp.exp(-x)
   return unnormalized / unnormalized.sum(axis, keepdims=True)
 
 
 def tanh_shrink(x):
-  r"""Applies the element-wise function:
+  r"""
+  Applies the element-wise function:
 
   .. math::
       \text{Tanhshrink}(x) = x - \tanh(x)
   """
   return x - jnp.tanh(x)
 
 
 def prelu(x, a=0.25):
-  r"""Applies the element-wise function:
+  r"""
+  Applies the element-wise function:
 
   .. math::
       \text{PReLU}(x) = \max(0,x) + a * \min(0,x)
 
   or
 
   .. math::
@@ -94,19 +111,23 @@
       ax, & \text{ otherwise }
       \end{cases}
 
   Here :math:`a` is a learnable parameter. When called without arguments, `nn.PReLU()` uses a single
   parameter :math:`a` across all input channels. If called with `nn.PReLU(nChannels)`,
   a separate :math:`a` is used for each input channel.
   """
-  return jnp.where(x >= 0., x, a * x)
+  dtype = bc.math.get_dtype(x)
+  return jnp.where(x >= jnp.asarray(0., dtype),
+                   x,
+                   jnp.asarray(a, dtype) * x)
 
 
 def soft_shrink(x, lambd=0.5):
-  r"""Applies the soft shrinkage function elementwise:
+  r"""
+  Applies the soft shrinkage function elementwise:
 
   .. math::
       \text{SoftShrinkage}(x) =
       \begin{cases}
       x - \lambda, & \text{ if } x > \lambda \\
       x + \lambda, & \text{ if } x < -\lambda \\
       0, & \text{ otherwise }
@@ -115,15 +136,19 @@
   Args:
       lambd: the :math:`\lambda` (must be no less than zero) value for the Softshrink formulation. Default: 0.5
 
   Shape:
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
   """
-  return jnp.where(x > lambd, x - lambd, jnp.where(x < -lambd, x + lambd, 0.))
+  dtype = bc.math.get_dtype(x)
+  lambd = jnp.asarray(lambd, dtype)
+  return jnp.where(x > lambd,
+                   x - lambd,
+                   jnp.where(x < -lambd, x + lambd, jnp.asarray(0., dtype)))
 
 
 def mish(x):
   r"""Applies the Mish function, element-wise.
 
   Mish: A Self Regularized Non-Monotonic Neural Activation Function.
 
@@ -136,15 +161,15 @@
   Shape:
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
   """
   return x * jnp.tanh(softplus(x))
 
 
-def rrelu(key, x, lower=0.125, upper=0.3333333333333333):
+def rrelu(x, lower=0.125, upper=0.3333333333333333):
   r"""Applies the randomized leaky rectified liner unit function, element-wise,
   as described in the paper:
 
   `Empirical Evaluation of Rectified Activations in Convolutional Network`_.
 
   The function is defined as:
 
@@ -167,17 +192,17 @@
   Shape:
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   .. _`Empirical Evaluation of Rectified Activations in Convolutional Network`:
       https://arxiv.org/abs/1505.00853
   """
-  x = jnp.asarray(x)
-  a = jax.random.uniform(key, x.shape, x.dtype, lower, upper)
-  return jnp.where(x >= 0., x, a * x)
+  dtype = bc.math.get_dtype(x)
+  a = bc.random.uniform(lower, upper, size=jnp.shape(x), dtype=dtype)
+  return jnp.where(x >= jnp.asarray(0., dtype), x, jnp.asarray(a, dtype) * x)
 
 
 def hard_shrink(x, lambd=0.5):
   r"""Applies the Hard Shrinkage (Hardshrink) function element-wise.
 
   Hardshrink is defined as:
 
@@ -193,31 +218,21 @@
       lambd: the :math:`\lambda` value for the Hardshrink formulation. Default: 0.5
 
   Shape:
       - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
       - Output: :math:`(*)`, same shape as the input.
 
   """
-  return jnp.where(x > lambd, x, jnp.where(x < -lambd, x, 0.))
-
+  dtype = bc.math.get_dtype(x)
+  lambd = jnp.asarray(lambd, dtype)
+  return jnp.where(x > lambd,
+                   x,
+                   jnp.where(x < -lambd, x, jnp.asarray(0., dtype)))
 
-def canonicalize_axis(axis, num_dims) -> int:
-  """Canonicalize an axis in [-num_dims, num_dims) to [0, num_dims)."""
-  axis = operator.index(axis)
-  if not -num_dims <= axis < num_dims:
-    raise ValueError(f"axis {axis} is out of bounds for array of dimension {num_dims}")
-  if axis < 0:
-    axis = axis + num_dims
-  return axis
 
-
-# activations
-
-@custom_jvp
-@jax.jit
 def relu(x: ArrayLike) -> jax.Array:
   r"""Rectified linear unit activation function.
 
   Computes the element-wise function:
 
   .. math::
     \mathrm{relu}(x) = \max(x, 0)
@@ -241,74 +256,63 @@
     >>> jax.nn.relu(jax.numpy.array([-2., -1., -0.5, 0, 0.5, 1., 2.]))
     Array([0. , 0. , 0. , 0. , 0.5, 1. , 2. ], dtype=float32)
 
   See also:
     :func:`relu6`
 
   """
-  return jnp.maximum(x, 0)
-
+  return jax.nn.relu(x)
 
-# For behavior at 0, see https://openreview.net/forum?id=urrcVI-_jRm
-relu.defjvps(lambda g, ans, x: lax.select(x > 0, g, lax.full_like(g, 0)))
 
-
-@jax.jit
 def squareplus(x: ArrayLike, b: ArrayLike = 4) -> jax.Array:
   r"""Squareplus activation function.
 
   Computes the element-wise function
 
   .. math::
     \mathrm{squareplus}(x) = \frac{x + \sqrt{x^2 + b}}{2}
 
   as described in https://arxiv.org/abs/2112.11687.
 
   Args:
     x : input array
     b : smoothness parameter
   """
-  x = jnp.asarray(x)
-  b = jnp.asarray(b)
-  y = x + jnp.sqrt(jnp.square(x) + b)
-  return y / 2
+  dtype = bc.math.get_dtype(x)
+  return jax.nn.squareplus(x, jnp.asarray(b, dtype))
 
 
-@jax.jit
 def softplus(x: ArrayLike) -> jax.Array:
   r"""Softplus activation function.
 
   Computes the element-wise function
 
   .. math::
     \mathrm{softplus}(x) = \log(1 + e^x)
 
   Args:
     x : input array
   """
-  return jnp.logaddexp(x, 0)
+  return jax.nn.softplus(x)
 
 
-@jax.jit
 def soft_sign(x: ArrayLike) -> jax.Array:
   r"""Soft-sign activation function.
 
   Computes the element-wise function
 
   .. math::
     \mathrm{soft\_sign}(x) = \frac{x}{|x| + 1}
 
   Args:
     x : input array
   """
-  x_arr = jnp.asarray(x)
-  return x_arr / (jnp.abs(x_arr) + 1)
+  return jax.nn.soft_sign(x)
 
 
-@partial(jax.jit, inline=True)
 def sigmoid(x: ArrayLike) -> jax.Array:
   r"""Sigmoid activation function.
 
   Computes the element-wise function:
 
   .. math::
     \mathrm{sigmoid}(x) = \frac{1}{1 + e^{-x}}
@@ -319,18 +323,17 @@
   Returns:
     An array.
 
   See also:
     :func:`log_sigmoid`
 
   """
-  return lax.logistic(x)
+  return jax.nn.sigmoid(x)
 
 
-@jax.jit
 def silu(x: ArrayLike) -> jax.Array:
   r"""SiLU (a.k.a. swish) activation function.
 
   Computes the element-wise function:
 
   .. math::
     \mathrm{silu}(x) = x \cdot \mathrm{sigmoid}(x) = \frac{x}{1 + e^{-x}}
@@ -342,22 +345,20 @@
 
   Returns:
     An array.
 
   See also:
     :func:`sigmoid`
   """
-  x_arr = jnp.asarray(x)
-  return x_arr * sigmoid(x_arr)
+  return jax.nn.silu(x)
 
 
 swish = silu
 
 
-@jax.jit
 def log_sigmoid(x: ArrayLike) -> jax.Array:
   r"""Log-sigmoid activation function.
 
   Computes the element-wise function:
 
   .. math::
     \mathrm{log\_sigmoid}(x) = \log(\mathrm{sigmoid}(x)) = -\log(1 + e^{-x})
@@ -367,19 +368,17 @@
 
   Returns:
     An array.
 
   See also:
     :func:`sigmoid`
   """
-  x_arr = jnp.asarray(x)
-  return -softplus(-x_arr)
+  return jax.nn.log_sigmoid(x)
 
 
-@jax.jit
 def elu(x: ArrayLike, alpha: ArrayLike = 1.0) -> jax.Array:
   r"""Exponential linear unit activation function.
 
   Computes the element-wise function:
 
   .. math::
     \mathrm{elu}(x) = \begin{cases}
@@ -393,21 +392,19 @@
 
   Returns:
     An array.
 
   See also:
     :func:`selu`
   """
-  x_arr = jnp.asarray(x)
-  return jnp.where(x_arr > 0,
-                   x_arr,
-                   alpha * jnp.expm1(jnp.where(x_arr > 0, 0., x_arr)))
+  dtype = bc.math.get_dtype(x)
+  alpha = jnp.asarray(alpha, dtype)
+  return jax.nn.elu(x, alpha)
 
 
-@jax.jit
 def leaky_relu(x: ArrayLike, negative_slope: ArrayLike = 1e-2) -> jax.Array:
   r"""Leaky rectified linear unit activation function.
 
   Computes the element-wise function:
 
   .. math::
     \mathrm{leaky\_relu}(x) = \begin{cases}
@@ -423,19 +420,19 @@
 
   Returns:
     An array.
 
   See also:
     :func:`relu`
   """
-  x_arr = jnp.asarray(x)
-  return jnp.where(x_arr >= 0, x_arr, negative_slope * x_arr)
+  dtype = bc.math.get_dtype(x)
+  negative_slope = jnp.asarray(negative_slope, dtype)
+  return jax.nn.leaky_relu(x, negative_slope=negative_slope)
 
 
-@jax.jit
 def hard_tanh(x: ArrayLike) -> jax.Array:
   r"""Hard :math:`\mathrm{tanh}` activation function.
 
   Computes the element-wise function:
 
   .. math::
     \mathrm{hard\_tanh}(x) = \begin{cases}
@@ -446,19 +443,17 @@
 
   Args:
     x : input array
 
   Returns:
     An array.
   """
-  x_arr = jnp.asarray(x)
-  return jnp.where(x_arr > 1, 1, jnp.where(x_arr < -1, -1, x_arr))
+  return jax.nn.hard_tanh(x)
 
 
-@jax.jit
 def celu(x: ArrayLike, alpha: ArrayLike = 1.0) -> jax.Array:
   r"""Continuously-differentiable exponential linear unit activation.
 
   Computes the element-wise function:
 
   .. math::
     \mathrm{celu}(x) = \begin{cases}
@@ -473,18 +468,19 @@
   Args:
     x : input array
     alpha : array or scalar (default: 1.0)
 
   Returns:
     An array.
   """
-  return jnp.maximum(x, 0.0) + alpha * jnp.expm1(jnp.minimum(x, 0.0) / alpha)
+  dtype = bc.math.get_dtype(x)
+  alpha = jnp.asarray(alpha, dtype)
+  return jax.nn.celu(x, alpha)
 
 
-@jax.jit
 def selu(x: ArrayLike) -> jax.Array:
   r"""Scaled exponential linear unit activation.
 
   Computes the element-wise function:
 
   .. math::
     \mathrm{selu}(x) = \lambda \begin{cases}
@@ -504,17 +500,15 @@
 
   Returns:
     An array.
 
   See also:
     :func:`elu`
   """
-  alpha = 1.6732632423543772848170429916717
-  scale = 1.0507009873554804934193349852946
-  return scale * elu(x, alpha)
+  return jax.nn.selu(x)
 
 
 def gelu(x: ArrayLike, approximate: bool = True) -> jax.Array:
   r"""Gaussian error linear unit activation function.
 
   If ``approximate=False``, computes the element-wise function:
 
@@ -531,27 +525,17 @@
   For more information, see `Gaussian Error Linear Units (GELUs)
   <https://arxiv.org/abs/1606.08415>`_, section 2.
 
   Args:
     x : input array
     approximate: whether to use the approximate or exact formulation.
   """
-  from jax._src.numpy import util as numpy_util
-  [x_arr] = numpy_util.promote_args_inexact("gelu", x)
-
-  if approximate:
-    sqrt_2_over_pi = np.sqrt(2 / np.pi).astype(x_arr.dtype)
-    cdf = 0.5 * (1.0 + jnp.tanh(sqrt_2_over_pi * (x_arr + 0.044715 * (x_arr ** 3))))
-    return x_arr * cdf
-  else:
-    sqrt_2 = np.sqrt(2).astype(x_arr.dtype)
-    return jnp.array(x_arr * (lax.erf(x_arr / sqrt_2) + 1) / 2, dtype=x_arr.dtype)
+  return jax.nn.gelu(x, approximate=approximate)
 
 
-@partial(jax.jit, static_argnames=("axis",))
 def glu(x: ArrayLike, axis: int = -1) -> jax.Array:
   r"""Gated linear unit activation function.
 
   Computes the function:
 
   .. math::
     \mathrm{glu}(x) =  x\left[\ldots, 0:\frac{n}{2}, \ldots\right] \cdot
@@ -567,22 +551,17 @@
 
   Returns:
     An array.
 
   See also:
     :func:`sigmoid`
   """
-  x_arr = jnp.asarray(x)
-  size = x_arr.shape[axis]
-  assert size % 2 == 0, "axis size must be divisible by 2"
-  x1, x2 = jnp.split(x_arr, 2, axis)
-  return x1 * sigmoid(x2)
+  return jax.nn.glu(x, axis=axis)
 
 
-@partial(jax.jit, static_argnames=("axis",))
 def log_softmax(x: ArrayLike,
                 axis: int | tuple[int, ...] | None = -1,
                 where: ArrayLike | None = None,
                 initial: ArrayLike | None = None) -> jax.Array:
   r"""Log-Softmax function.
 
   Computes the logarithm of the :code:`softmax` function, which rescales
@@ -602,24 +581,15 @@
 
   Returns:
     An array.
 
   See also:
     :func:`softmax`
   """
-  x_arr = jnp.asarray(x)
-  x_max = jnp.max(x_arr, axis, where=where, initial=initial, keepdims=True)
-  x_safe = x_arr if where is None else jnp.where(where, x_arr, initial)
-  shifted = x_safe - lax.stop_gradient(x_max)
-  shifted_logsumexp = jnp.log(
-    jnp.sum(jnp.exp(shifted), axis, where=where, keepdims=True))
-  result = shifted - shifted_logsumexp
-  if where is not None:
-    return jnp.where(where, result, -jnp.inf)
-  return result
+  return jax.nn.log_softmax(x, axis, where, initial)
 
 
 def softmax(x: ArrayLike,
             axis: int | tuple[int, ...] | None = -1,
             where: ArrayLike | None = None,
             initial: ArrayLike | None = None) -> jax.Array:
   r"""Softmax function.
@@ -641,83 +611,24 @@
 
   Returns:
     An array.
 
   See also:
     :func:`log_softmax`
   """
-  return _softmax(x, axis, where, initial)  # type: ignore[return-value]
-
+  return jax.nn.softmax(x, axis, where, initial)
 
-@partial(jax.custom_jvp, nondiff_argnums=(1,))
-def _softmax(
-    x: ArrayLike,
-    axis: int | tuple[int, ...] | None = -1,
-    where: ArrayLike | None = None,
-    initial: ArrayLike | None = None
-) -> jax.Array:
-  x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)
-  x_safe = x if where is None else jnp.where(where, x, initial)
-  unnormalized = jnp.exp(x_safe - x_max)
-  result = unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)
-  if where is not None:
-    result = jnp.where(where, result, 0)
-  return result
-
-
-@_softmax.defjvp
-def _softmax_jvp(axis, primals, tangents):
-  (x, where, initial), (x_dot, _, _) = primals, tangents
-  y = _softmax(x, axis, where, initial)
-  return y, y * (x_dot - (y * x_dot).sum(axis, where=where, keepdims=True))
 
-
-@partial(jax.jit, static_argnames=("axis",))
 def standardize(x: ArrayLike,
                 axis: int | tuple[int, ...] | None = -1,
                 variance: ArrayLike | None = None,
                 epsilon: ArrayLike = 1e-5,
                 where: ArrayLike | None = None) -> jax.Array:
   r"""Normalizes an array by subtracting ``mean`` and dividing by :math:`\sqrt{\mathrm{variance}}`."""
-  mean = jnp.mean(x, axis, keepdims=True, where=where)
-  if variance is None:
-    # this definition is traditionally seen as less accurate than jnp.var's
-    # mean((x - mean(x))**2) but may be faster and even, given typical
-    # activation distributions and low-precision arithmetic, more accurate
-    # when used in neural network normalization layers
-    variance = jnp.mean(
-      jnp.square(x), axis, keepdims=True, where=where) - jnp.square(mean)
-  return jnp.subtract(x, jnp.asarray(mean)) * lax.rsqrt(jnp.asarray(variance) + epsilon)
-
-
-@partial(jax.jit, static_argnames=("num_classes", "dtype", "axis"))
-def _one_hot(x: Any,
-             num_classes: int, *,
-             dtype: Any,
-             axis: Union[int, Sequence[int]]) -> jax.Array:
-  num_classes = jax.core.concrete_dim_or_error(
-    num_classes,
-    "The error arose in jax.nn.one_hot argument `num_classes`.")
-  dtype = jax.dtypes.canonicalize_dtype(dtype)
-  x_arr = jnp.asarray(x)
-  try:
-    output_pos_axis = canonicalize_axis(axis, x_arr.ndim + 1)
-  except TypeError:
-    axis_size = lax.psum(1, axis)
-    if num_classes != axis_size:
-      raise ValueError(f"Expected num_classes to match the size of axis {axis}, "
-                       f"but {num_classes} != {axis_size}") from None
-    axis_idx = lax.axis_index(axis)
-    return jnp.asarray(x_arr == axis_idx, dtype=dtype)
-  axis = operator.index(axis)  # type: ignore[arg-type]
-  lhs = lax.expand_dims(x_arr, (axis,))
-  rhs_shape = [1] * x_arr.ndim
-  rhs_shape.insert(output_pos_axis, num_classes)
-  rhs = lax.broadcasted_iota(x_arr.dtype, rhs_shape, output_pos_axis)
-  return jnp.asarray(lhs == rhs, dtype=dtype)
+  return jax.nn.standardize(x, axis, variance, epsilon, where)
 
 
 def one_hot(x: Any,
             num_classes: int, *,
             dtype: Any = jnp.float_,
             axis: Union[int, Sequence[int]] = -1) -> jax.Array:
   """One-hot encodes the given indices.
@@ -739,22 +650,17 @@
   Args:
     x: A tensor of indices.
     num_classes: Number of classes in the one-hot dimension.
     dtype: optional, a float dtype for the returned values (default :obj:`jnp.float_`).
     axis: the axis or axes along which the function should be
       computed.
   """
-  num_classes = jax.core.concrete_dim_or_error(
-    num_classes,
-    "The error arose in jax.nn.one_hot argument `num_classes`.")
-  return _one_hot(x, num_classes, dtype=dtype, axis=axis)
+  return jax.nn.one_hot(x, num_classes, dtype=dtype, axis=axis)
 
 
-@jax.custom_jvp
-@jax.jit
 def relu6(x: ArrayLike) -> jax.Array:
   r"""Rectified Linear Unit 6 activation function.
 
   Computes the element-wise function
 
   .. math::
     \mathrm{relu6}(x) = \min(\max(x, 0), 6)
@@ -774,22 +680,17 @@
 
   Returns:
     An array.
 
   See also:
     :func:`relu`
   """
-  return jnp.minimum(jnp.maximum(x, 0), 6.)
-
-
-relu6.defjvps(lambda g, ans, x:
-              lax.select((x > 0) & (x < 6), g, lax.full_like(g, 0)))
+  return jax.nn.relu6(x)
 
 
-@jax.jit
 def hard_sigmoid(x: ArrayLike) -> jax.Array:
   r"""Hard Sigmoid activation function.
 
   Computes the element-wise function
 
   .. math::
     \mathrm{hard\_sigmoid}(x) = \frac{\mathrm{relu6}(x + 3)}{6}
@@ -799,18 +700,17 @@
 
   Returns:
     An array.
 
   See also:
     :func:`relu6`
   """
-  return relu6(x + 3.) / 6.
+  return jax.nn.hard_sigmoid(x)
 
 
-@jax.jit
 def hard_silu(x: ArrayLike) -> jax.Array:
   r"""Hard SiLU (swish) activation function
 
   Computes the element-wise function
 
   .. math::
     \mathrm{hard\_silu}(x) = x \cdot \mathrm{hard\_sigmoid}(x)
@@ -823,12 +723,11 @@
 
   Returns:
     An array.
 
   See also:
     :func:`hard_sigmoid`
   """
-  x_arr = jnp.asarray(x)
-  return x_arr * hard_sigmoid(x_arr)
+  return jax.nn.hard_silu(x)
 
 
 hard_swish = hard_silu
```

## braintools/functional/_spikes.py

```diff
@@ -1,7 +1,24 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+from __future__ import annotations
+
 __all__ = [
   'spike_bitwise_or',
   'spike_bitwise_and',
   'spike_bitwise_iand',
   'spike_bitwise_not',
   'spike_bitwise_xor',
   'spike_bitwise_ixor',
```

## braintools/init/__init__.py

```diff
@@ -1,7 +1,23 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+
 from ._base import *
 from ._base import __all__ as _base_all
 from ._generic import *
 from ._generic import __all__ as _generic_all
 from ._random_inits import *
 from ._random_inits import __all__ as _random_inits_all
 from ._regular_inits import *
```

## braintools/init/_base.py

```diff
@@ -1,8 +1,36 @@
-__all__ = ['Initializer']
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+
+from typing import Optional, Tuple
+
+import numpy as np
+
+__all__ = ['Initializer', 'to_size']
 
 
 class Initializer(object):
   def __call__(self, *args, **kwargs):
     raise NotImplementedError
 
 
+def to_size(x) -> Optional[Tuple[int]]:
+  if isinstance(x, (tuple, list)):
+    return tuple(x)
+  if isinstance(x, (int, np.integer)):
+    return (x,)
+  if x is None:
+    return x
+  raise ValueError(f'Cannot make a size for {x}')
```

## braintools/init/_generic.py

```diff
@@ -1,46 +1,66 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
 # -*- coding: utf-8 -*-
 
+import numbers
 from typing import Union, Callable, Optional, Sequence
 
 import braincore as bc
 import jax
 import jax.numpy as jnp
 import numpy as np
-from brainpy.tools import to_size
+
+from ._base import to_size
 
 __all__ = [
   'parameter',
   'state',
   'noise',
 ]
 
 
 def _is_scalar(x):
-  return isinstance(x, (float, int, bool, complex))
+  return isinstance(x, numbers.Number)
 
 
 def parameter(
     param: Union[Callable, np.ndarray, jax.Array, float, int, bool],
     sizes: Union[int, Sequence[int]],
+    batch_or_mode: Optional[Union[int, bool, bc.mixin.Mode]] = None,
     allow_none: bool = True,
     allow_scalar: bool = True,
 ):
   """Initialize parameters.
 
   Parameters
   ----------
   param: callable, Initializer, bm.ndarray, jnp.ndarray, onp.ndarray, float, int, bool
     The initialization of the parameter.
     - If it is None, the created parameter will be None.
     - If it is a callable function :math:`f`, the ``f(size)`` will be returned.
-    - If it is an instance of :py:class:`brainonline.init.Initializer``, the ``f(size)`` will be returned.
+    - If it is an instance of :py:class:`init.Initializer``, the ``f(size)`` will be returned.
     - If it is a tensor, then this function check whether ``tensor.shape`` is equal to the given ``size``.
   sizes: int, sequence of int
     The shape of the parameter.
+  batch_or_mode: int, bool, braincore.mixin.Mode
+    The batch size or the mode.
   allow_none: bool
     Whether allow the parameter is None.
   allow_scalar: bool
     Whether allow the parameter is a scalar value.
 
   Returns
   -------
@@ -53,58 +73,79 @@
   """
   if param is None:
     if allow_none:
       return None
     else:
       raise ValueError(f'Expect a parameter with type of float, ArrayType, Initializer, or '
                        f'Callable function, but we got None. ')
-  sizes = to_size(sizes)
+  sizes = list(to_size(sizes))
   if allow_scalar and _is_scalar(param):
     return param
 
+  batch_axis = None
+  batch_size = None
+  if isinstance(batch_or_mode, bc.mixin.Mode):
+    if batch_or_mode.has(bc.mixin.Batching):
+      batch_axis = batch_or_mode.batch_axis
+      batch_size = batch_or_mode.batch_size
+  elif batch_or_mode in (None, False):
+    pass
+  elif isinstance(batch_or_mode, int):
+    batch_axis = 0
+    batch_size = batch_or_mode
+  else:
+    raise ValueError('Unknown batch_or_mode.')
+  if batch_size is not None:
+    sizes.insert(batch_axis, batch_size)
+
   if callable(param):
-    return param(*sizes)
-  elif isinstance(param, np.ndarray):
+    return param(sizes)
+  elif isinstance(param, (np.ndarray, jax.Array)):
     param = jnp.asarray(param)
+    if batch_size is not None:
+      param = jnp.repeat(jnp.expand_dims(param, axis=batch_axis), batch_size, axis=batch_axis)
   elif isinstance(param, bc.State):
     param = param
+    if batch_size is not None:
+      param = bc.State(jnp.repeat(jnp.expand_dims(param.value, axis=batch_axis), batch_size, axis=batch_axis))
   else:
-    param = param
+    raise ValueError(f'Unknown parameter type: {type(param)}')
 
   if allow_scalar:
     if param.shape == () or param.shape == (1,):
       return param
-  if param.shape != sizes:
+  if param.shape != tuple(sizes):
     raise ValueError(f'The shape of the parameters should be {sizes}, but we got {param.shape}')
   return param
 
 
 def state(
     init: Union[Callable, np.ndarray, jax.Array],
     sizes: Union[int, Sequence[int]] = None,
     batch_or_mode: Optional[Union[int, bool, bc.mixin.Mode]] = None,
 ):
   """
   Initialize a :math:`~.State` from a callable function or a data.
   """
-  sizes = list(to_size(sizes))
+  sizes = to_size(sizes)
 
   if callable(init):
     if sizes is None:
       raise ValueError('"varshape" cannot be None when data is a callable function.')
+    sizes = list(sizes)
     if isinstance(batch_or_mode, bc.mixin.Mode):
       if batch_or_mode.has(bc.mixin.Batching):
         sizes.insert(batch_or_mode.batch_axis, batch_or_mode.batch_size)
     elif batch_or_mode in (None, False):
       pass
     elif isinstance(batch_or_mode, int):
       sizes.insert(0, batch_or_mode)
     else:
       raise ValueError(f'Unknown batch_size_or_mode: {batch_or_mode}')
-    data = bc.State(init(*sizes))
+    return bc.State(init(sizes))
 
   else:
     if sizes is not None:
       if jnp.shape(init) != sizes:
         raise ValueError(f'The shape of "data" {jnp.shape(init)} does not match with "var_shape" {sizes}')
     batch_axis = None
     batch_size = None
```

## braintools/init/_random_inits.py

```diff
@@ -1,16 +1,31 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
 # -*- coding: utf-8 -*-
 
 import math
 
 import braincore as bc
 import jax.numpy as jnp
 import numpy as np
 
-from ._base import Initializer
+from ._base import Initializer, to_size
 
 __all__ = [
   'Normal',
   'TruncatedNormal',
   'Uniform',
   'VarianceScaling',
   'KaimingUniform',
@@ -109,15 +124,16 @@
 
   def __init__(self, mean=0., scale=1., dtype=None):
     super(Normal, self).__init__()
     self.scale = scale
     self.mean = mean
     self.dtype = dtype or bc.environ.dftype()
 
-  def __call__(self, *shape):
+  def __call__(self, shape):
+    shape = to_size(shape)
     weights = bc.random.normal(size=shape, loc=self.mean, scale=self.scale, dtype=self.dtype)
     return weights
 
   def __repr__(self):
     return f'{self.__class__.__name__}(scale={self.scale}, dtype={self.dtype})'
 
 
@@ -146,15 +162,15 @@
     assert scale > 0, '`scale` must be positive.'
     self.scale = scale
     self.loc = loc
     self.lower = lower
     self.upper = upper
     self.dtype = dtype or bc.environ.dftype()
 
-  def __call__(self, *shape):
+  def __call__(self, shape):
     weights = bc.random.truncated_normal(
       size=shape,
       scale=self.scale,
       lower=self.lower,
       upper=self.upper,
       loc=self.loc,
       dtype=self.dtype
@@ -179,15 +195,16 @@
   """
 
   def __init__(self, shape, scale=None, dtype=None):
     self.shape = shape
     self.scale = scale
     self.dtype = dtype or bc.environ.dftype()
 
-  def __call__(self, *shape):
+  def __call__(self, shape):
+    shape = to_size(shape)
     weights = bc.random.gamma(self.shape, scale=self.scale, size=shape, dtype=self.dtype)
     return weights
 
   def __repr__(self):
     return f'{self.__class__.__name__}(shape={self.shape}, scale={self.scale}, dtype={self.dtype})'
 
 
@@ -201,15 +218,16 @@
 
   """
 
   def __init__(self, scale=None, dtype=None):
     self.scale = scale
     self.dtype = dtype or bc.environ.dftype()
 
-  def __call__(self, *shape):
+  def __call__(self, shape):
+    shape = to_size(shape)
     weights = bc.random.exponential(scale=self.scale, size=shape, dtype=self.dtype)
     return weights
 
   def __repr__(self):
     return f'{self.__class__.__name__}(scale={self.scale}, dtype={self.dtype})'
 
 
@@ -226,15 +244,16 @@
 
   def __init__(self, min_val: float = 0., max_val: float = 1., dtype=None):
     super(Uniform, self).__init__()
     self.min_val = min_val
     self.max_val = max_val
     self.dtype = dtype or bc.environ.dftype()
 
-  def __call__(self, *shape):
+  def __call__(self, shape):
+    shape = to_size(shape)
     return bc.random.uniform(low=self.min_val, high=self.max_val, size=shape, dtype=self.dtype)
 
   def __repr__(self):
     return (f'{self.__class__.__name__}(min_val={self.min_val}, '
             f'max_val={self.max_val}, dtype={self.dtype})')
 
 
@@ -253,15 +272,16 @@
     self.scale = scale
     self.mode = mode
     self.in_axis = in_axis
     self.out_axis = out_axis
     self.distribution = distribution
     self.dtype = dtype or bc.environ.dftype()
 
-  def __call__(self, *shape):
+  def __call__(self, shape):
+    shape = to_size(shape)
     fan_in, fan_out = _compute_fans(shape, in_axis=self.in_axis, out_axis=self.out_axis)
     if self.mode == "fan_in":
       denominator = fan_in
     elif self.mode == "fan_out":
       denominator = fan_out
     elif self.mode == "fan_avg":
       denominator = (fan_in + fan_out) / 2
@@ -410,15 +430,16 @@
       dtype=None
   ):
     super().__init__()
     self.scale = scale
     self.axis = axis
     self.dtype = dtype or bc.environ.dftype()
 
-  def __call__(self, *shape):
+  def __call__(self, shape):
+    shape = to_size(shape)
     n_rows = shape[self.axis]
     n_cols = np.prod(shape) // n_rows
     matrix_shape = (n_rows, n_cols) if n_rows > n_cols else (n_cols, n_rows)
     norm_dst = bc.random.normal(size=matrix_shape, dtype=self.dtype)
     q_mat, r_mat = jnp.linalg.qr(norm_dst)
     # Enforce Q is uniformly distributed
     q_mat *= jnp.sign(jnp.diag(r_mat))
@@ -441,15 +462,16 @@
 
   def __init__(self, scale=1.0, axis=-1, dtype=None):
     super(DeltaOrthogonal, self).__init__()
     self.scale = scale
     self.axis = axis
     self.dtype = dtype or bc.environ.dftype()
 
-  def __call__(self, *shape):
+  def __call__(self, shape):
+    shape = to_size(shape)
     if len(shape) not in [3, 4, 5]:
       raise ValueError("Delta orthogonal initializer requires a 3D, 4D or 5D shape.")
     if shape[-1] < shape[-2]:
       raise ValueError("`fan_in` must be less or equal than `fan_out`. ")
     ortho_matrix = Orthogonal(scale=self.scale, axis=self.axis, dtype=self.dtype)(*shape[-2:])
     W = jnp.zeros(shape, dtype=self.dtype)
     if len(shape) == 3:
```

## braintools/init/_regular_inits.py

```diff
@@ -1,13 +1,28 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
 # -*- coding: utf-8 -*-
 
 import braincore as bc
 import jax.numpy as jnp
 
-from ._base import Initializer
+from ._base import Initializer, to_size
 
 __all__ = [
   'ZeroInit',
   'Constant',
   'Identity',
 ]
 
@@ -18,15 +33,16 @@
   Initialize the weights with zeros.
   """
 
   def __init__(self, dtype=None):
     super(ZeroInit, self).__init__()
     self.dtype = dtype or bc.environ.dftype()
 
-  def __call__(self, *shape):
+  def __call__(self, shape):
+    shape = to_size(shape)
     return jnp.zeros(shape, dtype=self.dtype)
 
   def __repr__(self):
     return f"{self.__class__.__name__}(dtype={self.dtype})"
 
 
 class Constant(Initializer):
@@ -41,15 +57,16 @@
   """
 
   def __init__(self, value=1., dtype=None):
     super(Constant, self).__init__()
     self.dtype = dtype or bc.environ.dftype()
     self.value = jnp.asarray(value, dtype=self.dtype)
 
-  def __call__(self, *shape):
+  def __call__(self, shape):
+    shape = to_size(shape)
     return jnp.full(shape, self.value, dtype=self.dtype)
 
   def __repr__(self):
     return f'{self.__class__.__name__}(value={self.value}, dtype={self.dtype})'
 
 
 class Identity(Initializer):
@@ -75,17 +92,18 @@
   """
 
   def __init__(self, value=1., dtype=None):
     super(Identity, self).__init__()
     self.dtype = dtype or bc.environ.dftype()
     self.value = jnp.asarray(value, dtype=self.dtype)
 
-  def __call__(self, *shape):
+  def __call__(self, shape):
+    shape = to_size(shape)
     if isinstance(shape, (tuple, list)):
       if len(shape) > 2:
         raise ValueError(f'Only support initialize 2D weights for {self.__class__.__name__}.')
-    r = jnp.eye(*shape, dtype=self.dtype)
+    r = jnp.eye(shape, dtype=self.dtype)
     r = jnp.fill_diagonal(r, self.value)
     return r
 
   def __repr__(self):
     return f'{self.__class__.__name__}(value={self.value}, dtype={self.dtype})'
```

## braintools/optim/__init__.py

```diff
@@ -1,7 +1,22 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
 
 from ._lr_scheduler import *
 from ._lr_scheduler import __all__ as scheduler_all
 from ._sgd_optimizer import *
 from ._sgd_optimizer import __all__ as optimizer_all
 
 __all__ = scheduler_all + optimizer_all
```

## braintools/optim/_lr_scheduler.py

```diff
@@ -1,22 +1,33 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
 # -*- coding: utf-8 -*-
 
 from typing import Sequence, Union
 
 import braincore as bc
 import jax
 import jax.numpy as jnp
-from brainpy import check
-from brainpy.errors import MathError
-
 
 __all__ = [
-  'LRScheduler',
-  'Constant',
-  'CallBasedLRScheduler',
+  'LearningRateScheduler',
+  'ConstantLR',
   'StepLR',
   'MultiStepLR',
   'CosineAnnealingLR',
   'CosineAnnealingWarmRestarts',
   'ExponentialLR',
   'ExponentialDecayLR',
   'InverseTimeDecayLR',
@@ -26,41 +37,41 @@
 
 
 # learning rate schedules #
 # ----------------------- #
 
 
 def make_schedule(scalar_or_schedule):
-  if isinstance(scalar_or_schedule, LRScheduler):
+  if isinstance(scalar_or_schedule, LearningRateScheduler):
     return scalar_or_schedule
   elif isinstance(scalar_or_schedule, (int, float, bc.State)):
-    return Constant(scalar_or_schedule)
+    return ConstantLR(scalar_or_schedule)
   else:
     raise TypeError(type(scalar_or_schedule))
 
 
-class LRScheduler(bc.Module):
+class LearningRateScheduler(bc.Module):
   """
   The learning rate scheduler.
 
   Attributes
   ----------
   lr: float, State
     The learning rate.
   last_epoch: int
     The index of last epoch.
 
   """
 
   def __init__(self, lr: Union[float, bc.State], last_epoch: int = -1):
-    super(LRScheduler, self).__init__()
+    super(LearningRateScheduler, self).__init__()
     if isinstance(lr, bc.State):
-      lr.value = jnp.asarray(lr.value, dtype=bc.environ.ditype())
+      lr.value = jnp.asarray(lr.value, dtype=bc.environ.dftype())
     else:
-      lr = jnp.asarray(lr, dtype=bc.environ.ditype())
+      lr = jnp.asarray(lr, dtype=bc.environ.dftype())
     self._lr = lr
     assert last_epoch >= -1, 'last_epoch should be greater than -1.'
     self.last_epoch = bc.State(jnp.asarray(last_epoch, dtype=bc.environ.ditype()))
 
   @property
   def lr(self):
     return self._lr.value if isinstance(self._lr, bc.State) else self._lr
@@ -93,36 +104,38 @@
   def extra_repr(self):
     return ''
 
   def __call__(self, i=None):
     raise NotImplementedError
 
 
-class Constant(LRScheduler):
+class ConstantLR(LearningRateScheduler):
   """
   Constant learning rate scheduler.
   """
+
   def __call__(self, i=None):
     return self.lr
 
 
-class CallBasedLRScheduler(LRScheduler):
+class CallBasedLRScheduler(LearningRateScheduler):
   """
   The learning rate scheduler based on the call count.
 
   Attributes
   ----------
   lr: float
     The learning rate.
   last_epoch: int
     The index of last epoch.
   last_call: int
     The index of last call.
 
   """
+
   def __init__(self, lr: Union[float, bc.State], last_epoch: int = -1, last_call: int = -1):
     super().__init__(lr=lr, last_epoch=last_epoch)
 
     assert last_call >= -1, 'last_call should be greater than -1.'
     self.last_call = bc.State(jnp.asarray(last_call, dtype=bc.environ.ditype()))
 
   def step_call(self):
@@ -133,15 +146,15 @@
 
   def __repr__(self):
     return (f'{self.__class__.__name__}(lr={self.lr.value}, '
             f'last_epoch={self.last_epoch.value}, '
             f'last_call={self.last_call.value}{self.extra_repr()})')
 
 
-class StepLR(LRScheduler):
+class StepLR(LearningRateScheduler):
   """Decays the learning rate of each parameter group by gamma every
   `step_size` epochs.
 
   Parameters
   ----------
   lr: float
     Initial learning rate.
@@ -159,26 +172,28 @@
       lr: float,
       step_size: int,
       gamma: float = 0.1,
       last_epoch: int = -1
   ):
     super().__init__(lr=lr, last_epoch=last_epoch)
 
-    self.step_size = check.is_integer(step_size, min_bound=1, allow_none=False)
-    self.gamma = check.is_float(gamma, min_bound=0., max_bound=1., allow_int=False)
+    assert step_size >= 1, 'step_size should be greater than or equal to 1.'
+    assert 1. >= gamma >= 0, 'gamma should be in the range [0, 1].'
+    self.step_size = step_size
+    self.gamma = gamma
 
   def __call__(self, i=None):
     i = (self.last_epoch.value + 1) if i is None else i
     return self.lr * self.gamma ** (jnp.floor_divide(i, self.step_size))
 
   def extra_repr(self):
     return f', gamma={self.gamma}, step_size={self.step_size}'
 
 
-class MultiStepLR(LRScheduler):
+class MultiStepLR(LearningRateScheduler):
   """Decays the learning rate of each parameter group by gamma once the
   number of epoch reaches one of the milestones. Notice that such decay can
   happen simultaneously with other changes to the learning rate from outside
   this scheduler. When last_epoch=-1, sets initial lr as lr.
 
   Parameters
   ----------
@@ -198,28 +213,33 @@
       lr: float,
       milestones: Sequence[int],
       gamma: float = 0.1,
       last_epoch: int = -1
   ):
     super().__init__(lr=lr, last_epoch=last_epoch)
 
-    self.milestones = check.is_sequence(milestones, elem_type=int, allow_none=False)
-    self.gamma = check.is_float(gamma, min_bound=0., max_bound=1., allow_int=False)
+    assert len(milestones) > 0, 'milestones should be a non-empty sequence.'
+    assert all([milestones[i] < milestones[i + 1] for i in range(len(milestones) - 1)]), (
+      'milestones should be a sequence of increasing integers.'
+    )
+    assert 1. >= gamma >= 0, 'gamma should be in the range [0, 1].'
+    self.milestones = jnp.asarray((-1,) + tuple(milestones), dtype=bc.environ.ditype())
+    self.gamma = gamma
 
   def __call__(self, i=None):
     i = (self.last_epoch.value + 1) if i is None else i
-    p = bc.ifelse([i < m for m in self.milestones],
-                  list(range(0, len(self.milestones))) + [len(self.milestones)])
+    conditions = (i > self.milestones[:-1]) & i < self.milestones[1:]
+    p = jnp.where(conditions, jnp.arange(0, len(self.milestones)))
     return self.lr * self.gamma ** p
 
   def extra_repr(self):
     return f', milestones={self.milestones}, gamma={self.gamma}'
 
 
-class CosineAnnealingLR(LRScheduler):
+class CosineAnnealingLR(LearningRateScheduler):
   r"""Set the learning rate of each parameter group using a cosine annealing
   schedule, where :math:`\eta_{max}` is set to the initial lr and
   :math:`T_{cur}` is the number of epochs since the last restart in SGDR:
 
   .. math::
       \begin{aligned}
           \eta_t & = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1
@@ -263,21 +283,22 @@
       lr: float,
       T_max: int,
       eta_min: float = 0.,
       last_epoch: int = -1,
   ):
     super().__init__(lr=lr, last_epoch=last_epoch)
 
+    assert T_max >= 1, 'T_max should be greater than or equal to 1.'
     self._init_epoch = last_epoch
-    self.T_max = check.is_integer(T_max, min_bound=1)
+    self.T_max = T_max
     self.eta_min = eta_min
 
   def __call__(self, i=None):
     i = (self.last_epoch.value + 1) if i is None else i
-    return (self.eta_min + (self.lr - self.eta_min) * (1 + jnp.cos(jnp.pi * i / self.T_max)) / 2)
+    return self.eta_min + (self.lr - self.eta_min) * (1 + jnp.cos(jnp.pi * i / self.T_max)) / 2
 
   def extra_repr(self):
     return f', T_max={self.T_max}, eta_min={self.eta_min}'
 
 
 class CosineAnnealingWarmRestarts(CallBasedLRScheduler):
   """Set the learning rate of each parameter group using a cosine annealing
@@ -358,15 +379,15 @@
     i = (self.last_call.value + 1) if i is None else i
     return jnp.floor(i / self.num_call_per_epoch)
 
   def extra_repr(self):
     return f', T_0={self.T_0}, T_mult={self.T_mult}, eta_min={self.eta_min}'
 
 
-class ExponentialLR(LRScheduler):
+class ExponentialLR(LearningRateScheduler):
   """Decays the learning rate of each parameter group by gamma every epoch.
   When last_epoch=-1, sets initial lr as lr.
 
   Parameters
   ----------
   lr: float
     Initial learning rate.
@@ -377,15 +398,16 @@
   """
 
   def __init__(self,
                lr: float,
                gamma: float,
                last_epoch: int = -1):
     super(ExponentialLR, self).__init__(lr=lr, last_epoch=last_epoch)
-    self.gamma = check.is_float(gamma, min_bound=0., max_bound=1.)
+    assert 1. >= gamma >= 0, 'gamma should be in the range [0, 1].'
+    self.gamma = gamma
 
   def __call__(self, i: int = None):
     i = (self.last_epoch.value + 1) if i is None else i
     return self.lr * self.gamma ** i
 
   def extra_repr(self):
     return f', gamma={self.gamma}'
@@ -442,17 +464,17 @@
 class PiecewiseConstantLR(CallBasedLRScheduler):
   def __init__(self, boundaries, values, last_epoch: int = -1, last_call: int = -1):
     super().__init__(0., last_epoch=last_epoch, last_call=last_call)
 
     boundaries = jnp.array(boundaries)
     values = jnp.array(values)
     if not boundaries.ndim == values.ndim == 1:
-      raise MathError("boundaries and values must be sequences")
+      raise ValueError("boundaries and values must be sequences")
     if not boundaries.shape[0] == values.shape[0] - 1:
-      raise MathError("boundaries length must be one shorter than values length")
+      raise ValueError("boundaries length must be one shorter than values length")
     self.boundaries = boundaries
     self.values = values
 
   def __call__(self, i=None):
     i = (self.last_call.value + 1) if i is None else i
     return self.values[jnp.sum(i > self.boundaries)]
```

## braintools/optim/_sgd_optimizer.py

```diff
@@ -1,23 +1,40 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
 # -*- coding: utf-8 -*-
 
-import copy
 import functools
 from typing import Union, Dict, Optional, Tuple, Any, TypeVar
 
 import braincore as bc
 import jax
 import jax.numpy as jnp
-from brainpy import check
 
-from ._lr_scheduler import make_schedule, LRScheduler
+from ._lr_scheduler import make_schedule, LearningRateScheduler
 
 __all__ = [
   'to_same_dict_tree',
+
+  # new class of braincore.State for optimizer
   'OptimState',
+
+  # commonly used optimizers
   'Optimizer',
   'SGD',
   'Momentum',
   'MomentumNesterov',
   'Adagrad',
   'Adadelta',
   'RMSProp',
@@ -36,92 +53,108 @@
   return jnp.asarray(value, dtype=dtype)
 
 
 def fcast(value: T, dtype: Any = None) -> jax.Array:
   return cast(value, dtype=dtype or bc.environ.dftype())
 
 
-def to_same_dict_tree(*dicts: dict):
+def _to_dict_value(old_dict: Dict) -> Dict:
+  new_dict = dict()
+  for k, v in old_dict.items():
+    if isinstance(v, bc.State):
+      new_dict[k] = v.value
+    else:
+      new_dict[k] = v
+  return new_dict
+
+
+def to_same_dict_tree(*dicts: Dict):
   """
   Convert multiple dictionaries to the same tree structure.
 
   Parameters
   ----------
   *dicts: dict
     The dictionaries to be converted.
 
   Returns
   -------
   dict
     The converted dictionary.
   """
   if len(dicts):
-    dicts = tuple(copy.copy(d) for d in dicts)
-
     # all keys
     all_keys = tuple(set(d.keys()) for d in dicts)
     for keys in all_keys[1:]:
       if len(all_keys[0].difference(keys)) > 0:
         raise ValueError('Dictionary does not match.')
 
     # flatten to normal python dict
-    r = [dict() for _ in range(len(dicts))]
-    for i in range(len(dicts)):
-      for k in all_keys[0]:
-        v = dicts[i][k]
-        if isinstance(v, bc.State):
-          r[i][k] = v.value
-        else:
-          r[i][k] = v
+    r = [_to_dict_value(d) for d in dicts]
+
     if len(dicts) == 1:
       return r[0]
     else:
       return tuple(r)
 
 
-def _sgd(pv, gd, weight_decay, lr=None):
+def _sgd(prev_weight, gradient, weight_decay, lr=None):
+  """
+  The update function for SGD learning.
+
+  Parameters
+  ----------
+  prev_weight: jax.Array
+    The previous weight.
+  gradient: jax.Array
+    The gradient.
+  weight_decay: float
+    The weight decay.
+  lr: float
+    The learning rate.
+  """
   if weight_decay is None:
     if lr is None:
-      return pv - gd
+      return prev_weight - gradient
     else:
-      return pv - lr * gd
+      return prev_weight - lr * gradient
   else:
     if lr is None:
-      return (1 - weight_decay) * pv - gd
+      return (1 - weight_decay) * prev_weight - gradient
     else:
-      return (1 - weight_decay) * pv - lr * gd
+      return (1 - weight_decay) * prev_weight - lr * gradient
 
 
 class OptimState(bc.LongTermState):
   """
   The state for optimizer.
   """
   pass
 
 
 class Optimizer(bc.Module):
   """Base Optimizer Class.
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate.
   """
 
-  lr: LRScheduler  # learning rate
-  weight_states: bc.StateStack  # states to train, invisible to ``.states()``
+  lr: LearningRateScheduler  # learning rate
+  weight_states: bc.StateDictManager  # states to train, invisible to ``.states()``
 
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State],
+      lr: Union[float, LearningRateScheduler, bc.State],
       name: Optional[str] = None
   ):
     super().__init__(name=name)
-    self.lr: LRScheduler = make_schedule(lr)
-    self.weight_states = bc.StateStack()
+    self.lr: LearningRateScheduler = make_schedule(lr)
+    self.weight_states = bc.StateDictManager()
 
   def register_trainable_weights(self, train_states: Optional[Dict[str, bc.State]] = None):
     raise NotImplementedError
 
   def __repr__(self):
     return f"{self.__class__.__name__}(lr={self.lr}{self.extra_repr()})"
 
@@ -131,21 +164,21 @@
   def update(self, grads: dict):
     raise NotImplementedError
 
 
 class _WeightDecayOptimizer(Optimizer):
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State],
+      lr: Union[float, LearningRateScheduler, bc.State],
       weight_decay: Optional[float] = None,
       name: Optional[str] = None
   ):
     super().__init__(lr=lr, name=name)
-    self.lr: LRScheduler = make_schedule(lr)
-    weight_decay = check.is_float(weight_decay, min_bound=0., max_bound=1., allow_none=True)
+    self.lr: LearningRateScheduler = make_schedule(lr)
+    assert weight_decay is None or 0. <= weight_decay <= 1., 'weight_decay must be in [0, 1].'
     self.weight_decay = (fcast(weight_decay) if weight_decay is not None else None)
 
   def extra_repr(self) -> str:
     return ''
 
   def __repr__(self):
     return f"{self.__class__.__name__}(lr={self.lr}, weight_decay={self.weight_decay}{self.extra_repr()})"
@@ -160,28 +193,26 @@
   .. math::
 
       \theta = \theta - \eta \cdot \nabla_\theta J(\theta; x; y)
 
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate.
 
   """
 
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State],
+      lr: Union[float, LearningRateScheduler, bc.State],
       weight_decay: Optional[float] = None,
       name: Optional[str] = None
   ):
-    super(SGD, self).__init__(lr=lr,
-                              weight_decay=weight_decay,
-                              name=name)
+    super().__init__(lr=lr, weight_decay=weight_decay,  name=name)
 
   def register_trainable_weights(self, states: Optional[Dict[str, bc.State]] = None):
     states = dict() if states is None else states
     assert isinstance(states, dict), '"states" must be a dict of braincore.State.'
     for k, v in states.items():
       assert isinstance(v, bc.State), f'"{k}" must be an instance of braincore.State.'
       self.weight_states.add_unique_elem(k, v)
@@ -210,29 +241,29 @@
     v_t &= \gamma v_{t-1} + \eta \nabla_\theta J( \theta) \\
     \theta &= \theta - v_t
     \end{split}
     \end{align}
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate.
 
   References
   ----------
 
   .. [1] Qian, N. (1999). On the momentum term in gradient descent learning
          algorithms. Neural Networks : The Official Journal of the International
          Neural Network Society, 12(1), 145151. http://doi.org/10.1016/S0893-6080(98)00116-6
 
   """
 
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State],
+      lr: Union[float, LearningRateScheduler, bc.State],
       momentum: float = 0.9,
       weight_decay: Optional[float] = None,
       name: Optional[str] = None
   ):
     super(Momentum, self).__init__(lr=lr, weight_decay=weight_decay, name=name)
     self.momentum = fcast(momentum)
     self.momentum_states = bc.visible_state_dict()
@@ -260,39 +291,40 @@
                                      momentum_values)
     self.momentum_states.assign_values(momentum_values)
     self.weight_states.assign_values(new_weight_values)
     self.lr.step_call()
 
 
 class MomentumNesterov(_WeightDecayOptimizer):
-  r"""Nesterov accelerated gradient optimizer [2]_.
+  r"""
+  Nesterov accelerated gradient optimizer [2]_.
 
   .. math::
 
       \begin{align}
       \begin{split}
       v_t &= \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} ) \\
       \theta &= \theta - v_t
       \end{split}
       \end{align}
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate.
 
   References
   ----------
   .. [2] Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543 547.
 
   """
 
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State],
+      lr: Union[float, LearningRateScheduler, bc.State],
       weight_decay: Optional[float] = None,
       momentum: float = 0.9,
       name: Optional[str] = None
   ):
     super(MomentumNesterov, self).__init__(lr=lr, weight_decay=weight_decay, name=name)
 
     self.momentum = fcast(momentum)
@@ -341,31 +373,31 @@
   Adagrad's main weakness is its accumulation of the squared gradients in the denominator:
   Since every added term is positive, the accumulated sum keeps growing during training.
   This in turn causes the learning rate to shrink and eventually become infinitesimally
   small, at which point the algorithm is no longer able to acquire additional knowledge.
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate.
 
   References
   ----------
   .. [3] Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 21212159. Retrieved from http://jmlr.org/papers/v12/duchi11a.html
 
   """
 
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State],
+      lr: Union[float, LearningRateScheduler, bc.State],
       weight_decay: Optional[float] = None,
       epsilon: float = 1e-6,
       name: Optional[str] = None
   ):
-    super(Adagrad, self).__init__(lr=lr, weight_decay=weight_decay, name=name)
+    super().__init__(lr=lr, weight_decay=weight_decay, name=name)
     self.epsilon = fcast(epsilon)
     self.cache_states = bc.visible_state_dict()
 
   def extra_repr(self) -> str:
     return f", epsilon={self.epsilon}"
 
   def register_trainable_weights(self, train_states: Optional[Dict[str, bc.State]] = None):
@@ -419,32 +451,32 @@
 
   In the paper, no learning rate is considered (so learning_rate=1.0). Probably best to
   keep it at this value. epsilon is important for the very first update (so the
   numerator does not become 0).
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate.
 
   References
   ----------
   .. [4] Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from http://arxiv.org/abs/1212.5701
 
   """
 
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State] = 0.01,
+      lr: Union[float, LearningRateScheduler, bc.State] = 0.01,
       weight_decay: Optional[float] = None,
       epsilon: float = 1e-6,
       rho: float = 0.95,
       name: Optional[str] = None
   ):
-    super(Adadelta, self).__init__(lr=lr, weight_decay=weight_decay, name=name)
+    super().__init__(lr=lr, weight_decay=weight_decay, name=name)
 
     self.epsilon = fcast(epsilon)
     self.rho = fcast(rho)
     self.cache_states = bc.visible_state_dict()
     self.delta_states = bc.visible_state_dict()
 
   def extra_repr(self) -> str:
@@ -492,27 +524,27 @@
     p_t &= \frac{\eta}{\sqrt{c_t + \epsilon}} * g \end{split}
 
   The centered version additionally maintains a moving average of the gradients,
   and uses that average to estimate the variance.
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate.
 
   References
   ----------
   .. [5] Tieleman, T. and Hinton, G. (2012):
          Neural Networks for Machine Learning, Lecture 6.5 - rmsprop.
          Coursera. http://www.youtube.com/watch?v=O3sxAc4hxZU (formula @5:20)
   """
 
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State],
+      lr: Union[float, LearningRateScheduler, bc.State],
       weight_decay: Optional[float] = None,
       epsilon: float = 1e-6,
       rho: float = 0.9,
       name: Optional[str] = None
   ):
     super(RMSProp, self).__init__(lr=lr, weight_decay=weight_decay, name=name)
 
@@ -549,15 +581,15 @@
 
   Adam [6]_ - a stochastic gradient descent method (SGD) that computes
   individual adaptive learning rates for different parameters from estimates of
   first- and second-order moments of the gradients.
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate.
   beta1: optional, float
     A positive scalar value for beta_1, the exponential decay rate
     for the first moment estimates (default 0.9).
   beta2: optional, float
     A positive scalar value for beta_2, the exponential decay rate
     for the second moment estimates (default 0.999).
@@ -570,15 +602,15 @@
   References
   ----------
   .. [6] Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
   """
 
   def __init__(
       self,
-      lr: Union[float, bc.State, LRScheduler],
+      lr: Union[float, bc.State, LearningRateScheduler],
       beta1: float = 0.9,
       beta2: float = 0.999,
       eps: float = 1e-8,
       weight_decay: Optional[float] = None,
       name: Optional[str] = None
   ):
     super(Adam, self).__init__(lr=lr,
@@ -635,15 +667,15 @@
   .. math::
 
      m_{t} = \beta_{1}m_{t-1} + \left(1-\beta_{1}\right)\left(g_{t} + \lambda{x_{t}}\right) \\
      x_{t+1}^{\left(i\right)} = x_{t}^{\left(i\right)}  - \eta_{t}\frac{\phi\left(|| x_{t}^{\left(i\right)} ||\right)}{|| m_{t}^{\left(i\right)} || }m_{t}^{\left(i\right)}
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate.
   momentum: float
     coefficient used for the moving average of the gradient.
   weight_decay: float
     weight decay coefficient.
   tc: float
     trust coefficient eta ( < 1) for trust ratio computation.
@@ -653,15 +685,15 @@
   References
   ----------
   .. [1] You, Yang, Igor Gitman and Boris Ginsburg. Large Batch Training of Convolutional Networks. arXiv: Computer Vision and Pattern Recognition (2017): n. pag.
   """
 
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State],
+      lr: Union[float, LearningRateScheduler, bc.State],
       momentum: float = 0.9,
       weight_decay: float = 1e-4,
       tc: float = 1e-3,
       eps: float = 1e-5,
       name: Optional[str] = None
   ):
     super(LARS, self).__init__(lr=lr,
@@ -717,15 +749,15 @@
       & \boldsymbol{\eta}_k=\eta /\left(\sqrt{\mathbf{n}_k+\varepsilon}\right)  \\
       & \boldsymbol{\theta}_{k+1}=\left(1+\lambda_k \eta\right)^{-1}\left[\boldsymbol{\theta}_k-\boldsymbol{\eta}_k \circ\left(\mathbf{m}_k+\left(1-\beta_2\right) \mathbf{v}_k\right)\right] \\
       \end{aligned}
       \end{equation}
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate. Can be much higher than Adam, up to 5-10x. (default: 1e-3)
   betas : tuple
      Coefficients used for computing running averages of gradient and its norm. (default: (0.02, 0.08, 0.01))
   eps : float
     The term added to the denominator to improve numerical stability. (default: 1e-8)
   weight_decay : float
     decoupled weight decay (L2 penalty) (default: 0)
@@ -747,15 +779,15 @@
   .. [1] Xie, Xingyu, Pan Zhou, Huan Li, Zhouchen Lin and Shuicheng Yan. 
          Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing 
          Deep Models. ArXiv abs/2208.06677 (2022): n. pag.
   """
 
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State] = 1e-3,
+      lr: Union[float, LearningRateScheduler, bc.State] = 1e-3,
       betas: Tuple[float, float, float] = (0.02, 0.08, 0.01),
       eps: float = 1e-8,
       weight_decay: float = 0.02,
       no_prox: bool = False,
       name: Optional[str] = None,
   ):
     super(Adan, self).__init__(lr=lr, weight_decay=weight_decay, name=name)
@@ -868,15 +900,15 @@
         &\bf{return} \:  \theta_t                                                     \\[-1.ex]
         &\rule{110mm}{0.4pt}                                                          \\[-1.ex]
      \end{aligned}
 
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate.
   beta1: optional, float
     A positive scalar value for beta_1, the exponential decay rate
     for the first moment estimates. Generally close to 1.
   beta2: optional, float
     A positive scalar value for beta_2, the exponential decay rate
     for the second moment estimates. Generally close to 1.
@@ -896,15 +928,15 @@
   ----------
   .. [1] Loshchilov, Ilya and Frank Hutter. Decoupled Weight Decay Regularization. International Conference on Learning Representations (2019).
 
   """
 
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State],
+      lr: Union[float, LearningRateScheduler, bc.State],
       beta1: float = 0.9,
       beta2: float = 0.999,
       eps: float = 1e-8,
       weight_decay: float = 1e-2,
       amsgrad: bool = False,
       name: Optional[str] = None,
   ):
@@ -997,15 +1029,15 @@
   This advantage drastically shrinks when `momentum > 0`. The momentum is
   tracked using a tensor of the same shape as the tensor being optimized. With
   momentum, SM3 will use just over half as much memory as Adam, and a bit more
   than Adagrad.
 
   Parameters
   ----------
-  lr: float, LRScheduler
+  lr: float, LearningRateScheduler
     learning rate.
   momentum: float
     coefficient used to scale prior updates
     before adding. This drastically increases memory usage if
     `momentum > 0.0`. (default: 0.0)
   beta: float
     coefficient used for exponential moving averages (default: 0.0)
@@ -1017,15 +1049,15 @@
   ----------
   .. [1] Anil, Rohan, Vineet Gupta, Tomer Koren and Yoram Singer. Memory Efficient Adaptive Optimization. Neural Information Processing Systems (2019).
 
   """
 
   def __init__(
       self,
-      lr: Union[float, LRScheduler, bc.State],
+      lr: Union[float, LearningRateScheduler, bc.State],
       beta: float = 0.,
       momentum: float = 0.,
       eps: float = 1e-30,
       weight_decay: Optional[float] = None,
       name: Optional[str] = None,
   ):
     super(SM3, self).__init__(lr=lr,
@@ -1075,15 +1107,15 @@
       update += g * g * (1 - self.beta)
       # Computes max along all dimensions except the given dim.
       # If tensor is a scalar, it returns tensor.
       for i in range(ndim):
         result = update
         for j in range(ndim):
           if i != j:
-            result = jnp.max(result, axis=j, keepdim=True)
+            result = jnp.maximum(result, axis=j, keepdim=True)
         acc = self.memory_states[f'{k}_m{i}'].value
         if self.beta > 0.:
           acc.value = jnp.maximum(acc, result)
         else:
           # No need to compare - nu_max is bigger because of grad ** 2
           acc.value = result
       update = g / jnp.sqrt(update + self.eps)
```

## Comparing `braintools/inputs/currents.py` & `braintools/input/currents.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+# Copyright 2024- BrainPy Ecosystem Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
 # -*- coding: utf-8 -*-
 
 
 import braincore as bc
 import jax.lax
 import jax.numpy as jnp
 import numpy as np
@@ -264,15 +279,15 @@
   i_start = int(t_start / dt)
   i_end = int(t_end / dt)
 
   def _f(x, key):
     x = x + dt * ((mean - x) / tau) + sigma * dt_sqrt * bc.random.rand(n, key=key)
     return x, x
 
-  noises = jax.lax.scan(_f, jnp.full(n, mean, dtype=bc.environ.dftype()), bc.random.split_keys(i_end - i_start))
+  _, noises = jax.lax.scan(_f, jnp.full(n, mean, dtype=bc.environ.dftype()), bc.random.split_keys(i_end - i_start))
   currents = jnp.zeros((int(duration / dt), n), dtype=bc.environ.dftype())
   return currents.at[i_start: i_end].set(noises)
 
 
 def sinusoidal_input(amplitude, frequency, duration, dt=None, t_start=0., t_end=None, bias=False):
   """Sinusoidal input.
```

## Comparing `braintools-0.0.1.dist-info/METADATA` & `braintools-0.0.2.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 Metadata-Version: 2.1
 Name: braintools
-Version: 0.0.1
+Version: 0.0.2
 Summary: The Toolbox for Brain Dynamics Programming.
 Home-page: https://github.com/brainpy/braintools
 Author: BrainPy Team
 Author-email: BrainPy Team <chao.brain@qq.com>
-License: GPL-3.0 license
+License: Apache-2.0 license
 Project-URL: homepage, http://github.com/brainpy/braintools
 Project-URL: repository, http://github.com/brainpy/braintools
 Keywords: BrainPy,brain simulation,brain-inspired computing
 Classifier: Natural Language :: English
 Classifier: Operating System :: OS Independent
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Developers
@@ -30,55 +30,64 @@
 License-File: LICENSE
 Requires-Dist: jax
 Requires-Dist: jaxlib
 Requires-Dist: numpy
 Requires-Dist: braincore
 Provides-Extra: cpu
 Requires-Dist: jaxlib ; extra == 'cpu'
-Requires-Dist: brainpylib ; extra == 'cpu'
 Provides-Extra: cpu_mini
 Requires-Dist: jaxlib ; extra == 'cpu_mini'
 Provides-Extra: cuda11
 Requires-Dist: jaxlib[cuda11_pip] ; extra == 'cuda11'
-Requires-Dist: brainpylib ; extra == 'cuda11'
 Provides-Extra: cuda11_mini
 Requires-Dist: jaxlib[cuda11_pip] ; extra == 'cuda11_mini'
 Provides-Extra: cuda12
 Requires-Dist: jaxlib[cuda12_pip] ; extra == 'cuda12'
-Requires-Dist: brainpylib ; extra == 'cuda12'
 Provides-Extra: cuda12_mini
 Requires-Dist: jaxlib[cuda12_pip] ; extra == 'cuda12_mini'
 Provides-Extra: testing
 Requires-Dist: pytest ; extra == 'testing'
 Provides-Extra: tpu
 Requires-Dist: jaxlib[tpu] ; extra == 'tpu'
 
 <p align="center">
-  	<img alt="Header image of BrainPy - brain dynamics programming in Python." src="https://github.com/brainpy/BrainPy/blob/master/images/logo.png" width=80%>
+  	<img alt="Header image of braintools." src="https://github.com/brainpy/braintools/blob/main/docs/_static/braintools.jpg" width=50%>
 </p> 
 
 
 
 <p align="center">
 	<a href="https://pypi.org/project/braintools/"><img alt="Supported Python Version" src="https://img.shields.io/pypi/pyversions/braintools"></a>
-	<a href="https://github.com/brainpy/braintools"><img alt="LICENSE" src="https://anaconda.org/brainpy/brainpy/badges/license.svg"></a>
+	<a href="https://github.com/brainpy/braintools/blob/main/LICENSE"><img alt="LICENSE" src="https://img.shields.io/badge/License-Apache%202.0-blue.svg"></a>
   	<a href="https://brainpy.readthedocs.io/en/latest/?badge=latest"><img alt="Documentation" src="https://readthedocs.org/projects/brainpy/badge/?version=latest"></a>
   	<a href="https://badge.fury.io/py/braintools"><img alt="PyPI version" src="https://badge.fury.io/py/braintools.svg"></a>
     <a href="https://github.com/brainpy/braintools/actions/workflows/CI.yml"><img alt="Continuous Integration" src="https://github.com/brainpy/braintools/actions/workflows/CI.yml/badge.svg"></a>
 </p>
 
 
-[``braintools``](https://github.com/brainpy/braintools) is the core system for general-purpose brain dynamics programming framework [``BrainPy``](https://github.com/brainpy/BrainPy). 
+[``braintools``](https://github.com/brainpy/braintools) provides toolboxes for brain dynamics programming. 
 
 
 ## Installation
 
 You can install ``braintools`` via pip:
 
 ```bash
 pip install braintools --upgrade
 ```
 
 ## Documentation
 
 The official documentation is hosted on Read the Docs: [https://braintools.readthedocs.io](https://braintools.readthedocs.io)
 
+
+
+## See also the BDP ecosystem
+
+- [``brainpy``](https://github.com/brainpy/BrainPy): The solution for the general-purpose brain dynamics programming.
+
+- [``braincore``](https://github.com/brainpy/braincore): The core system for the next generation of BrainPy framework.
+
+- [``braintools``](https://github.com/brainpy/braintools): The tools for the brain dynamics simulation and analysis.
+
+- [``brainscale``](https://github.com/brainpy/brainscale): The scalable online learning for biological spiking neural networks.
+
```

### html2text {}

```diff
@@ -1,11 +1,11 @@
-Metadata-Version: 2.1 Name: braintools Version: 0.0.1 Summary: The Toolbox for
+Metadata-Version: 2.1 Name: braintools Version: 0.0.2 Summary: The Toolbox for
 Brain Dynamics Programming. Home-page: https://github.com/brainpy/braintools
 Author: BrainPy Team Author-email: BrainPy Team
-qq.com> License: GPL-3.0 license Project-URL: homepage, http://github.com/
+qq.com> License: Apache-2.0 license Project-URL: homepage, http://github.com/
 brainpy/braintools Project-URL: repository, http://github.com/brainpy/
 braintools Keywords: BrainPy,brain simulation,brain-inspired computing
 Classifier: Natural Language :: English Classifier: Operating System :: OS
 Independent Classifier: Development Status :: 4 - Beta Classifier: Intended
 Audience :: Developers Classifier: Intended Audience :: Science/Research
 Classifier: Programming Language :: Python :: 3 Classifier: Programming
 Language :: Python :: 3.9 Classifier: Programming Language :: Python :: 3.10
@@ -14,25 +14,29 @@
 Software License Classifier: Programming Language :: Python Classifier: Topic
 :: Scientific/Engineering :: Bio-Informatics Classifier: Topic :: Scientific/
 Engineering :: Mathematics Classifier: Topic :: Scientific/Engineering ::
 Artificial Intelligence Classifier: Topic :: Software Development :: Libraries
 Requires-Python: >=3.9 Description-Content-Type: text/markdown License-File:
 LICENSE Requires-Dist: jax Requires-Dist: jaxlib Requires-Dist: numpy Requires-
 Dist: braincore Provides-Extra: cpu Requires-Dist: jaxlib ; extra == 'cpu'
-Requires-Dist: brainpylib ; extra == 'cpu' Provides-Extra: cpu_mini Requires-
-Dist: jaxlib ; extra == 'cpu_mini' Provides-Extra: cuda11 Requires-Dist: jaxlib
-[cuda11_pip] ; extra == 'cuda11' Requires-Dist: brainpylib ; extra == 'cuda11'
-Provides-Extra: cuda11_mini Requires-Dist: jaxlib[cuda11_pip] ; extra ==
-'cuda11_mini' Provides-Extra: cuda12 Requires-Dist: jaxlib[cuda12_pip] ; extra
-== 'cuda12' Requires-Dist: brainpylib ; extra == 'cuda12' Provides-Extra:
-cuda12_mini Requires-Dist: jaxlib[cuda12_pip] ; extra == 'cuda12_mini'
-Provides-Extra: testing Requires-Dist: pytest ; extra == 'testing' Provides-
-Extra: tpu Requires-Dist: jaxlib[tpu] ; extra == 'tpu'
-       [Header image of BrainPy - brain dynamics programming in Python.]
+Provides-Extra: cpu_mini Requires-Dist: jaxlib ; extra == 'cpu_mini' Provides-
+Extra: cuda11 Requires-Dist: jaxlib[cuda11_pip] ; extra == 'cuda11' Provides-
+Extra: cuda11_mini Requires-Dist: jaxlib[cuda11_pip] ; extra == 'cuda11_mini'
+Provides-Extra: cuda12 Requires-Dist: jaxlib[cuda12_pip] ; extra == 'cuda12'
+Provides-Extra: cuda12_mini Requires-Dist: jaxlib[cuda12_pip] ; extra ==
+'cuda12_mini' Provides-Extra: testing Requires-Dist: pytest ; extra ==
+'testing' Provides-Extra: tpu Requires-Dist: jaxlib[tpu] ; extra == 'tpu'
+                         [Header image of braintools.]
   _[_S_u_p_p_o_r_t_e_d_ _P_y_t_h_o_n_ _V_e_r_s_i_o_n_]_[_L_I_C_E_N_S_E_]_[_D_o_c_u_m_e_n_t_a_t_i_o_n_]_[_P_y_P_I_ _v_e_r_s_i_o_n_]_[_C_o_n_t_i_n_u_o_u_s
                                  _I_n_t_e_g_r_a_t_i_o_n_]
-[``braintools``](https://github.com/brainpy/braintools) is the core system for
-general-purpose brain dynamics programming framework [``BrainPy``](https://
-github.com/brainpy/BrainPy). ## Installation You can install ``braintools`` via
+[``braintools``](https://github.com/brainpy/braintools) provides toolboxes for
+brain dynamics programming. ## Installation You can install ``braintools`` via
 pip: ```bash pip install braintools --upgrade ``` ## Documentation The official
 documentation is hosted on Read the Docs: [https://braintools.readthedocs.io]
-(https://braintools.readthedocs.io)
+(https://braintools.readthedocs.io) ## See also the BDP ecosystem -
+[``brainpy``](https://github.com/brainpy/BrainPy): The solution for the
+general-purpose brain dynamics programming. - [``braincore``](https://
+github.com/brainpy/braincore): The core system for the next generation of
+BrainPy framework. - [``braintools``](https://github.com/brainpy/braintools):
+The tools for the brain dynamics simulation and analysis. - [``brainscale``]
+(https://github.com/brainpy/brainscale): The scalable online learning for
+biological spiking neural networks.
```

