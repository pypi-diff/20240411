# Comparing `tmp/hat_event-0.8.9-cp310.cp311-abi3-win_amd64.whl.zip` & `tmp/hat_event-0.9.0-cp310.cp311.cp312-abi3-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,65 +1,64 @@
-Zip file size: 110802 bytes, number of entries: 63
--rw-r--r--  2.0 unx       47 b- defN 23-Feb-14 18:33 hat/event/__init__.py
--rw-r--r--  2.0 unx     2696 b- defN 22-Jul-20 12:38 hat/event/common/__init__.py
--rw-r--r--  2.0 unx     8535 b- defN 23-Jul-09 14:18 hat/event/common/data.py
--rw-r--r--  2.0 unx     9158 b- defN 23-Aug-16 20:32 hat/event/common/json_schema_repo.json
--rw-r--r--  2.0 unx     7842 b- defN 23-Aug-16 20:32 hat/event/common/sbs_repo.json
--rw-r--r--  2.0 unx     4235 b- defN 21-Nov-15 15:43 hat/event/common/timestamp.py
--rw-r--r--  2.0 unx      534 b- defN 23-May-08 19:23 hat/event/common/subscription/__init__.py
--rwxr-xr-x  2.0 unx   122941 b- defN 23-Aug-16 20:32 hat/event/common/subscription/_csubscription.abi3.pyd
--rw-r--r--  2.0 unx     5758 b- defN 23-May-08 19:41 hat/event/common/subscription/common.py
--rw-r--r--  2.0 unx     1530 b- defN 23-May-08 19:39 hat/event/common/subscription/csubscription.py
--rw-r--r--  2.0 unx     2670 b- defN 23-Jun-05 15:08 hat/event/common/subscription/pysubscription.py
--rw-r--r--  2.0 unx     2550 b- defN 23-Feb-24 00:11 hat/event/eventer/__init__.py
--rw-r--r--  2.0 unx    12692 b- defN 23-Jul-09 14:22 hat/event/eventer/client.py
--rw-r--r--  2.0 unx     6502 b- defN 23-Jul-09 14:25 hat/event/eventer/server.py
--rw-r--r--  2.0 unx      521 b- defN 23-Mar-02 17:36 hat/event/mariner/__init__.py
--rw-r--r--  2.0 unx     3940 b- defN 23-Jul-09 15:25 hat/event/mariner/client.py
--rw-r--r--  2.0 unx      481 b- defN 23-Jul-09 15:14 hat/event/mariner/common.py
--rw-r--r--  2.0 unx     6806 b- defN 23-Mar-03 14:27 hat/event/mariner/encoder.py
--rw-r--r--  2.0 unx     5119 b- defN 23-Jul-09 15:24 hat/event/mariner/server.py
--rw-r--r--  2.0 unx     1509 b- defN 23-Jul-09 15:04 hat/event/mariner/transport.py
--rw-r--r--  2.0 unx        0 b- defN 21-Jan-29 21:03 hat/event/server/__init__.py
--rw-r--r--  2.0 unx      138 b- defN 23-Feb-15 17:22 hat/event/server/__main__.py
--rw-r--r--  2.0 unx     5613 b- defN 23-Jul-09 14:55 hat/event/server/common.py
--rw-r--r--  2.0 unx     6751 b- defN 23-Jul-09 14:57 hat/event/server/engine.py
--rw-r--r--  2.0 unx     2095 b- defN 23-Feb-15 17:17 hat/event/server/eventer_server.py
--rw-r--r--  2.0 unx     2330 b- defN 23-Feb-22 02:31 hat/event/server/main.py
--rw-r--r--  2.0 unx     3742 b- defN 23-Jul-09 15:24 hat/event/server/mariner_server.py
--rw-r--r--  2.0 unx    10721 b- defN 23-Jul-09 14:59 hat/event/server/runner.py
--rw-r--r--  2.0 unx     7577 b- defN 23-Aug-16 18:48 hat/event/server/syncer_client.py
--rw-r--r--  2.0 unx     2041 b- defN 23-Jul-09 15:01 hat/event/server/syncer_server.py
--rw-r--r--  2.0 unx        0 b- defN 21-Jan-30 01:25 hat/event/server/backends/__init__.py
--rw-r--r--  2.0 unx     2156 b- defN 23-Jul-09 14:46 hat/event/server/backends/dummy.py
--rw-r--r--  2.0 unx     5647 b- defN 23-Jul-09 14:47 hat/event/server/backends/memory.py
--rw-r--r--  2.0 unx      241 b- defN 21-Aug-09 19:28 hat/event/server/backends/lmdb/__init__.py
--rw-r--r--  2.0 unx     9528 b- defN 23-Jul-09 14:36 hat/event/server/backends/lmdb/backend.py
--rw-r--r--  2.0 unx     2498 b- defN 23-Jul-09 14:39 hat/event/server/backends/lmdb/common.py
--rw-r--r--  2.0 unx     2779 b- defN 23-Jul-09 14:39 hat/event/server/backends/lmdb/conditions.py
--rw-r--r--  2.0 unx     6427 b- defN 23-Jul-09 14:40 hat/event/server/backends/lmdb/encoder.py
--rw-r--r--  2.0 unx     1594 b- defN 23-Jul-09 14:40 hat/event/server/backends/lmdb/environment.py
--rw-r--r--  2.0 unx     4458 b- defN 23-Jul-09 14:42 hat/event/server/backends/lmdb/latestdb.py
--rw-r--r--  2.0 unx    17619 b- defN 23-Jul-09 14:43 hat/event/server/backends/lmdb/ordereddb.py
--rw-r--r--  2.0 unx     4947 b- defN 23-Jul-09 14:44 hat/event/server/backends/lmdb/refdb.py
--rw-r--r--  2.0 unx     2158 b- defN 23-Jul-09 14:45 hat/event/server/backends/lmdb/systemdb.py
--rw-r--r--  2.0 unx        0 b- defN 22-Sep-29 22:53 hat/event/server/backends/lmdb/convert/__init__.py
--rw-r--r--  2.0 unx     8667 b- defN 22-Sep-29 22:53 hat/event/server/backends/lmdb/convert/convert_v06_to_v07.py
--rw-r--r--  2.0 unx     6153 b- defN 23-Jul-09 14:33 hat/event/server/backends/lmdb/convert/v06.py
--rw-r--r--  2.0 unx     2444 b- defN 23-Jul-09 14:34 hat/event/server/backends/lmdb/convert/v07.py
--rw-r--r--  2.0 unx        0 b- defN 22-Oct-03 17:02 hat/event/server/backends/lmdb/manager/__init__.py
--rw-r--r--  2.0 unx      166 b- defN 22-Oct-03 17:02 hat/event/server/backends/lmdb/manager/__main__.py
--rw-r--r--  2.0 unx     1254 b- defN 23-Mar-20 20:57 hat/event/server/backends/lmdb/manager/common.py
--rw-r--r--  2.0 unx     1668 b- defN 23-Mar-20 20:58 hat/event/server/backends/lmdb/manager/copy.py
--rw-r--r--  2.0 unx      951 b- defN 23-Apr-04 16:29 hat/event/server/backends/lmdb/manager/main.py
--rw-r--r--  2.0 unx     5625 b- defN 23-Mar-20 20:59 hat/event/server/backends/lmdb/manager/query.py
--rw-r--r--  2.0 unx      618 b- defN 23-Feb-20 18:22 hat/event/syncer/__init__.py
--rw-r--r--  2.0 unx     3787 b- defN 23-Aug-16 18:48 hat/event/syncer/client.py
--rw-r--r--  2.0 unx     1957 b- defN 23-Aug-16 18:31 hat/event/syncer/common.py
--rw-r--r--  2.0 unx    13536 b- defN 23-Aug-16 18:40 hat/event/syncer/server.py
--rw-r--r--  2.0 unx    11358 b- defN 23-Aug-16 20:32 hat_event-0.8.9.dist-info/LICENSE
--rw-r--r--  2.0 unx     3080 b- defN 23-Aug-16 20:32 hat_event-0.8.9.dist-info/METADATA
--rw-r--r--  2.0 unx      127 b- defN 23-Aug-16 20:32 hat_event-0.8.9.dist-info/WHEEL
--rw-r--r--  2.0 unx      138 b- defN 23-Aug-16 20:32 hat_event-0.8.9.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        4 b- defN 23-Aug-16 20:32 hat_event-0.8.9.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     5785 b- defN 23-Aug-16 20:32 hat_event-0.8.9.dist-info/RECORD
-63 files, 374444 bytes uncompressed, 101456 bytes compressed:  72.9%
+Zip file size: 110680 bytes, number of entries: 62
+?rw-------  2.0 unx     4452 b- defN 24-Mar-24 23:44 hat/event/common/common.py
+?rw-------  2.0 unx      317 b- defN 24-Mar-24 23:44 hat/event/common/collection/__init__.py
+?rw-------  2.0 unx     6950 b- defN 24-Mar-24 23:44 hat/event/server/engine.py
+?rw-------  2.0 unx      595 b- defN 24-Mar-24 23:44 hat/event/common/collection/common.py
+?rw-------  2.0 unx     2666 b- defN 24-Mar-24 23:44 hat/event/common/subscription/pysubscription.py
+?rw-------  2.0 unx     9464 b- defN 24-Mar-24 23:44 hat/event/common/sbs_repo.json
+?rw-------  2.0 unx      301 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/__init__.py
+?rw-------  2.0 unx        0 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/manager/__init__.py
+?rw-------  2.0 unx     8711 b- defN 24-Mar-24 23:44 hat/event/eventer/server.py
+?rw-------  2.0 unx     8880 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/common.py
+?rw-------  2.0 unx     3475 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/manager/copy.py
+?rw-------  2.0 unx     5400 b- defN 24-Mar-24 23:44 hat/event/backends/memory.py
+?rw-------  2.0 unx     1304 b- defN 24-Mar-24 23:44 hat/event/backends/dummy.py
+?rw-------  2.0 unx     3926 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/latestdb.py
+?rw-------  2.0 unx        0 b- defN 24-Mar-24 23:44 hat/event/backends/__init__.py
+?rw-------  2.0 unx        0 b- defN 24-Mar-24 23:44 hat/event/server/__init__.py
+?rw-------  2.0 unx      924 b- defN 24-Mar-24 23:44 hat/event/eventer/__init__.py
+?rw-------  2.0 unx      580 b- defN 24-Mar-24 23:44 hat/event/common/collection/list.py
+?rw-------  2.0 unx        0 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/convert/__init__.py
+?rw-------  2.0 unx       47 b- defN 24-Mar-24 23:44 hat/event/__init__.py
+?rw-------  2.0 unx     1526 b- defN 24-Mar-24 23:44 hat/event/common/subscription/csubscription.py
+?rw-------  2.0 unx    10715 b- defN 24-Mar-24 23:44 hat/event/common/encoder.py
+?rw-------  2.0 unx     4599 b- defN 24-Mar-24 23:44 hat/event/server/eventer_client.py
+?rw-------  2.0 unx    11486 b- defN 24-Mar-24 23:44 hat/event/server/runner.py
+?rw-------  2.0 unx     2493 b- defN 24-Mar-24 23:44 hat/event/common/backend.py
+?rw-------  2.0 unx    10502 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/backend.py
+?rw-------  2.0 unx     2720 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/conditions.py
+?rw-------  2.0 unx      159 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/manager/__main__.py
+?rw-------  2.0 unx     8373 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/convert/convert_v06_to_v07.py
+?rw-------  2.0 unx     1488 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/manager/common.py
+?rw-------  2.0 unx     1682 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/convert/version.py
+?rw-------  2.0 unx      600 b- defN 24-Mar-24 23:44 hat/event/common/subscription/__init__.py
+?rw-------  2.0 unx     8780 b- defN 24-Mar-24 23:44 hat/event/eventer/client.py
+?rw-------  2.0 unx     1346 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/convert/v09.py
+?rw-------  2.0 unx     9947 b- defN 24-Mar-24 23:44 hat/event/component.py
+?rw-------  2.0 unx    17350 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/timeseriesdb.py
+?rw-------  2.0 unx     5705 b- defN 24-Mar-24 23:44 hat/event/common/json_schema_repo.json
+?rw-------  2.0 unx     7747 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/manager/query.py
+?rw-------  2.0 unx      138 b- defN 24-Mar-24 23:44 hat/event/server/__main__.py
+?rw-------  2.0 unx      737 b- defN 24-Mar-24 23:44 hat/event/eventer/common.py
+?rw-------  2.0 unx     3365 b- defN 24-Mar-24 23:44 hat/event/server/eventer_server.py
+?rw-------  2.0 unx      159 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/convert/__main__.py
+?rw-------  2.0 unx     2207 b- defN 24-Mar-24 23:44 hat/event/server/main.py
+?rw-------  2.0 unx     3184 b- defN 24-Mar-24 23:44 hat/event/common/module.py
+?rw-------  2.0 unx     1556 b- defN 24-Mar-24 23:44 hat/event/common/matches.py
+?rw-------  2.0 unx     3227 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/systemdb.py
+?rw-------  2.0 unx   116911 b- defN 24-Mar-24 23:44 hat/event/common/subscription/_csubscription.abi3.pyd
+?rw-------  2.0 unx     2407 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/convert/main.py
+?rw-------  2.0 unx     7858 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/refdb.py
+?rw-------  2.0 unx     5116 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/convert/v06.py
+?rw-------  2.0 unx      752 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/manager/main.py
+?rw-------  2.0 unx     1570 b- defN 24-Mar-24 23:44 hat/event/common/collection/tree.py
+?rw-------  2.0 unx     4265 b- defN 24-Mar-24 23:44 hat/event/common/subscription/common.py
+?rw-------  2.0 unx     6751 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/convert/convert_v07_to_v09.py
+?rw-------  2.0 unx     3194 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/environment.py
+?rw-------  2.0 unx     5782 b- defN 24-Mar-24 23:44 hat/event/common/__init__.py
+?rw-------  2.0 unx    12740 b- defN 24-Mar-24 23:44 hat/event/backends/lmdb/convert/v07.py
+?rw-------  2.0 unx    11358 b- defN 24-Mar-24 23:44 hat_event-0.9.0.dist-info/LICENSE
+?rw-------  2.0 unx      198 b- defN 24-Mar-24 23:44 hat_event-0.9.0.dist-info/entry_points.txt
+?rw-------  2.0 unx     2872 b- defN 24-Mar-24 23:44 hat_event-0.9.0.dist-info/METADATA
+?rw-------  2.0 unx      137 b- defN 24-Mar-24 23:44 hat_event-0.9.0.dist-info/WHEEL
+?rw-------  2.0 unx     5622 b- defN 24-Mar-24 23:44 hat_event-0.9.0.dist-info/RECORD
+62 files, 367316 bytes uncompressed, 101624 bytes compressed:  72.3%
```

## zipnote {}

```diff
@@ -1,190 +1,187 @@
-Filename: hat/event/__init__.py
+Filename: hat/event/common/common.py
 Comment: 
 
-Filename: hat/event/common/__init__.py
+Filename: hat/event/common/collection/__init__.py
 Comment: 
 
-Filename: hat/event/common/data.py
+Filename: hat/event/server/engine.py
 Comment: 
 
-Filename: hat/event/common/json_schema_repo.json
+Filename: hat/event/common/collection/common.py
 Comment: 
 
-Filename: hat/event/common/sbs_repo.json
+Filename: hat/event/common/subscription/pysubscription.py
 Comment: 
 
-Filename: hat/event/common/timestamp.py
+Filename: hat/event/common/sbs_repo.json
 Comment: 
 
-Filename: hat/event/common/subscription/__init__.py
+Filename: hat/event/backends/lmdb/__init__.py
 Comment: 
 
-Filename: hat/event/common/subscription/_csubscription.abi3.pyd
+Filename: hat/event/backends/lmdb/manager/__init__.py
 Comment: 
 
-Filename: hat/event/common/subscription/common.py
+Filename: hat/event/eventer/server.py
 Comment: 
 
-Filename: hat/event/common/subscription/csubscription.py
+Filename: hat/event/backends/lmdb/common.py
 Comment: 
 
-Filename: hat/event/common/subscription/pysubscription.py
+Filename: hat/event/backends/lmdb/manager/copy.py
 Comment: 
 
-Filename: hat/event/eventer/__init__.py
+Filename: hat/event/backends/memory.py
 Comment: 
 
-Filename: hat/event/eventer/client.py
+Filename: hat/event/backends/dummy.py
 Comment: 
 
-Filename: hat/event/eventer/server.py
-Comment: 
-
-Filename: hat/event/mariner/__init__.py
+Filename: hat/event/backends/lmdb/latestdb.py
 Comment: 
 
-Filename: hat/event/mariner/client.py
+Filename: hat/event/backends/__init__.py
 Comment: 
 
-Filename: hat/event/mariner/common.py
+Filename: hat/event/server/__init__.py
 Comment: 
 
-Filename: hat/event/mariner/encoder.py
+Filename: hat/event/eventer/__init__.py
 Comment: 
 
-Filename: hat/event/mariner/server.py
+Filename: hat/event/common/collection/list.py
 Comment: 
 
-Filename: hat/event/mariner/transport.py
+Filename: hat/event/backends/lmdb/convert/__init__.py
 Comment: 
 
-Filename: hat/event/server/__init__.py
+Filename: hat/event/__init__.py
 Comment: 
 
-Filename: hat/event/server/__main__.py
+Filename: hat/event/common/subscription/csubscription.py
 Comment: 
 
-Filename: hat/event/server/common.py
+Filename: hat/event/common/encoder.py
 Comment: 
 
-Filename: hat/event/server/engine.py
+Filename: hat/event/server/eventer_client.py
 Comment: 
 
-Filename: hat/event/server/eventer_server.py
+Filename: hat/event/server/runner.py
 Comment: 
 
-Filename: hat/event/server/main.py
+Filename: hat/event/common/backend.py
 Comment: 
 
-Filename: hat/event/server/mariner_server.py
+Filename: hat/event/backends/lmdb/backend.py
 Comment: 
 
-Filename: hat/event/server/runner.py
+Filename: hat/event/backends/lmdb/conditions.py
 Comment: 
 
-Filename: hat/event/server/syncer_client.py
+Filename: hat/event/backends/lmdb/manager/__main__.py
 Comment: 
 
-Filename: hat/event/server/syncer_server.py
+Filename: hat/event/backends/lmdb/convert/convert_v06_to_v07.py
 Comment: 
 
-Filename: hat/event/server/backends/__init__.py
+Filename: hat/event/backends/lmdb/manager/common.py
 Comment: 
 
-Filename: hat/event/server/backends/dummy.py
+Filename: hat/event/backends/lmdb/convert/version.py
 Comment: 
 
-Filename: hat/event/server/backends/memory.py
+Filename: hat/event/common/subscription/__init__.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/__init__.py
+Filename: hat/event/eventer/client.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/backend.py
+Filename: hat/event/backends/lmdb/convert/v09.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/common.py
+Filename: hat/event/component.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/conditions.py
+Filename: hat/event/backends/lmdb/timeseriesdb.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/encoder.py
+Filename: hat/event/common/json_schema_repo.json
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/environment.py
+Filename: hat/event/backends/lmdb/manager/query.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/latestdb.py
+Filename: hat/event/server/__main__.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/ordereddb.py
+Filename: hat/event/eventer/common.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/refdb.py
+Filename: hat/event/server/eventer_server.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/systemdb.py
+Filename: hat/event/backends/lmdb/convert/__main__.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/convert/__init__.py
+Filename: hat/event/server/main.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/convert/convert_v06_to_v07.py
+Filename: hat/event/common/module.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/convert/v06.py
+Filename: hat/event/common/matches.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/convert/v07.py
+Filename: hat/event/backends/lmdb/systemdb.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/manager/__init__.py
+Filename: hat/event/common/subscription/_csubscription.abi3.pyd
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/manager/__main__.py
+Filename: hat/event/backends/lmdb/convert/main.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/manager/common.py
+Filename: hat/event/backends/lmdb/refdb.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/manager/copy.py
+Filename: hat/event/backends/lmdb/convert/v06.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/manager/main.py
+Filename: hat/event/backends/lmdb/manager/main.py
 Comment: 
 
-Filename: hat/event/server/backends/lmdb/manager/query.py
+Filename: hat/event/common/collection/tree.py
 Comment: 
 
-Filename: hat/event/syncer/__init__.py
+Filename: hat/event/common/subscription/common.py
 Comment: 
 
-Filename: hat/event/syncer/client.py
+Filename: hat/event/backends/lmdb/convert/convert_v07_to_v09.py
 Comment: 
 
-Filename: hat/event/syncer/common.py
+Filename: hat/event/backends/lmdb/environment.py
 Comment: 
 
-Filename: hat/event/syncer/server.py
+Filename: hat/event/common/__init__.py
 Comment: 
 
-Filename: hat_event-0.8.9.dist-info/LICENSE
+Filename: hat/event/backends/lmdb/convert/v07.py
 Comment: 
 
-Filename: hat_event-0.8.9.dist-info/METADATA
+Filename: hat_event-0.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: hat_event-0.8.9.dist-info/WHEEL
+Filename: hat_event-0.9.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: hat_event-0.8.9.dist-info/entry_points.txt
+Filename: hat_event-0.9.0.dist-info/METADATA
 Comment: 
 
-Filename: hat_event-0.8.9.dist-info/top_level.txt
+Filename: hat_event-0.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: hat_event-0.8.9.dist-info/RECORD
+Filename: hat_event-0.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## hat/event/common/__init__.py

```diff
@@ -1,68 +1,144 @@
 """Common functionality shared between clients and event server"""
 
-from hat.event.common.data import (json_schema_repo,
-                                   sbs_repo,
-                                   EventType,
-                                   Order,
-                                   OrderBy,
-                                   EventPayloadType,
-                                   EventId,
-                                   EventPayload,
-                                   SbsData,
-                                   Event,
-                                   RegisterEvent,
-                                   QueryData,
-                                   event_to_sbs,
-                                   event_from_sbs,
-                                   register_event_to_sbs,
-                                   register_event_from_sbs,
-                                   query_to_sbs,
-                                   query_from_sbs,
-                                   event_payload_to_sbs,
-                                   event_payload_from_sbs)
-from hat.event.common.timestamp import (Timestamp,
-                                        now,
-                                        timestamp_to_bytes,
-                                        timestamp_from_bytes,
-                                        timestamp_to_float,
-                                        timestamp_from_float,
-                                        timestamp_to_datetime,
-                                        timestamp_from_datetime,
-                                        timestamp_to_sbs,
-                                        timestamp_from_sbs)
-from hat.event.common.subscription import (matches_query_type,
-                                           Subscription)
+import time
 
+from hat.event.common.backend import (BackendClosedError,
+                                      Backend,
+                                      BackendConf,
+                                      BackendRegisteredEventsCb,
+                                      BackendFlushedEventsCb,
+                                      CreateBackend,
+                                      BackendInfo,
+                                      import_backend_info)
+from hat.event.common.collection import (EventTypeCollection,
+                                         ListEventTypeCollection,
+                                         TreeEventTypeCollection)
+from hat.event.common.common import (json_schema_repo,
+                                     sbs_repo,
+                                     ServerId,
+                                     SessionId,
+                                     InstanceId,
+                                     EventTypeSegment,
+                                     EventType,
+                                     Timestamp,
+                                     min_timestamp,
+                                     max_timestamp,
+                                     Status,
+                                     Order,
+                                     OrderBy,
+                                     EventId,
+                                     EventPayloadBinary,
+                                     EventPayloadJson,
+                                     EventPayload,
+                                     Event,
+                                     RegisterEvent,
+                                     QueryLatestParams,
+                                     QueryTimeseriesParams,
+                                     QueryServerParams,
+                                     QueryParams,
+                                     QueryResult)
+from hat.event.common.encoder import (timestamp_to_bytes,
+                                      timestamp_from_bytes,
+                                      timestamp_to_float,
+                                      timestamp_from_float,
+                                      timestamp_to_datetime,
+                                      timestamp_from_datetime,
+                                      timestamp_to_sbs,
+                                      timestamp_from_sbs,
+                                      status_to_sbs,
+                                      status_from_sbs,
+                                      event_to_sbs,
+                                      event_from_sbs,
+                                      register_event_to_sbs,
+                                      register_event_from_sbs,
+                                      query_params_to_sbs,
+                                      query_params_from_sbs,
+                                      query_result_to_sbs,
+                                      query_result_from_sbs,
+                                      event_payload_to_sbs,
+                                      event_payload_from_sbs)
+from hat.event.common.matches import matches_query_type
+from hat.event.common.module import (SourceType,
+                                     Source,
+                                     Engine,
+                                     Module,
+                                     ModuleConf,
+                                     CreateModule,
+                                     ModuleInfo,
+                                     import_module_info)
+from hat.event.common.subscription import (Subscription,
+                                           create_subscription)
 
-__all__ = ['json_schema_repo',
+
+__all__ = ['BackendClosedError',
+           'Backend',
+           'BackendConf',
+           'BackendRegisteredEventsCb',
+           'BackendFlushedEventsCb',
+           'CreateBackend',
+           'BackendInfo',
+           'import_backend_info',
+           'EventTypeCollection',
+           'ListEventTypeCollection',
+           'TreeEventTypeCollection',
+           'json_schema_repo',
            'sbs_repo',
+           'ServerId',
+           'SessionId',
+           'InstanceId',
+           'EventTypeSegment',
            'EventType',
+           'Timestamp',
+           'min_timestamp',
+           'max_timestamp',
+           'Status',
            'Order',
            'OrderBy',
-           'EventPayloadType',
            'EventId',
+           'EventPayloadBinary',
+           'EventPayloadJson',
            'EventPayload',
-           'SbsData',
            'Event',
            'RegisterEvent',
-           'QueryData',
-           'event_to_sbs',
-           'event_from_sbs',
-           'register_event_to_sbs',
-           'register_event_from_sbs',
-           'query_to_sbs',
-           'query_from_sbs',
-           'event_payload_to_sbs',
-           'event_payload_from_sbs',
-           'Timestamp',
-           'now',
+           'QueryLatestParams',
+           'QueryTimeseriesParams',
+           'QueryServerParams',
+           'QueryParams',
+           'QueryResult',
            'timestamp_to_bytes',
            'timestamp_from_bytes',
            'timestamp_to_float',
            'timestamp_from_float',
            'timestamp_to_datetime',
            'timestamp_from_datetime',
            'timestamp_to_sbs',
            'timestamp_from_sbs',
+           'status_to_sbs',
+           'status_from_sbs',
+           'event_to_sbs',
+           'event_from_sbs',
+           'register_event_to_sbs',
+           'register_event_from_sbs',
+           'query_params_to_sbs',
+           'query_params_from_sbs',
+           'query_result_to_sbs',
+           'query_result_from_sbs',
+           'event_payload_to_sbs',
+           'event_payload_from_sbs',
            'matches_query_type',
-           'Subscription']
+           'SourceType',
+           'Source',
+           'Engine',
+           'Module',
+           'ModuleConf',
+           'CreateModule',
+           'ModuleInfo',
+           'import_module_info',
+           'Subscription',
+           'create_subscription',
+           'now']
+
+
+def now() -> Timestamp:
+    """Create new timestamp representing current time"""
+    return timestamp_from_float(time.time())
```

## hat/event/common/json_schema_repo.json

### Pretty-printed

 * *Similarity: 0.8175552856414556%*

 * *Differences: {"'hat-event'": "{'monitor_data.yaml': {'required': {insert: [(1, 'eventer_server'), (2, "*

 * *                "'server_token')], delete: [1, 0]}, 'properties': {'eventer_server': "*

 * *                "OrderedDict([('type', 'object'), ('required', ['host', 'port']), ('properties', "*

 * *                "OrderedDict([('host', OrderedDict([('type', 'string')])), ('port', "*

 * *                "OrderedDict([('type', 'integer')]))]))]), 'server_token': OrderedDict([('type', "*

 * *                "['string', 'null'])]), delete: [' [â€¦]*

```diff
@@ -123,14 +123,17 @@
                             "type": "array"
                         }
                     ]
                 }
             },
             "id": "hat-event://backends/lmdb.yaml#",
             "properties": {
+                "cleanup_period": {
+                    "type": "number"
+                },
                 "conditions": {
                     "items": {
                         "properties": {
                             "condition": {
                                 "$ref": "hat-event://backends/lmdb.yaml#/definitions/condition"
                             },
                             "subscriptions": {
@@ -147,29 +150,32 @@
                 },
                 "db_path": {
                     "type": "string"
                 },
                 "flush_period": {
                     "type": "number"
                 },
+                "identifier": {
+                    "type": [
+                        "null",
+                        "string"
+                    ]
+                },
                 "latest": {
                     "properties": {
                         "subscriptions": {
                             "$ref": "hat-event://backends/lmdb.yaml#/definitions/event_types"
                         }
                     },
                     "required": [
                         "subscriptions"
                     ],
                     "type": "object"
                 },
-                "max_db_size": {
-                    "type": "number"
-                },
-                "ordered": {
+                "timeseries": {
                     "items": {
                         "properties": {
                             "limit": {
                                 "$ref": "hat-event://backends/lmdb.yaml#/definitions/limit"
                             },
                             "order_by": {
                                 "enum": [
@@ -188,19 +194,20 @@
                         "type": "object"
                     },
                     "type": "array"
                 }
             },
             "required": [
                 "db_path",
-                "max_db_size",
+                "identifier",
                 "flush_period",
+                "cleanup_period",
                 "conditions",
                 "latest",
-                "ordered"
+                "timeseries"
             ],
             "title": "LMDB backend",
             "type": "object"
         },
         "backends/sqlite.yaml": {
             "description": "Sqlite backend configuration",
             "id": "hat-event://backends/sqlite.yaml#",
@@ -217,470 +224,186 @@
             "required": [
                 "db_path",
                 "query_pool_size"
             ],
             "title": "Sqlite backend",
             "type": "object"
         },
-        "mariner.yaml": {
+        "events.yaml": {
             "definitions": {
-                "event": {
-                    "properties": {
-                        "id": {
-                            "$ref": "hat-event://mariner.yaml#/definitions/event_id"
-                        },
-                        "payload": {
-                            "oneOf": [
-                                "null",
-                                {
-                                    "$ref": "hat-event://mariner.yaml#/definitions/payload"
-                                }
-                            ]
-                        },
-                        "source_timestamp": {
-                            "oneOf": [
-                                "null",
-                                {
-                                    "$ref": "hat-event://mariner.yaml#/definitions/timestamp"
-                                }
-                            ]
-                        },
-                        "timestamp": {
-                            "$ref": "hat-event://mariner.yaml#/definitions/timestamp"
-                        },
-                        "type": {
-                            "$ref": "hat-event://mariner.yaml#/definitions/event_type"
-                        }
-                    },
-                    "required": [
-                        "id",
-                        "type",
-                        "timestamp",
-                        "source_timestamp",
-                        "payload"
-                    ],
-                    "type": "object"
-                },
-                "event_id": {
-                    "properties": {
-                        "instance": {
-                            "type": "integer"
-                        },
-                        "server": {
-                            "type": "integer"
-                        },
-                        "session": {
-                            "type": "integer"
-                        }
-                    },
-                    "required": [
-                        "server",
-                        "session",
-                        "instance"
-                    ],
-                    "type": "object"
-                },
-                "event_type": {
-                    "items": {
-                        "type": "string"
-                    },
-                    "type": "array"
-                },
-                "messages": {
-                    "events": {
-                        "properties": {
-                            "events": {
-                                "items": {
-                                    "$ref": "hat-event://mariner.yaml#/definitions/event"
-                                },
-                                "type": "array"
-                            },
-                            "type": {
-                                "const": "events"
-                            }
-                        },
-                        "required": [
-                            "type",
-                            "events"
-                        ],
-                        "type": "object"
-                    },
-                    "init": {
-                        "properties": {
-                            "client_id": {
-                                "type": "string"
-                            },
-                            "client_token": {
-                                "type": [
-                                    "string",
-                                    "null"
-                                ]
-                            },
-                            "last_event_id": {
-                                "oneOf": [
-                                    {
-                                        "type": "null"
-                                    },
-                                    {
-                                        "$ref": "hat-event://mariner.yaml#/definitions/event_id"
-                                    }
-                                ]
-                            },
-                            "subscriptions": {
-                                "items": {
-                                    "$ref": "hat-event://mariner.yaml#/definitions/event_type"
-                                },
-                                "type": "array"
-                            },
-                            "type": {
-                                "const": "init"
-                            }
-                        },
-                        "required": [
-                            "type",
-                            "client_id",
-                            "client_token",
-                            "last_event_id",
-                            "subscriptions"
-                        ],
-                        "type": "object"
+                "events": {
+                    "engine": {
+                        "enum": [
+                            "STARTED",
+                            "STOPPED"
+                        ]
                     },
-                    "ping": {
-                        "properties": {
-                            "type": {
-                                "const": "ping"
-                            }
-                        },
-                        "required": [
-                            "type"
-                        ],
-                        "type": "object"
+                    "eventer": {
+                        "enum": [
+                            "CONNECTED",
+                            "DISCONNECTED"
+                        ]
                     },
-                    "pong": {
-                        "properties": {
-                            "type": {
-                                "const": "pong"
-                            }
-                        },
-                        "required": [
-                            "type"
-                        ],
-                        "type": "object"
+                    "synced": {
+                        "type": "boolean"
                     }
-                },
-                "payload": {
-                    "oneOf": [
-                        {
-                            "properties": {
-                                "data": {
-                                    "type": "string"
-                                },
-                                "type": {
-                                    "const": "binary"
-                                }
-                            },
-                            "required": [
-                                "type",
-                                "data"
-                            ],
-                            "type": "object"
-                        },
-                        {
-                            "properties": {
-                                "type": {
-                                    "const": "json"
-                                }
-                            },
-                            "required": [
-                                "type",
-                                "data"
-                            ],
-                            "type": "object"
-                        },
-                        {
-                            "properties": {
-                                "data": {
-                                    "properties": {
-                                        "module": {
-                                            "data": {
-                                                "type": "bytes"
-                                            },
-                                            "module": [
-                                                "null",
-                                                "string"
-                                            ],
-                                            "type": {
-                                                "type": "string"
-                                            }
-                                        }
-                                    },
-                                    "required": [
-                                        "module",
-                                        "type",
-                                        "data"
-                                    ],
-                                    "type": "object"
-                                },
-                                "type": {
-                                    "const": "sbs"
-                                }
-                            },
-                            "required": [
-                                "type",
-                                "data"
-                            ],
-                            "type": "object"
-                        }
-                    ]
-                },
-                "timestamp": {
+                }
+            },
+            "id": "hat-event://events.yaml#"
+        },
+        "monitor_data.yaml": {
+            "description": "data property of monitor component info",
+            "id": "hat-event://monitor_data.yaml#",
+            "properties": {
+                "eventer_server": {
                     "properties": {
-                        "s": {
-                            "type": "integer"
+                        "host": {
+                            "type": "string"
                         },
-                        "us": {
+                        "port": {
                             "type": "integer"
                         }
                     },
                     "required": [
-                        "s",
-                        "us"
+                        "host",
+                        "port"
                     ],
                     "type": "object"
-                }
-            },
-            "id": "hat-event://mariner.yaml#",
-            "oneOf": [
-                {
-                    "$ref": "hat-event://mariner.yaml#/definitions/messages/ping"
-                },
-                {
-                    "$ref": "hat-event://mariner.yaml#/definitions/messages/pong"
-                },
-                {
-                    "$ref": "hat-event://mariner.yaml#/definitions/messages/init"
-                },
-                {
-                    "$ref": "hat-event://mariner.yaml#/definitions/messages/events"
-                }
-            ],
-            "title": "Mariner communication messages"
-        },
-        "monitor_data.yaml": {
-            "description": "data property of monitor component info",
-            "id": "hat-event://monitor_data.yaml#",
-            "properties": {
-                "eventer_server_address": {
-                    "type": "string"
                 },
                 "server_id": {
                     "type": "integer"
                 },
-                "syncer_server_address": {
-                    "type": "string"
-                },
-                "syncer_token": {
-                    "type": "string"
+                "server_token": {
+                    "type": [
+                        "string",
+                        "null"
+                    ]
                 }
             },
             "required": [
-                "eventer_server_address",
-                "syncer_server_address",
-                "server_id"
+                "server_id",
+                "eventer_server",
+                "server_token"
             ],
             "title": "monitor data",
             "type": "object"
         },
         "server.yaml": {
             "definitions": {
                 "backend": {
                     "description": "structure of backend configuration depends on backend type\n",
                     "properties": {
                         "module": {
-                            "description": "full python module name that implements backend\n",
+                            "description": "full python module name that implements backend",
                             "type": "string"
                         }
                     },
                     "required": [
                         "module"
                     ],
                     "type": "object"
                 },
-                "engine": {
+                "module": {
+                    "description": "structure of module configuration depends on module type\n",
                     "properties": {
-                        "modules": {
-                            "items": {
-                                "$ref": "hat-event://server.yaml#/definitions/module"
-                            },
-                            "type": "array"
-                        },
-                        "server_id": {
-                            "type": "integer"
+                        "module": {
+                            "description": "full python module name that implements module",
+                            "type": "string"
                         }
                     },
                     "required": [
-                        "server_id",
-                        "modules"
+                        "module"
                     ],
                     "type": "object"
-                },
-                "event_types": {
-                    "items": {
-                        "items": {
-                            "type": "string"
-                        },
-                        "type": "array"
-                    },
-                    "type": "array"
+                }
+            },
+            "description": "Event Server's configuration",
+            "id": "hat-event://server.yaml#",
+            "properties": {
+                "backend": {
+                    "$ref": "hat-event://server.yaml#/definitions/backend"
                 },
                 "eventer_server": {
                     "properties": {
-                        "address": {
-                            "default": "tcp+sbs://127.0.0.1:23012",
+                        "host": {
+                            "default": "127.0.0.1",
                             "type": "string"
+                        },
+                        "port": {
+                            "default": 23012,
+                            "type": "integer"
                         }
                     },
                     "required": [
-                        "address"
+                        "host",
+                        "port"
                     ],
                     "type": "object"
                 },
-                "events": {
-                    "engine": {
-                        "enum": [
-                            "STARTED",
-                            "STOPPED"
-                        ]
-                    },
-                    "eventer": {
-                        "enum": [
-                            "CONNECTED",
-                            "DISCONNECTED"
-                        ]
-                    },
-                    "syncer": {
-                        "client": {
-                            "type": "boolean"
-                        },
-                        "server": {
-                            "items": {
-                                "properties": {
-                                    "client_name": {
-                                        "type": "string"
-                                    },
-                                    "synced": {
-                                        "type": "boolean"
-                                    }
-                                },
-                                "required": [
-                                    "client_name",
-                                    "synced"
-                                ],
-                                "type": "object"
-                            },
-                            "type": "array"
-                        }
-                    }
+                "log": {
+                    "$ref": "hat-json://logging.yaml#"
                 },
-                "mariner_server": {
-                    "properties": {
-                        "address": {
-                            "default": "tcp://127.0.0.1:23014",
-                            "type": "string"
-                        },
-                        "subscriptions": {
-                            "$ref": "hat-event://server.yaml#/definitions/event_types"
-                        }
+                "modules": {
+                    "items": {
+                        "$ref": "hat-event://server.yaml#/definitions/module"
                     },
-                    "required": [
-                        "address",
-                        "subscriptions"
-                    ],
-                    "type": "object"
+                    "type": "array"
                 },
-                "module": {
-                    "description": "structure of module configuration depends on module type\n",
+                "monitor_component": {
                     "properties": {
-                        "module": {
-                            "description": "full python module name that implements module\n",
+                        "group": {
                             "type": "string"
-                        }
-                    },
-                    "required": [
-                        "module"
-                    ],
-                    "type": "object"
-                },
-                "syncer_server": {
-                    "properties": {
-                        "address": {
-                            "default": "tcp+sbs://127.0.0.1:23013",
+                        },
+                        "host": {
+                            "default": "127.0.0.1",
                             "type": "string"
                         },
-                        "subscriptions": {
-                            "$ref": "hat-event://server.yaml#/definitions/event_types"
+                        "name": {
+                            "type": "string"
                         },
-                        "token": {
-                            "type": [
-                                "null",
-                                "string"
-                            ]
+                        "port": {
+                            "default": 23010,
+                            "type": "integer"
                         }
                     },
                     "required": [
-                        "address"
+                        "host",
+                        "port",
+                        "name",
+                        "group"
                     ],
                     "type": "object"
-                }
-            },
-            "description": "Event Server's configuration",
-            "id": "hat-event://server.yaml#",
-            "properties": {
-                "backend": {
-                    "$ref": "hat-event://server.yaml#/definitions/backend"
                 },
-                "engine": {
-                    "$ref": "hat-event://server.yaml#/definitions/engine"
-                },
-                "eventer_server": {
-                    "$ref": "hat-event://server.yaml#/definitions/eventer_server"
-                },
-                "log": {
-                    "$ref": "hat-json://logging.yaml#"
-                },
-                "mariner_server": {
-                    "$ref": "hat-event://server.yaml#/definitions/mariner_server"
+                "server_id": {
+                    "description": "server identifier",
+                    "type": "integer"
                 },
-                "monitor": {
-                    "$ref": "hat-monitor://client.yaml#"
+                "server_token": {
+                    "description": "server token",
+                    "type": [
+                        "string",
+                        "null"
+                    ]
                 },
                 "synced_restart_engine": {
                     "type": "boolean"
                 },
-                "syncer_server": {
-                    "$ref": "hat-event://server.yaml#/definitions/syncer_server"
-                },
-                "syncer_token": {
-                    "description": "match of event servers' syncer_token is necessery condition for \ntheir synchronization\n",
-                    "type": "string"
-                },
                 "type": {
                     "const": "event",
                     "description": "configuration type identification"
                 },
                 "version": {
                     "description": "component version",
                     "type": "string"
                 }
             },
             "required": [
-                "type",
-                "log",
+                "server_id",
                 "backend",
-                "engine",
+                "modules",
                 "eventer_server"
             ],
             "title": "Event Server",
             "type": "object"
         }
     }
 }
```

## hat/event/common/sbs_repo.json

### Pretty-printed

 * *Similarity: 0.42827594091021826%*

 * *Differences: {'0': "{'type_defs': {'MsgRegisterRes': {'type': {'name': 'Choice', 'entries': "*

 * *      "[OrderedDict([('name', 'events'), ('type', OrderedDict([('module', None), ('name', "*

 * *      "'Array'), ('entries', []), ('args', [OrderedDict([('module', None), ('name', 'Event'), "*

 * *      "('entries', []), ('args', [])])])]))]), OrderedDict([('name', 'failure'), ('type', "*

 * *      "OrderedDict([('module', None), ('name', 'None'), ('entries', []), ('args', [])]))])], "*

 * *      "'args': []}}, 'MsgQueryReq': {'type': {'name': 'Q [â€¦]*

```diff
@@ -1,151 +1,9 @@
 [
     {
-        "name": "HatSyncer",
-        "type_defs": {
-            "MsgEvents": {
-                "args": [],
-                "name": "MsgEvents",
-                "type": {
-                    "args": [
-                        {
-                            "args": [],
-                            "entries": [],
-                            "module": "HatEventer",
-                            "name": "Event"
-                        }
-                    ],
-                    "entries": [],
-                    "module": null,
-                    "name": "Array"
-                }
-            },
-            "MsgFlushReq": {
-                "args": [],
-                "name": "MsgFlushReq",
-                "type": {
-                    "args": [],
-                    "entries": [],
-                    "module": null,
-                    "name": "None"
-                }
-            },
-            "MsgFlushRes": {
-                "args": [],
-                "name": "MsgFlushRes",
-                "type": {
-                    "args": [],
-                    "entries": [],
-                    "module": null,
-                    "name": "None"
-                }
-            },
-            "MsgInitReq": {
-                "args": [],
-                "name": "MsgInitReq",
-                "type": {
-                    "args": [],
-                    "entries": [
-                        {
-                            "name": "lastEventId",
-                            "type": {
-                                "args": [],
-                                "entries": [],
-                                "module": "HatEventer",
-                                "name": "EventId"
-                            }
-                        },
-                        {
-                            "name": "clientName",
-                            "type": {
-                                "args": [],
-                                "entries": [],
-                                "module": null,
-                                "name": "String"
-                            }
-                        },
-                        {
-                            "name": "clientToken",
-                            "type": {
-                                "args": [
-                                    {
-                                        "args": [],
-                                        "entries": [],
-                                        "module": null,
-                                        "name": "String"
-                                    }
-                                ],
-                                "entries": [],
-                                "module": null,
-                                "name": "Optional"
-                            }
-                        },
-                        {
-                            "name": "subscriptions",
-                            "type": {
-                                "args": [
-                                    {
-                                        "args": [],
-                                        "entries": [],
-                                        "module": "HatEventer",
-                                        "name": "EventType"
-                                    }
-                                ],
-                                "entries": [],
-                                "module": null,
-                                "name": "Array"
-                            }
-                        }
-                    ],
-                    "module": null,
-                    "name": "Record"
-                }
-            },
-            "MsgInitRes": {
-                "args": [],
-                "name": "MsgInitRes",
-                "type": {
-                    "args": [],
-                    "entries": [
-                        {
-                            "name": "success",
-                            "type": {
-                                "args": [],
-                                "entries": [],
-                                "module": null,
-                                "name": "None"
-                            }
-                        },
-                        {
-                            "name": "error",
-                            "type": {
-                                "args": [],
-                                "entries": [],
-                                "module": null,
-                                "name": "String"
-                            }
-                        }
-                    ],
-                    "module": null,
-                    "name": "Choice"
-                }
-            },
-            "MsgSynced": {
-                "args": [],
-                "name": "MsgSynced",
-                "type": {
-                    "args": [],
-                    "entries": [],
-                    "module": null,
-                    "name": "None"
-                }
-            }
-        }
-    },
-    {
         "name": "HatEventer",
         "type_defs": {
             "Event": {
                 "args": [],
                 "name": "Event",
                 "type": {
                     "args": [],
@@ -260,38 +118,68 @@
                     "entries": [
                         {
                             "name": "binary",
                             "type": {
                                 "args": [],
                                 "entries": [],
                                 "module": null,
-                                "name": "Bytes"
+                                "name": "EventPayloadBinary"
                             }
                         },
                         {
                             "name": "json",
                             "type": {
                                 "args": [],
                                 "entries": [],
                                 "module": null,
+                                "name": "EventPayloadJson"
+                            }
+                        }
+                    ],
+                    "module": null,
+                    "name": "Choice"
+                }
+            },
+            "EventPayloadBinary": {
+                "args": [],
+                "name": "EventPayloadBinary",
+                "type": {
+                    "args": [],
+                    "entries": [
+                        {
+                            "name": "type",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
                                 "name": "String"
                             }
                         },
                         {
-                            "name": "sbs",
+                            "name": "data",
                             "type": {
                                 "args": [],
                                 "entries": [],
-                                "module": "Hat",
-                                "name": "Data"
+                                "module": null,
+                                "name": "Bytes"
                             }
                         }
                     ],
                     "module": null,
-                    "name": "Choice"
+                    "name": "Record"
+                }
+            },
+            "EventPayloadJson": {
+                "args": [],
+                "name": "EventPayloadJson",
+                "type": {
+                    "args": [],
+                    "entries": [],
+                    "module": null,
+                    "name": "String"
                 }
             },
             "EventType": {
                 "args": [],
                 "name": "EventType",
                 "type": {
                     "args": [
@@ -303,56 +191,155 @@
                         }
                     ],
                     "entries": [],
                     "module": null,
                     "name": "Array"
                 }
             },
-            "MsgNotify": {
+            "MsgEventsNotify": {
                 "args": [],
-                "name": "MsgNotify",
+                "name": "MsgEventsNotify",
                 "type": {
                     "args": [
                         {
                             "args": [],
                             "entries": [],
                             "module": null,
                             "name": "Event"
                         }
                     ],
                     "entries": [],
                     "module": null,
                     "name": "Array"
                 }
             },
+            "MsgInitReq": {
+                "args": [],
+                "name": "MsgInitReq",
+                "type": {
+                    "args": [],
+                    "entries": [
+                        {
+                            "name": "clientName",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "String"
+                            }
+                        },
+                        {
+                            "name": "clientToken",
+                            "type": {
+                                "args": [
+                                    {
+                                        "args": [],
+                                        "entries": [],
+                                        "module": null,
+                                        "name": "String"
+                                    }
+                                ],
+                                "entries": [],
+                                "module": null,
+                                "name": "Optional"
+                            }
+                        },
+                        {
+                            "name": "subscriptions",
+                            "type": {
+                                "args": [
+                                    {
+                                        "args": [],
+                                        "entries": [],
+                                        "module": null,
+                                        "name": "EventType"
+                                    }
+                                ],
+                                "entries": [],
+                                "module": null,
+                                "name": "Array"
+                            }
+                        },
+                        {
+                            "name": "serverId",
+                            "type": {
+                                "args": [
+                                    {
+                                        "args": [],
+                                        "entries": [],
+                                        "module": null,
+                                        "name": "Integer"
+                                    }
+                                ],
+                                "entries": [],
+                                "module": null,
+                                "name": "Optional"
+                            }
+                        },
+                        {
+                            "name": "persisted",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "Boolean"
+                            }
+                        }
+                    ],
+                    "module": null,
+                    "name": "Record"
+                }
+            },
+            "MsgInitRes": {
+                "args": [],
+                "name": "MsgInitRes",
+                "type": {
+                    "args": [],
+                    "entries": [
+                        {
+                            "name": "success",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "Status"
+                            }
+                        },
+                        {
+                            "name": "error",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "String"
+                            }
+                        }
+                    ],
+                    "module": null,
+                    "name": "Choice"
+                }
+            },
             "MsgQueryReq": {
                 "args": [],
                 "name": "MsgQueryReq",
                 "type": {
                     "args": [],
                     "entries": [],
                     "module": null,
-                    "name": "QueryData"
+                    "name": "QueryParams"
                 }
             },
             "MsgQueryRes": {
                 "args": [],
                 "name": "MsgQueryRes",
                 "type": {
-                    "args": [
-                        {
-                            "args": [],
-                            "entries": [],
-                            "module": null,
-                            "name": "Event"
-                        }
-                    ],
+                    "args": [],
                     "entries": [],
                     "module": null,
-                    "name": "Array"
+                    "name": "QueryResult"
                 }
             },
             "MsgRegisterReq": {
                 "args": [],
                 "name": "MsgRegisterReq",
                 "type": {
                     "args": [
@@ -368,61 +355,64 @@
                     "name": "Array"
                 }
             },
             "MsgRegisterRes": {
                 "args": [],
                 "name": "MsgRegisterRes",
                 "type": {
-                    "args": [
+                    "args": [],
+                    "entries": [
                         {
-                            "args": [],
-                            "entries": [
-                                {
-                                    "name": "event",
-                                    "type": {
+                            "name": "events",
+                            "type": {
+                                "args": [
+                                    {
                                         "args": [],
                                         "entries": [],
                                         "module": null,
                                         "name": "Event"
                                     }
-                                },
-                                {
-                                    "name": "failure",
-                                    "type": {
-                                        "args": [],
-                                        "entries": [],
-                                        "module": null,
-                                        "name": "None"
-                                    }
-                                }
-                            ],
-                            "module": null,
-                            "name": "Choice"
+                                ],
+                                "entries": [],
+                                "module": null,
+                                "name": "Array"
+                            }
+                        },
+                        {
+                            "name": "failure",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "None"
+                            }
                         }
                     ],
+                    "module": null,
+                    "name": "Choice"
+                }
+            },
+            "MsgStatusAck": {
+                "args": [],
+                "name": "MsgStatusAck",
+                "type": {
+                    "args": [],
                     "entries": [],
                     "module": null,
-                    "name": "Array"
+                    "name": "None"
                 }
             },
-            "MsgSubscribe": {
+            "MsgStatusNotify": {
                 "args": [],
-                "name": "MsgSubscribe",
+                "name": "MsgStatusNotify",
                 "type": {
-                    "args": [
-                        {
-                            "args": [],
-                            "entries": [],
-                            "module": null,
-                            "name": "EventType"
-                        }
-                    ],
+                    "args": [],
                     "entries": [],
                     "module": null,
-                    "name": "Array"
+                    "name": "Status"
                 }
             },
             "Order": {
                 "args": [],
                 "name": "Order",
                 "type": {
                     "args": [],
@@ -475,61 +465,191 @@
                             }
                         }
                     ],
                     "module": null,
                     "name": "Choice"
                 }
             },
-            "QueryData": {
+            "QueryLatestParams": {
+                "args": [],
+                "name": "QueryLatestParams",
+                "type": {
+                    "args": [],
+                    "entries": [
+                        {
+                            "name": "eventTypes",
+                            "type": {
+                                "args": [
+                                    {
+                                        "args": [
+                                            {
+                                                "args": [],
+                                                "entries": [],
+                                                "module": null,
+                                                "name": "EventType"
+                                            }
+                                        ],
+                                        "entries": [],
+                                        "module": null,
+                                        "name": "Array"
+                                    }
+                                ],
+                                "entries": [],
+                                "module": null,
+                                "name": "Optional"
+                            }
+                        }
+                    ],
+                    "module": null,
+                    "name": "Record"
+                }
+            },
+            "QueryParams": {
+                "args": [],
+                "name": "QueryParams",
+                "type": {
+                    "args": [],
+                    "entries": [
+                        {
+                            "name": "latest",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "QueryLatestParams"
+                            }
+                        },
+                        {
+                            "name": "timeseries",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "QueryTimeseriesParams"
+                            }
+                        },
+                        {
+                            "name": "server",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "QueryServerParams"
+                            }
+                        }
+                    ],
+                    "module": null,
+                    "name": "Choice"
+                }
+            },
+            "QueryResult": {
+                "args": [],
+                "name": "QueryResult",
+                "type": {
+                    "args": [],
+                    "entries": [
+                        {
+                            "name": "events",
+                            "type": {
+                                "args": [
+                                    {
+                                        "args": [],
+                                        "entries": [],
+                                        "module": null,
+                                        "name": "Event"
+                                    }
+                                ],
+                                "entries": [],
+                                "module": null,
+                                "name": "Array"
+                            }
+                        },
+                        {
+                            "name": "moreFollows",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "Boolean"
+                            }
+                        }
+                    ],
+                    "module": null,
+                    "name": "Record"
+                }
+            },
+            "QueryServerParams": {
                 "args": [],
-                "name": "QueryData",
+                "name": "QueryServerParams",
                 "type": {
                     "args": [],
                     "entries": [
                         {
                             "name": "serverId",
                             "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "Integer"
+                            }
+                        },
+                        {
+                            "name": "persisted",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "Boolean"
+                            }
+                        },
+                        {
+                            "name": "maxResults",
+                            "type": {
                                 "args": [
                                     {
                                         "args": [],
                                         "entries": [],
                                         "module": null,
                                         "name": "Integer"
                                     }
                                 ],
                                 "entries": [],
                                 "module": null,
                                 "name": "Optional"
                             }
                         },
                         {
-                            "name": "ids",
+                            "name": "lastEventId",
                             "type": {
                                 "args": [
                                     {
-                                        "args": [
-                                            {
-                                                "args": [],
-                                                "entries": [],
-                                                "module": null,
-                                                "name": "EventId"
-                                            }
-                                        ],
+                                        "args": [],
                                         "entries": [],
                                         "module": null,
-                                        "name": "Array"
+                                        "name": "EventId"
                                     }
                                 ],
                                 "entries": [],
                                 "module": null,
                                 "name": "Optional"
                             }
-                        },
+                        }
+                    ],
+                    "module": null,
+                    "name": "Record"
+                }
+            },
+            "QueryTimeseriesParams": {
+                "args": [],
+                "name": "QueryTimeseriesParams",
+                "type": {
+                    "args": [],
+                    "entries": [
                         {
-                            "name": "types",
+                            "name": "eventTypes",
                             "type": {
                                 "args": [
                                     {
                                         "args": [
                                             {
                                                 "args": [],
                                                 "entries": [],
@@ -608,30 +728,14 @@
                                 ],
                                 "entries": [],
                                 "module": null,
                                 "name": "Optional"
                             }
                         },
                         {
-                            "name": "payload",
-                            "type": {
-                                "args": [
-                                    {
-                                        "args": [],
-                                        "entries": [],
-                                        "module": null,
-                                        "name": "EventPayload"
-                                    }
-                                ],
-                                "entries": [],
-                                "module": null,
-                                "name": "Optional"
-                            }
-                        },
-                        {
                             "name": "order",
                             "type": {
                                 "args": [],
                                 "entries": [],
                                 "module": null,
                                 "name": "Order"
                             }
@@ -642,31 +746,38 @@
                                 "args": [],
                                 "entries": [],
                                 "module": null,
                                 "name": "OrderBy"
                             }
                         },
                         {
-                            "name": "uniqueType",
+                            "name": "maxResults",
                             "type": {
-                                "args": [],
+                                "args": [
+                                    {
+                                        "args": [],
+                                        "entries": [],
+                                        "module": null,
+                                        "name": "Integer"
+                                    }
+                                ],
                                 "entries": [],
                                 "module": null,
-                                "name": "Boolean"
+                                "name": "Optional"
                             }
                         },
                         {
-                            "name": "maxResults",
+                            "name": "lastEventId",
                             "type": {
                                 "args": [
                                     {
                                         "args": [],
                                         "entries": [],
                                         "module": null,
-                                        "name": "Integer"
+                                        "name": "EventId"
                                     }
                                 ],
                                 "entries": [],
                                 "module": null,
                                 "name": "Optional"
                             }
                         }
@@ -723,14 +834,43 @@
                             }
                         }
                     ],
                     "module": null,
                     "name": "Record"
                 }
             },
+            "Status": {
+                "args": [],
+                "name": "Status",
+                "type": {
+                    "args": [],
+                    "entries": [
+                        {
+                            "name": "standby",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "None"
+                            }
+                        },
+                        {
+                            "name": "operational",
+                            "type": {
+                                "args": [],
+                                "entries": [],
+                                "module": null,
+                                "name": "None"
+                            }
+                        }
+                    ],
+                    "module": null,
+                    "name": "Choice"
+                }
+            },
             "Timestamp": {
                 "args": [],
                 "name": "Timestamp",
                 "type": {
                     "args": [],
                     "entries": [
                         {
```

## hat/event/common/subscription/__init__.py

```diff
@@ -1,18 +1,22 @@
-from hat.event.common.subscription.common import (matches_query_type,
-                                                  BaseSubscription)
-from hat.event.common.subscription.pysubscription import PySubscription
-
+from collections.abc import Iterable
 
-__all__ = ['matches_query_type',
-           'BaseSubscription',
-           'Subscription',
-           'PySubscription']
+from hat.event.common.common import EventType
+from hat.event.common.subscription.common import Subscription
+from hat.event.common.subscription.pysubscription import PySubscription
 
 try:
     from hat.event.common.subscription.csubscription import CSubscription
 
-    Subscription = CSubscription
-    __all__ += ['CSubscription']
-
 except ImportError:
-    Subscription = PySubscription
+    CSubscription = None
+
+
+__all__ = ['Subscription',
+           'create_subscription']
+
+
+def create_subscription(query_types: Iterable[EventType]) -> Subscription:
+    if CSubscription is not None:
+        return CSubscription(query_types)
+
+    return PySubscription(query_types)
```

## hat/event/common/subscription/common.py

```diff
@@ -1,104 +1,62 @@
+from collections.abc import Iterable
 import abc
 import itertools
 import typing
 
-from hat.event.common.data import EventTypeSegment, EventType
+from hat.event.common.common import EventTypeSegment, EventType
 
 
-def matches_query_type(event_type: EventType,
-                       query_type: EventType
-                       ) -> bool:
-    """Determine if event type matches query type
-
-    Event type is tested if it matches query type according to the following
-    rules:
-
-        * Matching is performed on subtypes in increasing order.
-        * Event type is a match only if all its subtypes are matched by
-          corresponding query subtypes.
-        * Matching is finished when all query subtypes are exhausted.
-        * Query subtype '?' matches exactly one event subtype of any value.
-          The subtype must exist.
-        * Query subtype '*' matches 0 or more event subtypes of any value. It
-          must be the last query subtype.
-        * All other values of query subtype match exactly one event subtype
-          of the same value.
-        * Query type without subtypes is matched only by event type with no
-          subtypes.
-
-    As a consequence of aforementioned matching rules, event subtypes '*' and
-    '?' cannot be directly matched and it is advisable not to use them in event
-    types.
-
-    """
-    is_variable = bool(query_type and query_type[-1] == '*')
-    if is_variable:
-        query_type = query_type[:-1]
-
-    if len(event_type) < len(query_type):
-        return False
-
-    if len(event_type) > len(query_type) and not is_variable:
-        return False
-
-    for i, j in zip(event_type, query_type):
-        if j != '?' and i != j:
-            return False
-
-    return True
-
-
-class BaseSubscription(abc.ABC):
+class Subscription(abc.ABC):
     """Subscription defined by query event types"""
 
     @abc.abstractmethod
-    def __init__(self, query_types: typing.Iterable[EventType]):
-        pass
+    def __init__(self, query_types: Iterable[EventType]):
+        """Create subscription instance"""
 
     @abc.abstractmethod
-    def get_query_types(self) -> typing.Iterable[EventType]:
+    def get_query_types(self) -> Iterable[EventType]:
         """Calculate sanitized query event types"""
 
     @abc.abstractmethod
     def matches(self, event_type: EventType) -> bool:
         """Does `event_type` match subscription"""
 
     @abc.abstractmethod
-    def union(self, *others: 'BaseSubscription') -> 'BaseSubscription':
+    def union(self, *others: 'Subscription') -> 'Subscription':
         """Create new subscription including event types from this and
         other subscriptions."""
 
     @abc.abstractmethod
-    def intersection(self, *others: 'BaseSubscription') -> 'BaseSubscription':
+    def intersection(self, *others: 'Subscription') -> 'Subscription':
         """Create new subscription containing event types in common with
         other subscriptions."""
 
     @abc.abstractmethod
-    def isdisjoint(self, other: 'BaseSubscription') -> bool:
+    def isdisjoint(self, other: 'Subscription') -> bool:
         """Return ``True`` if this subscription has no event types in common
         with other subscription."""
 
 
 class Node(typing.NamedTuple):
     """Subscription tree node"""
     is_leaf: bool
     children: typing.Dict[EventTypeSegment, 'Node']
 
 
-def node_from_query_types(query_types: typing.Iterable[EventType]) -> Node:
+def node_from_query_types(query_types: Iterable[EventType]) -> Node:
     node = Node(False, {})
 
     for query_type in query_types:
         node = _add_query_type(node, query_type)
 
     return node
 
 
-def node_to_query_types(node: Node) -> typing.Iterable[EventType]:
+def node_to_query_types(node: Node) -> Iterable[EventType]:
     is_leaf, children = node
 
     if is_leaf and '*' not in children:
         yield ()
 
     for head, child in children.items():
         for rest in node_to_query_types(child):
```

## hat/event/common/subscription/csubscription.py

```diff
@@ -1,13 +1,13 @@
 from hat.event.common.subscription import common
 
 from hat.event.common.subscription import _csubscription
 
 
-class CSubscription(common.BaseSubscription):
+class CSubscription(common.Subscription):
     """C implementation of Subscription"""
 
     def __init__(self, query_types):
         self._subscription = _csubscription.Subscription(query_types)
 
     def get_query_types(self):
         return self._subscription.get_query_types()
```

## hat/event/common/subscription/pysubscription.py

```diff
@@ -1,11 +1,11 @@
 from hat.event.common.subscription import common
 
 
-class PySubscription(common.BaseSubscription):
+class PySubscription(common.Subscription):
     """Python implementation of Subscription"""
 
     def __init__(self, query_types):
         self._root = common.node_from_query_types(query_types)
 
     def get_query_types(self):
         return common.node_to_query_types(self._root)
```

## hat/event/eventer/__init__.py

```diff
@@ -1,72 +1,28 @@
-"""Eventer communication protocol
+"""Eventer communication protocol"""
 
-This package provides Eventer Client as:
-    * low-level interface - `Client`
-    * high-level interface - `Component`
-
-`connect` is used for establishing single eventer connection
-with Eventer Server which is represented by `Client`. Once
-connection is terminated (signaled with `Client.closed`),
-it is up to user to repeat `connect` call and create new `Client`
-instance, if additional communication with Event Server is required.
-
-Example of low-level interface usage::
-
-    client = await hat.event.eventer.connect(
-        'tcp+sbs://127.0.0.1:23012',
-        [['x', 'y', 'z']])
-
-    registered_events = await client.register_with_response([
-        hat.event.common.RegisterEvent(
-            event_type=['x', 'y', 'z'],
-            source_timestamp=hat.event.common.now(),
-            payload=hat.event.common.EventPayload(
-                type=hat.event.common.EventPayloadType.BINARY,
-                data=b'test'))])
-
-    received_events = await client.receive()
-
-    queried_events = await client.query(
-        hat.event.common.QueryData(
-            event_types=[['x', 'y', 'z']],
-            max_results=1))
-
-    assert registered_events == received_events
-    assert received_events == queried_events
-
-    await client.async_close()
-
-`Component` provides high-level interface for continuous communication with
-currenty active Event Server based on information obtained from Monitor Server.
-This implementation repeatedly tries to create active connection
-with Eventer Server. When this connection is created, users code is notified by
-calling `component_cb` callback. Once connection is closed, user defined
-resource, resulting from `component_cb`, is cancelled and `Component` repeats
-connection estabishment process.
-
-"""
-
-from hat.event.eventer.client import (connect,
-                                      Client,
-                                      Runner,
-                                      ComponentCb,
-                                      Component)
-from hat.event.eventer.server import (ClientId,
-                                      ClientCb,
+from hat.event.eventer.client import (StatusCb,
+                                      EventsCb,
+                                      EventerInitError,
+                                      connect,
+                                      Client)
+from hat.event.eventer.server import (ConnectionId,
+                                      ConnectionInfo,
+                                      ConnectionCb,
                                       RegisterCb,
                                       QueryCb,
                                       listen,
                                       Server)
 
 
-__all__ = ['connect',
+__all__ = ['StatusCb',
+           'EventsCb',
+           'EventerInitError',
+           'connect',
            'Client',
-           'Runner',
-           'ComponentCb',
-           'Component',
-           'ClientId',
-           'ClientCb',
+           'ConnectionId',
+           'ConnectionInfo',
+           'ConnectionCb',
            'RegisterCb',
            'QueryCb',
            'listen',
            'Server']
```

## hat/event/eventer/client.py

```diff
@@ -1,59 +1,115 @@
+"""Eventer Client"""
+
+from collections.abc import Collection, Iterable
 import asyncio
 import logging
 import typing
 
 from hat import aio
-from hat import chatter
-from hat import util
-import hat.monitor.client
+from hat.drivers import chatter
+from hat.drivers import tcp
 
-from hat.event import common
+from hat.event.eventer import common
 
 
 mlog: logging.Logger = logging.getLogger(__name__)
 """Module logger"""
 
-
-async def connect(address: str,
-                  subscriptions: list[common.EventType] = [],
+StatusCb: typing.TypeAlias = aio.AsyncCallable[['Client', common.Status],
+                                               None]
+"""Status callback"""
+
+EventsCb: typing.TypeAlias = aio.AsyncCallable[['Client',
+                                                Collection[common.Event]],
+                                               None]
+"""Events callback"""
+
+
+class EventerInitError(Exception):
+    """Eventer initialization error"""
+
+
+async def connect(addr: tcp.Address,
+                  client_name: str,
+                  *,
+                  client_token: str | None = None,
+                  subscriptions: Iterable[common.EventType] = [],
+                  server_id: common.ServerId | None = None,
+                  persisted: bool = False,
+                  status_cb: StatusCb | None = None,
+                  events_cb: EventsCb | None = None,
                   **kwargs
                   ) -> 'Client':
-    """Connect to eventer server
+    """Connect to Eventer Server
 
-    For address format see `hat.chatter.connect` coroutine.
+    Arguments `client_name` and optional `client_token` identifies eventer
+    client.
 
     According to Event Server specification, each subscription is event
     type identifier which can contain special subtypes ``?`` and ``*``.
-    Subtype ``?`` can occure at any position inside event type identifier
+    Subtype ``?`` can occur at any position inside event type identifier
     and is used as replacement for any single subtype. Subtype ``*`` is valid
     only as last subtype in event type identifier and is used as replacement
     for zero or more arbitrary subtypes.
 
-    If subscription is empty list, client doesn't subscribe for any events and
-    will not receive server's notifications.
+    If `subscriptions` is empty list, client doesn't subscribe for any events
+    and will not receive server's notifications.
 
-    Args:
-        address: event server's address
-        subscriptions: subscriptions
-        kwargs: additional arguments passed to `hat.chatter.connect` coroutine
+    If `server_id` is ``None``, client will receive all event notifications,
+    in accordance to `subscriptions`, regardless of event's server id. If
+    `server_id` is set, Eventer Server will only send events notifications
+    for events with provided server id.
+
+    If `persisted` is set to ``True``, Eventer Server will notify events
+    after they are persisted (flushed to disk). Otherwise, events are
+    notified immediately after registration.
+
+    Additional arguments are passed to `hat.chatter.connect` coroutine.
 
     """
     client = Client()
+    client._status_cb = status_cb
+    client._events_cb = events_cb
+    client._loop = asyncio.get_running_loop()
     client._conv_futures = {}
-    client._event_queue = aio.Queue()
+    client._status = common.Status.STANDBY
 
-    client._conn = await chatter.connect(common.sbs_repo, address, **kwargs)
+    client._conn = await chatter.connect(addr, **kwargs)
 
-    if subscriptions:
-        client._conn.send(chatter.Data(module='HatEventer',
-                                       type='MsgSubscribe',
-                                       data=[list(i) for i in subscriptions]))
+    try:
+        req_data = {'clientName': client_name,
+                    'clientToken': _optional_to_sbs(client_token),
+                    'subscriptions': [list(i) for i in subscriptions],
+                    'serverId': _optional_to_sbs(server_id),
+                    'persisted': persisted}
+        conv = await common.send_msg(conn=client._conn,
+                                     msg_type='HatEventer.MsgInitReq',
+                                     msg_data=req_data,
+                                     last=False)
+
+        res, res_type, res_data = await common.receive_msg(client._conn)
+        if res_type != 'HatEventer.MsgInitRes' or res.conv != conv:
+            raise Exception('invalid init response')
+
+        if res_data[0] == 'success':
+            client._status = common.Status(common.status_from_sbs(res_data[1]))
+
+        elif res_data[0] == 'error':
+            raise EventerInitError(res_data[1])
+
+        else:
+            raise ValueError('unsupported init response')
+
+        client.async_group.spawn(client._receive_loop)
+
+    except BaseException:
+        await aio.uncancellable(client.async_close())
+        raise
 
-    client.async_group.spawn(client._receive_loop)
     return client
 
 
 class Client(aio.Resource):
     """Eventer client
 
     For creating new client see `connect` coroutine.
@@ -61,313 +117,141 @@
     """
 
     @property
     def async_group(self) -> aio.Group:
         """Async group"""
         return self._conn.async_group
 
-    async def receive(self) -> list[common.Event]:
-        """Receive subscribed event notifications
-
-        Raises:
-            ConnectionError
-
-        """
-        try:
-            return await self._event_queue.get()
-
-        except aio.QueueClosedError:
-            raise ConnectionError()
-
-    def register(self, events: list[common.RegisterEvent]):
-        """Register events
+    @property
+    def status(self) -> common.Status:
+        """Status"""
+        return self._status
+
+    async def register(self,
+                       events: Collection[common.RegisterEvent],
+                       with_response: bool = False
+                       ) -> Collection[common.Event] | None:
+        """Register events and optionally wait for response
 
-        Raises:
-            ConnectionError
+        If `with_response` is ``True``, this coroutine returns list of events
+        or ``None`` if registration failure occurred.
 
         """
-        msg_data = chatter.Data(module='HatEventer',
-                                type='MsgRegisterReq',
-                                data=[common.register_event_to_sbs(i)
-                                      for i in events])
-        self._conn.send(msg_data)
-
-    async def register_with_response(self,
-                                     events: list[common.RegisterEvent]
-                                     ) -> list[common.Event | None]:
-        """Register events
-
-        Each `common.RegisterEvent` from `events` is paired with results
-        `common.Event` if new event was successfuly created or ``None`` is new
-        event could not be created.
+        msg_data = [common.register_event_to_sbs(i) for i in events]
+        conv = await common.send_msg(conn=self._conn,
+                                     msg_type='HatEventer.MsgRegisterReq',
+                                     msg_data=msg_data,
+                                     last=not with_response)
 
-        Raises:
-            ConnectionError
-
-        """
-        msg_data = chatter.Data(module='HatEventer',
-                                type='MsgRegisterReq',
-                                data=[common.register_event_to_sbs(i)
-                                      for i in events])
-        conv = self._conn.send(msg_data, last=False)
-        return await self._wait_conv_res(conv)
+        if with_response:
+            return await self._wait_conv_res(conv)
 
     async def query(self,
-                    data: common.QueryData
-                    ) -> list[common.Event]:
-        """Query events from server
+                    params: common.QueryParams
+                    ) -> common.QueryResult:
+        """Query events from server"""
+        msg_data = common.query_params_to_sbs(params)
+        conv = await common.send_msg(conn=self._conn,
+                                     msg_type='HatEventer.MsgQueryReq',
+                                     msg_data=msg_data,
+                                     last=False)
 
-        Raises:
-            ConnectionError
-
-        """
-        msg_data = chatter.Data(module='HatEventer',
-                                type='MsgQueryReq',
-                                data=common.query_to_sbs(data))
-        conv = self._conn.send(msg_data, last=False)
         return await self._wait_conv_res(conv)
 
     async def _receive_loop(self):
         mlog.debug("starting receive loop")
         try:
             while True:
                 mlog.debug("waiting for incoming message")
-                msg = await self._conn.receive()
-                msg_type = msg.data.module, msg.data.type
+                msg, msg_type, msg_data = await common.receive_msg(self._conn)
 
-                if msg_type == ('HatEventer', 'MsgNotify'):
-                    mlog.debug("received event notification")
-                    self._process_msg_notify(msg)
+                if msg_type == 'HatEventer.MsgStatusNotify':
+                    mlog.debug("received status notification")
+                    await self._process_msg_status_notify(msg, msg_data)
 
-                elif msg_type == ('HatEventer', 'MsgQueryRes'):
-                    mlog.debug("received query response")
-                    self._process_msg_query_res(msg)
+                elif msg_type == 'HatEventer.MsgEventsNotify':
+                    mlog.debug("received events notification")
+                    await self._process_msg_events_notify(msg, msg_data)
 
-                elif msg_type == ('HatEventer', 'MsgRegisterRes'):
+                elif msg_type == 'HatEventer.MsgRegisterRes':
                     mlog.debug("received register response")
-                    self._process_msg_register_res(msg)
+                    await self._process_msg_register_res(msg, msg_data)
+
+                elif msg_type == 'HatEventer.MsgQueryRes':
+                    mlog.debug("received query response")
+                    await self._process_msg_query_res(msg, msg_data)
 
                 else:
                     raise Exception("unsupported message type")
 
         except ConnectionError:
             pass
 
         except Exception as e:
             mlog.error("read loop error: %s", e, exc_info=e)
 
         finally:
             mlog.debug("stopping receive loop")
             self.close()
-            self._event_queue.close()
-            for f in self._conv_futures.values():
-                if not f.done():
-                    f.set_exception(ConnectionError())
+
+            for future in self._conv_futures.values():
+                if not future.done():
+                    future.set_exception(ConnectionError())
 
     async def _wait_conv_res(self, conv):
         if not self.is_open:
             raise ConnectionError()
 
-        response_future = asyncio.Future()
-        self._conv_futures[conv] = response_future
-        try:
-            return await response_future
-        finally:
-            self._conv_futures.pop(conv, None)
-
-    def _process_msg_notify(self, msg):
-        events = [common.event_from_sbs(e) for e in msg.data.data]
-        self._event_queue.put_nowait(events)
-
-    def _process_msg_query_res(self, msg):
-        f = self._conv_futures.get(msg.conv)
-        if not f or f.done():
-            return
-        events = [common.event_from_sbs(e) for e in msg.data.data]
-        f.set_result(events)
-
-    def _process_msg_register_res(self, msg):
-        f = self._conv_futures.get(msg.conv)
-        if not f or f.done():
-            return
-        events = [common.event_from_sbs(e) if t == 'event' else None
-                  for t, e in msg.data.data]
-        f.set_result(events)
-
-
-Runner: typing.TypeAlias = aio.Resource
-"""Component runner"""
-
-ComponentCb: typing.TypeAlias = typing.Callable[[Client], Runner]
-"""Component callback"""
-
-
-class Component(aio.Resource):
-    """Eventer component
-
-    High-level interface for communication with Event Server, based on
-    information obtained from Monitor Server.
-
-    Instance of this class tries to establish active connection with
-    Event Server within monitor component group `server_group`. Once this
-    connection is established, `component_cb` is called with currently active
-    `Client` instance. Result of calling `component_cb` should be `Runner`
-    representing user defined components activity associated with connection
-    to active Event Server. Once connection to Event Server is closed or new
-    active Event Server is detected, associated `Runner` is closed. If new
-    connection to Event Server is successfully established,
-    `component_cb` will be called again to create new `Runner` associated with
-    new instance of `Client`.
-
-    `component_cb` is called when:
-        * new active `Client` is created
-
-    `Runner`, returned by `component_cb`, is closed when:
-        * `Component` is closed
-        * connection to Event Server is closed
-        * different active Event Server is detected from Monitor Server's list
-          of components
-
-    `Component` is closed when:
-        * connection to Monitor Server is closed
-        * `Runner`, returned by `component_cb`, is closed by causes other
-          than change of active Event Server
-
-    `reconnect_delay` defines delay in seconds before trying to reconnect to
-    Event Server.
-
-    """
-
-    def __init__(self,
-                 monitor_client: hat.monitor.client.Client,
-                 server_group: str,
-                 component_cb: ComponentCb,
-                 subscriptions: list[common.EventType] = [],
-                 reconnect_delay: float = 0.5):
-        self._monitor_client = monitor_client
-        self._server_group = server_group
-        self._component_cb = component_cb
-        self._subscriptions = subscriptions
-        self._reconnect_delay = reconnect_delay
-        self._async_group = aio.Group()
-        self._address_queue = aio.Queue()
-
-        self.async_group.spawn(self._monitor_loop)
-        self.async_group.spawn(self._address_loop)
-
-        self.async_group.spawn(aio.call_on_done, monitor_client.wait_closing(),
-                               self.close)
-
-    @property
-    def async_group(self):
-        return self._async_group
-
-    async def _monitor_loop(self):
-        last_address = None
-        changes = aio.Queue()
-
-        def on_change():
-            changes.put_nowait(None)
-
-        def info_filter(info):
-            return (info.group == self._server_group and
-                    info.blessing_req.token is not None and
-                    info.blessing_req.token == info.blessing_res.token)
+        future = self._loop.create_future()
+        self._conv_futures[conv] = future
 
         try:
-            with self._monitor_client.register_change_cb(on_change):
-                while True:
-                    info = util.first(self._monitor_client.components,
-                                      info_filter)
-                    address = (info.data.get('eventer_server_address')
-                               if info else None)
-
-                    if address and address != last_address:
-                        mlog.debug("new server address: %s", address)
-                        last_address = address
-                        self._address_queue.put_nowait(address)
-
-                    await changes.get()
-
-        except Exception as e:
-            mlog.error("component monitor loop error: %s", e, exc_info=e)
+            return await future
 
         finally:
-            self.close()
-
-    async def _address_loop(self):
-        try:
-            address = None
-            while True:
-                while not address:
-                    address = await self._address_queue.get_until_empty()
+            self._conv_futures.pop(conv, None)
 
-                async with self.async_group.create_subgroup() as subgroup:
-                    address_future = subgroup.spawn(
-                        self._address_queue.get_until_empty)
-                    client_future = subgroup.spawn(self._client_loop, address)
+    async def _process_msg_status_notify(self, msg, msg_data):
+        self._status = common.status_from_sbs(msg_data)
 
-                    await asyncio.wait([address_future, client_future],
-                                       return_when=asyncio.FIRST_COMPLETED)
+        if self._status_cb:
+            await aio.call(self._status_cb, self, self._status)
 
-                    if address_future.done():
-                        address = address_future.result()
+        await common.send_msg(conn=self._conn,
+                              msg_type='HatEventer.MsgStatusAck',
+                              msg_data=None,
+                              conv=msg.conv)
 
-                    elif client_future.done():
-                        break
+    async def _process_msg_events_notify(self, msg, msg_data):
+        events = [common.event_from_sbs(event) for event in msg_data]
 
-        except Exception as e:
-            mlog.error("component address loop error: %s", e, exc_info=e)
+        if self._events_cb:
+            await aio.call(self._events_cb, self, events)
 
-        finally:
-            self.close()
+    async def _process_msg_register_res(self, msg, msg_data):
+        if msg_data[0] == 'events':
+            result = [common.event_from_sbs(event) for event in msg_data[1]]
 
-    async def _client_loop(self, address):
-        try:
-            while True:
-                try:
-                    mlog.debug("connecting to server %s", address)
-                    client = await connect(address, self._subscriptions)
-
-                except Exception as e:
-                    mlog.warning("error connecting to server: %s", e,
-                                 exc_info=e)
-                    await asyncio.sleep(self._reconnect_delay)
-                    continue
-
-                try:
-                    mlog.debug("connected to server")
-                    async with self.async_group.create_subgroup() as subgroup:
-                        client_future = subgroup.spawn(client.wait_closing)
-                        runner_future = subgroup.spawn(self._runner_loop,
-                                                       client)
-
-                        await asyncio.wait([client_future, runner_future],
-                                           return_when=asyncio.FIRST_COMPLETED)
-
-                        if client_future.done():
-                            pass
+        elif msg_data[0] == 'failure':
+            result = None
 
-                        elif runner_future.done():
-                            break
+        else:
+            raise ValueError('unsupported register response')
 
-                finally:
-                    await aio.uncancellable(client.async_close())
+        future = self._conv_futures.get(msg.conv)
+        if not future or future.done():
+            return
 
-                mlog.debug("connection to server closed")
-                await asyncio.sleep(self._reconnect_delay)
+        future.set_result(result)
 
-        except Exception as e:
-            mlog.error("component client loop error: %s", e, exc_info=e)
+    async def _process_msg_query_res(self, msg, msg_data):
+        result = common.query_result_from_sbs(msg_data)
 
-    async def _runner_loop(self, client):
-        try:
-            runner = self._component_cb(client)
+        future = self._conv_futures.get(msg.conv)
+        if not future or future.done():
+            return
 
-            try:
-                await runner.wait_closing()
+        future.set_result(result)
 
-            finally:
-                await aio.uncancellable(runner.async_close())
 
-        except Exception as e:
-            mlog.error("component runner loop error: %s", e, exc_info=e)
+def _optional_to_sbs(value):
+    return ('value', value) if value is not None else ('none', None)
```

## hat/event/eventer/server.py

```diff
@@ -1,197 +1,264 @@
+from collections.abc import Collection
+import asyncio
+import collections
 import contextlib
 import itertools
 import logging
 import typing
 
 from hat import aio
-from hat import chatter
+from hat.drivers import chatter
+from hat.drivers import tcp
 
-from hat.event import common
+from hat.event.eventer import common
 
 
 mlog: logging.Logger = logging.getLogger(__name__)
 """Module logger"""
 
-ClientId: typing.TypeAlias = int
-"""Client identifier"""
+ConnectionId: typing.TypeAlias = int
+"""Connection identifier"""
 
-ClientCb: typing.TypeAlias = aio.AsyncCallable[[ClientId], None]
-"""Client connected/disconnected callback"""
 
-RegisterCb: typing.TypeAlias = aio.AsyncCallable[[ClientId,
-                                                  list[common.RegisterEvent]],
-                                                 list[common.Event]]
+class ConnectionInfo(typing.NamedTuple):
+    id: ConnectionId
+    client_name: str
+    client_token: str | None
+    subscription: common.Subscription
+    server_id: int | None
+    persisted: bool
+
+
+ConnectionCb: typing.TypeAlias = aio.AsyncCallable[[ConnectionInfo], None]
+"""Connected/disconnected callback"""
+
+RegisterCb: typing.TypeAlias = aio.AsyncCallable[
+    [ConnectionInfo, Collection[common.RegisterEvent]],
+    Collection[common.Event] | None]
 """Register callback"""
 
-QueryCb: typing.TypeAlias = aio.AsyncCallable[[ClientId,
-                                               list[common.QueryData]],
-                                              list[common.Event]]
+QueryCb: typing.TypeAlias = aio.AsyncCallable[
+    [ConnectionInfo, common.QueryParams],
+    common.QueryResult]
 """Query callback"""
 
 
-async def listen(address: str,
-                 connected_cb: ClientCb | None = None,
-                 disconnected_cb: ClientCb | None = None,
+async def listen(addr: tcp.Address,
+                 *,
+                 status: common.Status = common.Status.STANDBY,
+                 connected_cb: ConnectionCb | None = None,
+                 disconnected_cb: ConnectionCb | None = None,
                  register_cb: RegisterCb | None = None,
-                 query_cb: QueryCb | None = None
+                 query_cb: QueryCb | None = None,
+                 close_timeout: float = 0.5,
+                 **kwargs
                  ) -> 'Server':
-    """Create eventer server instance"""
+    """Create listening Eventer Server instance"""
     server = Server()
+    server._status = status
     server._connected_cb = connected_cb
     server._disconnected_cb = disconnected_cb
     server._register_cb = register_cb
     server._query_cb = query_cb
-    server._next_client_ids = itertools.count(1)
-    server._conns = {}
+    server._close_timeout = close_timeout
+    server._loop = asyncio.get_running_loop()
+    server._next_conn_ids = itertools.count(1)
+    server._conn_infos = {}
+    server._conn_conv_futures = {}
 
-    server._srv = await chatter.listen(sbs_repo=common.sbs_repo,
-                                       address=address,
-                                       connection_cb=server._on_connection)
-    mlog.debug("listening on %s", address)
+    server._srv = await chatter.listen(server._connection_loop, addr, **kwargs)
+    mlog.debug("listening on %s", addr)
 
     return server
 
 
 class Server(aio.Resource):
 
     @property
     def async_group(self) -> aio.Group:
         """Async group"""
         return self._srv.async_group
 
-    def notify(self, events: list[common.Event]):
-        """Notify events to subscribed clients"""
-        for conn in self._conns.values():
-            conn.notify(events)
-
-    async def _on_connection(self, conn):
-        client_id = next(self._next_client_ids)
-
-        try:
-            if self._connected_cb:
-                await aio.call(self._connected_cb, client_id)
-
-            self._conns[client_id] = _Connection(conn=conn,
-                                                 client_id=client_id,
-                                                 register_cb=self._register_cb,
-                                                 query_cb=self._query_cb)
-
-            await self._conns[client_id].wait_closing()
-
-        except Exception as e:
-            mlog.error("on connection error: %s", e, exc_info=e)
-
-        finally:
-            conn.close()
-
-            if self._disconnected_cb:
-                with contextlib.suppress(Exception):
-                    await aio.call(self._disconnected_cb, client_id)
-
-
-class _Connection(aio.Resource):
-
-    def __init__(self,
-                 conn: chatter.Connection,
-                 client_id: int,
-                 register_cb: RegisterCb | None,
-                 query_cb: QueryCb | None):
-        self._conn = conn
-        self._client_id = client_id
-        self._register_cb = register_cb
-        self._query_cb = query_cb
-        self._subscription = None
-
-        self.async_group.spawn(self._connection_loop)
-
-    @property
-    def async_group(self) -> aio.Group:
-        return self._conn.async_group
+    async def set_status(self, status: common.Status):
+        """Set status and wait for acks"""
+        if self._status == status:
+            return
 
-    def notify(self, events: list[common.Event]):
-        if not self.is_open or not self._subscription:
+        self._status = status
+        tasks = [self.async_group.spawn(self._notify_status, conn, status)
+                 for conn in self._conn_infos.keys()]
+        if not tasks:
             return
 
-        events = [event for event in events
-                  if self._subscription.matches(event.event_type)]
+        await asyncio.wait(tasks)
 
-        if not events:
-            return
+    async def notify_events(self,
+                            events: Collection[common.Event],
+                            persisted: bool):
+        """Notify events to clients"""
+        for conn, info in list(self._conn_infos.items()):
+            if info.persisted != persisted:
+                continue
+
+            filtered_events = collections.deque(
+                event for event in events
+                if (info.subscription.matches(event.type) and
+                    (info.server_id is None or
+                     info.server_id == event.id.server)))
+            if not filtered_events:
+                continue
 
-        data = chatter.Data('HatEventer', 'MsgNotify',
-                            [common.event_to_sbs(e) for e in events])
-        with contextlib.suppress(ConnectionError):
-            self._conn.send(data)
+            await self._notify_events(conn, filtered_events)
 
-    async def _connection_loop(self):
+    async def _connection_loop(self, conn):
         mlog.debug("starting connection loop")
+        conn_id = next(self._next_conn_ids)
+        info = None
+
         try:
+            req, req_type, req_data = await common.receive_msg(conn)
+            if req_type != 'HatEventer.MsgInitReq':
+                raise Exception('invalid init request type')
+
+            try:
+                info = ConnectionInfo(
+                    id=conn_id,
+                    client_name=req_data['clientName'],
+                    client_token=_optional_from_sbs(req_data['clientToken']),
+                    subscription=common.create_subscription(
+                        tuple(i) for i in req_data['subscriptions']),
+                    server_id=_optional_from_sbs(req_data['serverId']),
+                    persisted=req_data['persisted'])
+
+                if self._connected_cb:
+                    await aio.call(self._connected_cb, info)
+
+                res_data = 'success', common.status_to_sbs(self._status)
+                self._conn_infos[conn] = info
+
+            except Exception as e:
+                info = None
+                res_data = 'error', str(e)
+
+            mlog.debug("sending init response %s", res_data[0])
+            await common.send_msg(conn, 'HatEventer.MsgInitRes', res_data,
+                                  conv=req.conv)
+
+            if res_data[0] != 'success':
+                with contextlib.suppress(asyncio.TimeoutError):
+                    await aio.wait_for(conn.wait_closing(),
+                                       self._close_timeout)
+                return
+
             while True:
                 mlog.debug("waiting for incomming messages")
-                msg = await self._conn.receive()
-                msg_type = msg.data.module, msg.data.type
+                msg, msg_type, msg_data = await common.receive_msg(conn)
 
-                if msg_type == ('HatEventer', 'MsgSubscribe'):
-                    mlog.debug("received subscribe message")
-                    await self._process_msg_subscribe(msg)
+                if msg_type == 'HatEventer.MsgStatusAck':
+                    mlog.debug("received status ack")
+                    future = self._conn_conv_futures.get((conn, msg.conv))
+                    if future and not future.done():
+                        future.set_result(None)
 
-                elif msg_type == ('HatEventer', 'MsgRegisterReq'):
+                elif msg_type == 'HatEventer.MsgRegisterReq':
                     mlog.debug("received register request")
-                    await self._process_msg_register(msg)
+                    await self._process_msg_register(conn, info, msg, msg_data)
 
-                elif msg_type == ('HatEventer', 'MsgQueryReq'):
+                elif msg_type == 'HatEventer.MsgQueryReq':
                     mlog.debug("received query request")
-                    await self._process_msg_query(msg)
+                    await self._process_msg_query(conn, info, msg, msg_data)
 
                 else:
                     raise Exception('unsupported message type')
 
         except ConnectionError:
             pass
 
         except Exception as e:
-            mlog.error("connection loop error: %s", e, exc_info=e)
+            mlog.error("on connection error: %s", e, exc_info=e)
 
         finally:
-            mlog.debug("closing connection loop")
-            self._conn.close()
+            mlog.debug("stopping connection loop")
+            conn.close()
+            self._conn_infos.pop(conn, None)
 
-    async def _process_msg_subscribe(self, msg):
-        self._subscription = common.Subscription([tuple(i)
-                                                  for i in msg.data.data])
+            for future in self._conn_conv_futures.values():
+                if not future.done():
+                    future.set_exception(ConnectionError())
+
+            if self._disconnected_cb and info:
+                with contextlib.suppress(Exception):
+                    await aio.call(self._disconnected_cb, info)
 
-    async def _process_msg_register(self, msg):
+    async def _process_msg_register(self, conn, info, req, req_data):
         register_events = [common.register_event_from_sbs(i)
-                           for i in msg.data.data]
+                           for i in req_data]
 
         if self._register_cb:
-            events = await aio.call(self._register_cb, self._client_id,
-                                    register_events)
+            events = await aio.call(self._register_cb, info, register_events)
 
         else:
-            events = [None for _ in register_events]
+            events = None
 
-        if msg.last:
+        if req.last:
             return
 
-        data = chatter.Data(module='HatEventer',
-                            type='MsgRegisterRes',
-                            data=[(('event', common.event_to_sbs(e))
-                                   if e is not None else ('failure', None))
-                                  for e in events])
-        self._conn.send(data, conv=msg.conv)
+        if events is not None:
+            res_data = 'events', [common.event_to_sbs(event)
+                                  for event in events]
+
+        else:
+            res_data = 'failure', None
+
+        await common.send_msg(conn, 'HatEventer.MsgRegisterRes', res_data,
+                              conv=req.conv)
 
-    async def _process_msg_query(self, msg):
-        query_data = common.query_from_sbs(msg.data.data)
+    async def _process_msg_query(self, conn, info, req, req_data):
+        params = common.query_params_from_sbs(req_data)
 
         if self._query_cb:
-            events = await aio.call(self._query_cb, self._client_id,
-                                    query_data)
+            result = await aio.call(self._query_cb, info, params)
 
         else:
-            events = []
+            result = common.QueryResult(events=[],
+                                        more_follows=False)
+
+        res_data = common.query_result_to_sbs(result)
+        await common.send_msg(conn, 'HatEventer.MsgQueryRes', res_data,
+                              conv=req.conv)
+
+    async def _notify_status(self, conn, status):
+        try:
+            req_data = common.status_to_sbs(self._status)
+            conv = await common.send_msg(conn, 'HatEventer.MsgStatusNotify',
+                                         req_data)
+
+            future = self._loop.create_future()
+            self._conn_conv_futures[(conn, conv)] = future
+
+            try:
+                await future
+
+            finally:
+                self._conn_conv_futures.pop((conn, conv))
+
+        except ConnectionError:
+            pass
+
+        except Exception as e:
+            mlog.error("notify status error: %s", e, exc_info=e)
+
+    async def _notify_events(self, conn, events):
+        try:
+            msg_data = [common.event_to_sbs(event) for event in events]
+            await common.send_msg(conn, 'HatEventer.MsgEventsNotify', msg_data)
+
+        except ConnectionError:
+            pass
+
+        except Exception as e:
+            mlog.error("notify events error: %s", e, exc_info=e)
+
 
-        data = chatter.Data(module='HatEventer',
-                            type='MsgQueryRes',
-                            data=[common.event_to_sbs(e) for e in events])
-        self._conn.send(data, conv=msg.conv)
+def _optional_from_sbs(data):
+    return data[1] if data[0] == 'value' else None
```

## hat/event/server/engine.py

```diff
@@ -1,58 +1,53 @@
 """Engine"""
 
+from collections.abc import Collection, Iterable
 import asyncio
 import collections
-import importlib
 import logging
 
 from hat import aio
 from hat import json
-from hat import util
 
-from hat.event.server import common
+from hat.event import common
 
 
 mlog: logging.Logger = logging.getLogger(__name__)
 """Module logger"""
 
 
-async def create_engine(conf: json.Data,
-                        backend: common.Backend
+async def create_engine(backend: common.Backend,
+                        module_confs: Iterable[json.Data],
+                        server_id: int,
+                        register_queue_size: int = 1024
                         ) -> 'Engine':
-    """Create engine
-
-    Args:
-        conf: configuration defined by
-            ``hat-event://main.yaml#/definitions/engine``
-        backend: backend
-
-    """
+    """Create engine"""
     engine = Engine()
     engine._backend = backend
+    engine._server_id = server_id
+    engine._loop = asyncio.get_running_loop()
     engine._async_group = aio.Group()
-    engine._register_queue = aio.Queue()
-    engine._register_cbs = util.CallbackRegistry()
+    engine._register_queue = aio.Queue(register_queue_size)
     engine._source_modules = collections.deque()
 
-    engine._last_event_id = await backend.get_last_event_id(conf['server_id'])
+    engine._last_event_id = await backend.get_last_event_id(server_id)
 
-    future = asyncio.Future()
+    future = engine._loop.create_future()
     source = common.Source(type=common.SourceType.ENGINE, id=0)
     events = [engine._create_status_reg_event('STARTED')]
     engine._register_queue.put_nowait((future, source, events))
-    try:
-        for source_id, module_conf in enumerate(conf['modules']):
-            py_module = importlib.import_module(module_conf['module'])
 
+    try:
+        for source_id, module_conf in enumerate(module_confs):
+            info = common.import_module_info(module_conf['module'])
             source = common.Source(type=common.SourceType.MODULE,
                                    id=source_id)
 
             module = await engine.async_group.spawn(
-                aio.call, py_module.create, module_conf, engine, source)
+                aio.call, info.create, module_conf, engine, source)
             engine.async_group.spawn(aio.call_on_cancel, module.async_close)
             engine.async_group.spawn(aio.call_on_done, module.wait_closing(),
                                      engine.close)
 
             engine._source_modules.append((source, module))
 
         engine.async_group.spawn(engine._register_loop)
@@ -67,125 +62,142 @@
 class Engine(common.Engine):
 
     @property
     def async_group(self) -> aio.Group:
         """Async group"""
         return self._async_group
 
-    def register_events_cb(self,
-                           cb: common.EventsCb
-                           ) -> util.RegisterCallbackHandle:
-        """Register events callback"""
-        return self._register_cbs.register(cb)
+    @property
+    def server_id(self) -> int:
+        """Event server identifier"""
+        return self._server_id
 
     async def register(self,
                        source: common.Source,
-                       events: list[common.RegisterEvent]
-                       ) -> list[common.Event | None]:
+                       events: Collection[common.RegisterEvent]
+                       ) -> Collection[common.Event] | None:
         """Register events"""
         if not events:
             return []
 
-        future = asyncio.Future()
-        self._register_queue.put_nowait((future, source, events))
+        future = self._loop.create_future()
+
+        try:
+            await self._register_queue.put((future, source, events))
+
+        except aio.QueueClosedError:
+            raise Exception('engine closed')
+
         return await future
 
     async def query(self,
-                    data: common.QueryData
-                    ) -> list[common.Event]:
+                    params: common.QueryParams
+                    ) -> common.QueryResult:
         """Query events"""
-        return await self._backend.query(data)
+        return await self._backend.query(params)
 
     async def _register_loop(self):
         future = None
         mlog.debug("starting register loop")
 
         try:
             while True:
                 mlog.debug("waiting for register requests")
                 future, source, register_events = \
                     await self._register_queue.get()
+
                 mlog.debug("processing session")
                 events = await self._process_sessions(source, register_events)
 
                 mlog.debug("registering to backend")
                 events = await self._backend.register(events)
-                if not future.done():
-                    result = events[:len(register_events)]
-                    future.set_result(result)
-
-                events = [event for event in events if event]
-                if events:
-                    self._register_cbs.notify(events)
+
+                if future.done():
+                    continue
+
+                result = (
+                    list(event for event, _ in zip(events, register_events))
+                    if events is not None else None)
+                future.set_result(result)
 
         except Exception as e:
             mlog.error("register loop error: %s", e, exc_info=e)
 
         finally:
             mlog.debug("register loop closed")
             self.close()
             self._register_queue.close()
 
             while True:
                 if future and not future.done():
-                    future.set_exception(Exception('module engine closed'))
+                    future.set_exception(Exception('engine closed'))
                 if self._register_queue.empty():
                     break
                 future, _, __ = self._register_queue.get_nowait()
 
+            timestamp = self._create_session()
             status_reg_event = self._create_status_reg_event('STOPPED')
-            events = [self._create_event(common.now(), status_reg_event)]
+            events = [self._create_event(timestamp, status_reg_event)]
             await self._backend.register(events)
 
     async def _process_sessions(self, source, register_events):
-        timestamp = common.now()
-        self._last_event_id = self._last_event_id._replace(
-            session=self._last_event_id.session + 1)
+        timestamp = self._create_session()
 
         for _, module in self._source_modules:
-            await module.on_session_start(self._last_event_id.session)
+            await aio.call(module.on_session_start,
+                           self._last_event_id.session)
 
         events = collections.deque(
             self._create_event(timestamp, register_event)
             for register_event in register_events)
 
         input_source_events = [(source, event) for event in events]
         while input_source_events:
             output_source_events = collections.deque()
 
             for output_source, module in self._source_modules:
                 for input_source, input_event in input_source_events:
-                    if not module.subscription.matches(input_event.event_type):
+                    if not module.subscription.matches(input_event.type):
                         continue
 
-                    async for register_event in module.process(input_source,
-                                                               input_event):
-                        output_event = self._create_event(timestamp,
-                                                          register_event)
-                        output_source_events.append((output_source,
-                                                     output_event))
+                    output_register_events = await aio.call(
+                        module.process, input_source, input_event)
+
+                    if not output_register_events:
+                        continue
+
+                    for output_register_event in output_register_events:
+                        output_event = self._create_event(
+                            timestamp, output_register_event)
+                        output_source_events.append(
+                            (output_source, output_event))
                         events.append(output_event)
 
             input_source_events = output_source_events
 
         for _, module in self._source_modules:
-            await module.on_session_stop(self._last_event_id.session)
+            await aio.call(module.on_session_stop, self._last_event_id.session)
 
-        return list(events)
+        return events
 
     def _create_status_reg_event(self, status):
         return common.RegisterEvent(
-            event_type=('event', 'engine'),
+            type=('event', str(self._server_id), 'engine'),
             source_timestamp=None,
-            payload=common.EventPayload(
-                type=common.EventPayloadType.JSON,
-                data=status))
+            payload=common.EventPayloadJson(status))
+
+    def _create_session(self):
+        self._last_event_id = self._last_event_id._replace(
+            session=self._last_event_id.session + 1,
+            instance=0)
+
+        return common.now()
 
     def _create_event(self, timestamp, register_event):
         self._last_event_id = self._last_event_id._replace(
             instance=self._last_event_id.instance + 1)
 
-        return common.Event(event_id=self._last_event_id,
-                            event_type=register_event.event_type,
+        return common.Event(id=self._last_event_id,
+                            type=register_event.type,
                             timestamp=timestamp,
                             source_timestamp=register_event.source_timestamp,
                             payload=register_event.payload)
```

## hat/event/server/eventer_server.py

```diff
@@ -1,74 +1,105 @@
 """Eventer server"""
 
+from collections.abc import Collection
 import logging
 
 from hat import aio
-from hat import json
+from hat.drivers import tcp
 
-from hat.event.server import common
-import hat.event.eventer
-import hat.event.server.engine
+from hat.event import common
+from hat.event import eventer
 
 
 mlog: logging.Logger = logging.getLogger(__name__)
 """Module logger"""
 
 
-async def create_eventer_server(conf: json.Data,
-                                engine: hat.event.server.engine.Engine
+async def create_eventer_server(addr: tcp.Address,
+                                backend: common.Backend,
+                                server_id: int,
+                                *,
+                                server_token: str | None = None,
+                                **kwargs
                                 ) -> 'EventerServer':
-    """Create eventer server
+    """Create eventer server"""
+    server = EventerServer()
+    server._backend = backend
+    server._server_id = server_id
+    server._server_token = server_token
+    server._engine = None
+
+    server._srv = await eventer.listen(addr,
+                                       connected_cb=server._on_connected,
+                                       disconnected_cb=server._on_disconnected,
+                                       register_cb=server._on_register,
+                                       query_cb=server._on_query,
+                                       **kwargs)
+
+    return server
+
+
+class EventerServer(aio.Resource):
+    """Eventer server
 
-    Args:
-        conf: configuration defined by
-            ``hat-event://main.yaml#/definitions/eventer_server``
-        engine: engine
+    For creating new server see `create_eventer_server` coroutine.
 
     """
 
-    async def on_connected(client_id):
-        await _register_eventer_event(engine, client_id, 'CONNECTED')
+    @property
+    def async_group(self) -> aio.Group:
+        """Async group"""
+        return self._srv.async_group
+
+    async def set_engine(self, engine: common.Engine | None):
+        """Set engine"""
+        self._engine = engine
 
-    async def on_disconnected(client_id):
-        await _register_eventer_event(engine, client_id, 'DISCONNECTED')
+        status = common.Status.OPERATIONAL if engine else common.Status.STANDBY
+        await self._srv.set_status(status)
 
-    async def on_register(client_id, register_events):
-        return await engine.register(_get_source(client_id), register_events)
+    async def notify_events(self,
+                            events: Collection[common.Event],
+                            persisted: bool):
+        """Notify events"""
+        await self._srv.notify_events(events, persisted)
 
-    async def on_query(client_id, query_data):
-        return await engine.query(query_data)
+    async def _on_connected(self, info):
+        if (info.client_token is not None and
+                info.client_token != self._server_token):
+            raise Exception('invalid client token')
 
-    server = EventerServer()
-    server._srv = await hat.event.eventer.listen(
-        address=conf['address'],
-        connected_cb=on_connected,
-        disconnected_cb=on_disconnected,
-        register_cb=on_register,
-        query_cb=on_query)
+        if not self._engine or not self._engine.is_open:
+            return
 
-    handler = engine.register_events_cb(server._srv.notify)
-    server.async_group.spawn(aio.call_on_cancel, handler.cancel)
+        source = _get_source(info.id)
+        register_event = self._create_eventer_event(info, 'CONNECTED')
+        await self._engine.register(source, [register_event])
 
-    return server
+    async def _on_disconnected(self, info):
+        if not self._engine or not self._engine.is_open:
+            return
 
+        source = _get_source(info.id)
+        register_event = self._create_eventer_event(info, 'DISCONNECTED')
+        await self._engine.register(source, [register_event])
 
-class EventerServer(aio.Resource):
+    async def _on_register(self, info, register_events):
+        if not self._engine:
+            return
 
-    @property
-    def async_group(self) -> aio.Group:
-        """Async group"""
-        return self._srv.async_group
+        source = _get_source(info.id)
+        return await self._engine.register(source, register_events)
 
+    async def _on_query(self, info, params):
+        return await self._backend.query(params)
 
-def _get_source(client_id):
-    return common.Source(type=common.SourceType.EVENTER,
-                         id=client_id)
+    def _create_eventer_event(self, info, status):
+        return common.RegisterEvent(
+            type=('event', str(self._server_id), 'eventer', info.client_name),
+            source_timestamp=None,
+            payload=common.EventPayloadJson(status))
 
 
-async def _register_eventer_event(engine, client_id, status):
-    register_event = common.RegisterEvent(
-        event_type=('event', 'eventer'),
-        source_timestamp=None,
-        payload=common.EventPayload(type=common.EventPayloadType.JSON,
-                                    data=status))
-    await engine.register(_get_source(client_id), [register_event])
+def _get_source(source_id):
+    return common.Source(type=common.SourceType.EVENTER,
+                         id=source_id)
```

## hat/event/server/main.py

```diff
@@ -1,23 +1,22 @@
 """Event server main"""
 
 from pathlib import Path
 import argparse
 import asyncio
 import contextlib
-import importlib
 import logging.config
 import sys
 
 import appdirs
 
 from hat import aio
 from hat import json
 
-from hat.event.server import common
+from hat.event import common
 from hat.event.server.runner import MainRunner
 
 
 mlog: logging.Logger = logging.getLogger('hat.event.server.main')
 """Module logger"""
 
 user_conf_dir: Path = Path(appdirs.user_config_dir('hat'))
@@ -26,51 +25,44 @@
 
 def create_argument_parser() -> argparse.ArgumentParser:
     """Create argument parser"""
     parser = argparse.ArgumentParser()
     parser.add_argument(
         '--conf', metavar='PATH', type=Path, default=None,
         help="configuration defined by hat-event://server.yaml# "
-             "(default $XDG_CONFIG_HOME/hat/event.{yaml|yml|json})")
+             "(default $XDG_CONFIG_HOME/hat/event.{yaml|yml|toml|json})")
     return parser
 
 
 def main():
     """Event Server"""
     parser = create_argument_parser()
     args = parser.parse_args()
-
-    conf_path = args.conf
-    if not conf_path:
-        for suffix in ('.yaml', '.yml', '.json'):
-            conf_path = (user_conf_dir / 'event').with_suffix(suffix)
-            if conf_path.exists():
-                break
-
-    if conf_path == Path('-'):
-        conf = json.decode_stream(sys.stdin)
-    else:
-        conf = json.decode_file(conf_path)
-
+    conf = json.read_conf(args.conf, user_conf_dir / 'event')
     sync_main(conf)
 
 
 def sync_main(conf: json.Data):
     """Sync main entry point"""
     aio.init_asyncio()
 
     common.json_schema_repo.validate('hat-event://server.yaml#', conf)
 
-    sub_confs = [conf['backend'], *conf['engine']['modules']]
-    for sub_conf in sub_confs:
-        module = importlib.import_module(sub_conf['module'])
-        if module.json_schema_repo and module.json_schema_id:
-            module.json_schema_repo.validate(module.json_schema_id, sub_conf)
-
-    logging.config.dictConfig(conf['log'])
+    info = common.import_backend_info(conf['backend']['module'])
+    if info.json_schema_repo and info.json_schema_id:
+        info.json_schema_repo.validate(info.json_schema_id, conf['backend'])
+
+    for module_conf in conf['modules']:
+        info = common.import_module_info(module_conf['module'])
+        if info.json_schema_repo and info.json_schema_id:
+            info.json_schema_repo.validate(info.json_schema_id, module_conf)
+
+    log_conf = conf.get('log')
+    if log_conf:
+        logging.config.dictConfig(log_conf)
 
     with contextlib.suppress(asyncio.CancelledError):
         aio.run_asyncio(async_main(conf))
 
 
 async def async_main(conf: json.Data):
     """Async main entry point"""
```

## hat/event/server/runner.py

```diff
@@ -1,319 +1,328 @@
 import asyncio
 import contextlib
-import importlib
 import logging
+import types
+import typing
 
 from hat import aio
 from hat import json
-import hat.monitor.client
-import hat.monitor.common
+from hat.drivers import tcp
+import hat.monitor.component
 
-from hat.event.server import common
+from hat.event import common
 from hat.event.server.engine import create_engine
-from hat.event.server.eventer_server import create_eventer_server
-from hat.event.server.mariner_server import create_mariner_server
-from hat.event.server.syncer_server import (create_syncer_server,
-                                            SyncerServer)
-from hat.event.server.syncer_client import (create_syncer_client,
-                                            SyncerClientState,
-                                            SyncerClient)
+from hat.event.server.eventer_client import create_eventer_client
+from hat.event.server.eventer_server import (create_eventer_server,
+                                             EventerServer)
 
 
 mlog: logging.Logger = logging.getLogger(__name__)
 """Module logger"""
 
 
+class EventerServerData(typing.NamedTuple):
+    server_id: common.ServerId
+    addr: tcp.Address
+
+
 class MainRunner(aio.Resource):
 
     def __init__(self, conf: json.Data):
         self._conf = conf
+        self._loop = asyncio.get_running_loop()
         self._async_group = aio.Group()
         self._backend = None
-        self._syncer_server = None
-        self._mariner_server = None
-        self._monitor_client = None
-        self._syncer_client = None
+        self._eventer_server = None
         self._monitor_component = None
+        self._eventer_client_runner = None
         self._engine_runner = None
 
         self.async_group.spawn(self._run)
 
     @property
-    def async_group(self):
+    def async_group(self) -> aio.Group:
         return self._async_group
 
     async def _run(self):
         try:
             await self._start()
-            await asyncio.Future()
+            await self._loop.create_future()
 
         except Exception as e:
             mlog.error("main runner loop error: %s", e, exc_info=e)
 
         finally:
             self.close()
             await aio.uncancellable(self._stop())
 
     async def _start(self):
-        await self._start_backend()
+        backend_conf = self._conf['backend']
+        backend_info = common.import_backend_info(backend_conf['module'])
+        self._backend = await aio.call(backend_info.create, backend_conf,
+                                       self._on_backend_registered_events,
+                                       self._on_backend_flushed_events)
+        _bind_resource(self.async_group, self._backend)
 
-        if 'syncer_server' in self._conf:
-            await self._start_syncer_server()
+        self._eventer_server = await create_eventer_server(
+            addr=tcp.Address(self._conf['eventer_server']['host'],
+                             self._conf['eventer_server']['port']),
+            backend=self._backend,
+            server_id=self._conf['server_id'],
+            server_token=self._conf.get('server_token'))
+        _bind_resource(self.async_group, self._eventer_server)
+
+        if 'monitor_component' in self._conf:
+            self._monitor_component = await hat.monitor.component.connect(
+                addr=tcp.Address(self._conf['monitor_component']['host'],
+                                 self._conf['monitor_component']['port']),
+                name=self._conf['monitor_component']['name'],
+                group=self._conf['monitor_component']['group'],
+                runner_cb=self._create_monitor_runner,
+                data={'server_id': self._conf['server_id'],
+                      'eventer_server': self._conf['eventer_server'],
+                      'server_token': self._conf.get('server_token')},
+                state_cb=self._on_monitor_state)
+            _bind_resource(self.async_group, self._monitor_component)
+
+            self._eventer_client_runner = EventerClientRunner(
+                conf=self._conf,
+                backend=self._backend,
+                synced_cb=self._on_eventer_client_synced)
+            _bind_resource(self.async_group, self._eventer_client_runner)
 
-        if 'mariner_server' in self._conf:
-            await self._start_mariner_server()
+            await self._eventer_client_runner.set_monitor_state(
+                self._monitor_component.state)
 
-        if 'monitor' in self._conf:
-            await self._start_monitor_client()
-            await self._start_syncer_client()
-            await self._start_monitor_component()
+            await self._monitor_component.set_ready(True)
 
         else:
-            await self._start_engine_runner()
+            self._engine_runner = EngineRunner(
+                conf=self._conf,
+                backend=self._backend,
+                eventer_server=self._eventer_server)
+            _bind_resource(self.async_group, self._engine_runner)
 
-    async def _start_backend(self):
-        py_module = importlib.import_module(self._conf['backend']['module'])
-        self._backend = await aio.call(py_module.create, self._conf['backend'])
-
-        _bind_resource(self.async_group, self._backend)
-
-    async def _start_syncer_server(self):
-        self._syncer_server = await create_syncer_server(
-            self._conf['syncer_server'],
-            self._backend)
+    async def _stop(self):
+        if self._engine_runner and not self._monitor_component:
+            await self._engine_runner.async_close()
 
-        _bind_resource(self.async_group, self._syncer_server)
+        if self._eventer_client_runner:
+            await self._eventer_client_runner.async_close()
 
-    async def _start_mariner_server(self):
-        self._mariner_server = await create_mariner_server(
-            self._conf['mariner_server'],
-            self._backend)
+        if self._monitor_component:
+            await self._monitor_component.async_close()
 
-        _bind_resource(self.async_group, self._mariner_server)
+        if self._eventer_server:
+            with contextlib.suppress(Exception):
+                await self._backend.flush()
 
-    async def _start_monitor_client(self):
-        data = {
-            'server_id': self._conf['engine']['server_id'],
-            'eventer_server_address': self._conf['eventer_server']['address']}
+            await self._eventer_server.async_close()
 
-        if 'syncer_server' in self._conf:
-            data['syncer_server_address'] = \
-                self._conf['syncer_server']['address']
+        if self._backend:
+            await self._backend.async_close()
 
-        if 'syncer_token' in self._conf:
-            data['syncer_token'] = self._conf['syncer_token']
+    async def _create_monitor_runner(self, monitor_component):
+        self._engine_runner = EngineRunner(conf=self._conf,
+                                           backend=self._backend,
+                                           eventer_server=self._eventer_server)
+        return self._engine_runner
 
-        self._monitor_client = await hat.monitor.client.connect(
-            self._conf['monitor'], data)
+    async def _on_backend_registered_events(self, events):
+        if not self._eventer_server:
+            return
 
-        _bind_resource(self.async_group, self._monitor_client)
+        await self._eventer_server.notify_events(events, False)
 
-    async def _start_syncer_client(self):
-        self._syncer_client = await create_syncer_client(
-            backend=self._backend,
-            monitor_client=self._monitor_client,
-            monitor_group=self._conf['monitor']['group'],
-            name=str(self._conf['engine']['server_id']),
-            syncer_token=self._conf.get('syncer_token'))
+    async def _on_backend_flushed_events(self, events):
+        if not self._eventer_server:
+            return
 
-        _bind_resource(self.async_group, self._syncer_client)
+        await self._eventer_server.notify_events(events, True)
 
-    async def _start_monitor_component(self):
+    async def _on_monitor_state(self, monitor_component, state):
+        if not self._eventer_client_runner:
+            return
 
-        async def on_run(monitor_component):
-            engine_runner = EngineRunner(self._conf, self._backend,
-                                         self._syncer_server,
-                                         self._syncer_client)
+        await self._eventer_client_runner.set_monitor_state(state)
 
-            try:
-                await engine_runner.wait_closing()
+    async def _on_eventer_client_synced(self, server_id, synced, counter):
+        if not self._engine_runner:
+            return
 
-            finally:
-                await aio.uncancellable(engine_runner.async_close())
+        await self._engine_runner.set_synced(server_id, synced, counter)
 
-        self._monitor_component = hat.monitor.client.Component(
-            self._monitor_client, on_run)
-        self._monitor_component.set_ready(True)
 
-        _bind_resource(self.async_group, self._monitor_component)
+class EventerClientRunner(aio.Resource):
 
-    async def _start_engine_runner(self):
-        self._engine_runner = EngineRunner(self._conf, self._backend,
-                                           self._syncer_server,
-                                           self._syncer_client)
+    def __init__(self,
+                 conf: json.Data,
+                 backend: common.Backend,
+                 synced_cb: aio.AsyncCallable[[common.ServerId, bool, int],
+                                              None],
+                 reconnect_delay: float = 5):
+        self._conf = conf
+        self._backend = backend
+        self._synced_cb = synced_cb
+        self._reconnect_delay = reconnect_delay
+        self._async_group = aio.Group()
+        self._client_subgroups = {}
 
-        _bind_resource(self.async_group, self._engine_runner)
+    @property
+    def async_group(self) -> aio.Group:
+        return self._async_group
 
-    async def _stop(self):
-        if self._engine_runner:
-            await self._engine_runner.async_close()
+    async def set_monitor_state(self, state: hat.monitor.component.State):
+        valid_server_data = set(_get_eventer_server_data(
+            group=self._conf['monitor_component']['group'],
+            server_token=self._conf.get('server_token'),
+            state=state))
+
+        for server_data in list(self._client_subgroups.keys()):
+            if server_data in valid_server_data:
+                continue
+
+            subgroup = self._client_subgroups.pop(server_data)
+            subgroup.close()
+
+        for server_data in valid_server_data:
+            subgroup = self._client_subgroups.get(server_data)
+            if subgroup and subgroup.is_open:
+                continue
+
+            subgroup = self.async_group.create_subgroup()
+            subgroup.spawn(self._client_loop, subgroup, server_data)
+            self._client_subgroups[server_data] = subgroup
 
-        if self._monitor_component:
-            await self._monitor_component.async_close()
+    async def _client_loop(self, async_group, server_data):
+        try:
+            while True:
+                try:
+                    eventer_client = await create_eventer_client(
+                        addr=server_data.addr,
+                        client_name=self._conf['monitor_component']['name'],
+                        server_id=server_data.server_id,
+                        backend=self._backend,
+                        client_token=self._conf.get('server_token'),
+                        synced_cb=self._on_synced)
+
+                except Exception:
+                    await asyncio.sleep(self._reconnect_delay)
+                    continue
 
-        if self._syncer_client:
-            await self._syncer_client.async_close()
+                try:
+                    await aio.call(self._synced_cb, server_data.server_id,
+                                   False, 0)
 
-        if self._monitor_client:
-            await self._monitor_client.async_close()
+                    await eventer_client.wait_closing()
 
-        if self._mariner_server:
-            await self._mariner_server.async_close()
+                finally:
+                    await aio.uncancellable(eventer_client.async_close())
 
-        if self._syncer_server:
-            with contextlib.suppress(Exception):
-                await self._backend.flush()
+        except Exception as e:
+            mlog.error("eventer client runner loop error: %s", e, exc_info=e)
+            self.close()
 
-            await self._syncer_server.async_close()
+        finally:
+            async_group.close()
 
-        if self._backend:
-            await self._backend.async_close()
+    async def _on_synced(self, server_id, counter):
+        await aio.call(self._synced_cb, server_id, True, counter)
 
 
 class EngineRunner(aio.Resource):
 
     def __init__(self,
                  conf: json.Data,
                  backend: common.Backend,
-                 syncer_server: SyncerServer | None,
-                 syncer_client: SyncerClient | None):
+                 eventer_server: EventerServer):
         self._conf = conf
         self._backend = backend
-        self._syncer_server = syncer_server
-        self._syncer_client = syncer_client
+        self._eventer_server = eventer_server
         self._async_group = aio.Group()
-        self._syncer_client_states = {}
         self._engine = None
-        self._eventer_server = None
-        self._restart_future = None
-        self._synced = True
+        self._restart = asyncio.Event()
 
         self.async_group.spawn(self._run)
 
     @property
-    def async_group(self):
+    def async_group(self) -> aio.Group:
         return self._async_group
 
+    async def set_synced(self,
+                         server_id: common.ServerId,
+                         synced: bool,
+                         counter: int):
+        if self._engine and self._engine.is_open:
+            source = common.Source(type=common.SourceType.SERVER, id=0)
+            event = common.RegisterEvent(
+                type=('event', str(self._conf['server_id']), 'synced',
+                      str(server_id)),
+                source_timestamp=None,
+                payload=common.EventPayloadJson(synced))
+
+            await self._engine.register(source, [event])
+
+        if synced and counter and self._conf.get('synced_restart_engine'):
+            self._restart.set()
+
     async def _run(self):
         try:
             while True:
-                subgroup = self.async_group.create_subgroup()
+                self._restart.clear()
 
-                try:
-                    await self._run_engine(subgroup)
+                self._engine = await create_engine(
+                    backend=self._backend,
+                    module_confs=self._conf['modules'],
+                    server_id=self._conf['server_id'])
+                await self._eventer_server.set_engine(self._engine)
+
+                async with self._async_group.create_subgroup() as subgroup:
+                    await asyncio.wait(
+                        [subgroup.spawn(self._engine.wait_closing),
+                         subgroup.spawn(self._restart.wait)],
+                        return_when=asyncio.FIRST_COMPLETED)
 
-                finally:
-                    await self._eventer_server.async_close()
-                    await self._engine.async_close()
+                if not self._engine.is_open:
+                    break
 
-                    await subgroup.async_close()
+                await self._close()
 
         except Exception as e:
             mlog.error("engine runner loop error: %s", e, exc_info=e)
 
         finally:
             self.close()
-            await aio.uncancellable(self._stop())
-
-    async def _run_engine(self, subgroup):
-        self._engine = await create_engine(self._conf['engine'],
-                                           self._backend)
-        _bind_resource(subgroup, self._engine)
-
-        with contextlib.ExitStack() as exit_stack:
-            if self._syncer_server:
-                exit_stack.enter_context(
-                    self._syncer_server.register_state_cb(
-                        self._on_syncer_server_state))
-                self._on_syncer_server_state(self._syncer_server.state)
-
-            self._restart_future = asyncio.Future()
-            self._synced = True
-
-            if self._syncer_client:
-                exit_stack.enter_context(
-                    self._syncer_client.register_state_cb(
-                        self._on_syncer_client_state))
-                exit_stack.enter_context(
-                    self._syncer_client.register_events_cb(
-                        self._on_syncer_client_events))
-
-            self._eventer_server = await create_eventer_server(
-                self._conf['eventer_server'], self._engine)
-            _bind_resource(subgroup, self._eventer_server)
-
-            await subgroup.wrap(self._restart_future)
-
-    async def _stop(self):
-        if self._eventer_server:
-            await self._eventer_server.async_close()
+            await aio.uncancellable(self._close())
 
+    async def _close(self):
         if self._engine:
             await self._engine.async_close()
 
-        with contextlib.suppress(Exception):
-            await self._backend.flush()
-
-        if self._syncer_server:
-            with contextlib.suppress(Exception):
-                await self._syncer_server.flush()
-
-    def _on_syncer_server_state(self, client_infos):
-        event = common.RegisterEvent(
-            event_type=('event', 'syncer', 'server'),
-            source_timestamp=None,
-            payload=common.EventPayload(
-                type=common.EventPayloadType.JSON,
-                data=[{'client_name': client_info.name,
-                       'synced': client_info.synced}
-                      for client_info in client_infos]))
-
-        self.async_group.spawn(self._engine.register, _syncer_source, [event])
-
-    def _on_syncer_client_state(self, server_id, state):
-        self._syncer_client_states[server_id] = state
-        if state != SyncerClientState.SYNCED:
-            return
-
-        async def register_with_restart(events):
-            await self._engine.register(_syncer_source, events)
-            if self._restart_future and not self._restart_future.done():
-                self._restart_future.set_result(None)
-
-        event = common.RegisterEvent(
-            event_type=('event', 'syncer', 'client', str(server_id), 'synced'),
-            source_timestamp=None,
-            payload=common.EventPayload(
-                type=common.EventPayloadType.JSON,
-                data=True))
+        await self._eventer_server.set_engine(None)
 
-        if self._conf.get('synced_restart_engine') and not self._synced:
-            self.async_group.spawn(register_with_restart, [event])
-
-        else:
-            self.async_group.spawn(self._engine.register, _syncer_source,
-                                   [event])
 
-    def _on_syncer_client_events(self, server_id, events):
-        state = self._syncer_client_states.pop(server_id, None)
-        if state != SyncerClientState.CONNECTED:
-            return
-
-        event = common.RegisterEvent(
-            event_type=('event', 'syncer', 'client', str(server_id), 'synced'),
-            source_timestamp=None,
-            payload=common.EventPayload(
-                type=common.EventPayloadType.JSON,
-                data=False))
-
-        self.async_group.spawn(self._engine.register, _syncer_source, [event])
-        self._synced = False
+def _bind_resource(async_group, resource):
+    async_group.spawn(aio.call_on_done, resource.wait_closing(),
+                      async_group.close)
 
 
-_syncer_source = common.Source(type=common.SourceType.SYNCER,
-                               id=0)
+def _get_eventer_server_data(group, server_token, state):
+    for info in state.components:
+        if info == state.info or info.group != group:
+            continue
+
+        server_id = json.get(info.data, 'server_id')
+        host = json.get(info.data, ['eventer_server', 'host'])
+        port = json.get(info.data, ['eventer_server', 'port'])
+        token = json.get(info.data, 'server_token')
+        if (not isinstance(server_id, int) or
+                not isinstance(host, str) or
+                not isinstance(port, int) or
+                not isinstance(token, (str, types.NoneType))):
+            continue
 
+        if server_token is not None and token != server_token:
+            continue
 
-def _bind_resource(async_group, resource):
-    async_group.spawn(aio.call_on_done, resource.wait_closing(),
-                      async_group.close)
+        yield EventerServerData(server_id=server_id,
+                                addr=tcp.Address(host, port))
```

## Comparing `hat/event/server/mariner_server.py` & `hat/event/backends/lmdb/latestdb.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,131 +1,116 @@
-"""Mariner server"""
+import itertools
+import typing
 
-import logging
-import urllib.parse
+import lmdb
 
-from hat import aio
-from hat import json
-from hat.drivers import tcp
+from hat.event.backends.lmdb import common
+from hat.event.backends.lmdb import environment
+from hat.event.backends.lmdb.conditions import Conditions
+
+
+class Changes(typing.NamedTuple):
+    data: dict[common.EventTypeRef, common.Event]
+    types: dict[common.EventTypeRef, common.EventId]
+
+
+class AddResult(typing.NamedTuple):
+    added_ref: common.EventRef | None
+    removed_ref: tuple[common.EventId, common.EventRef] | None
+
+
+def ext_create(env: environment.Environment,
+               txn: lmdb.Transaction,
+               conditions: Conditions,
+               subscription: common.Subscription
+               ) -> 'LatestDb':
+    db = LatestDb()
+    db._env = env
+    db._subscription = subscription
+    db._changes = Changes({}, {})
+
+    db._event_type_refs = {
+        event_type: ref
+        for ref, event_type in env.ext_read(txn, common.DbType.LATEST_TYPE)}
+
+    db._events = {
+        event.type: event
+        for ref, event in env.ext_read(txn, common.DbType.LATEST_DATA)
+        if conditions.matches(event) and
+        subscription.matches(event.type) and
+        db._event_type_refs.get(event.type) == ref}
+
+    db._next_event_type_refs = itertools.count(
+        max(db._event_type_refs.values(), default=0) + 1)
+
+    return db
+
+
+class LatestDb:
+
+    def add(self,
+            event: common.Event
+            ) -> AddResult:
+        if not self._subscription.matches(event.type):
+            return AddResult(added_ref=None,
+                             removed_ref=None)
+
+        previous_event = self._events.get(event.type)
+        if previous_event and previous_event > event:
+            return AddResult(added_ref=None,
+                             removed_ref=None)
+
+        event_type_ref = self._event_type_refs.get(event.type)
+        if event_type_ref is None:
+            event_type_ref = next(self._next_event_type_refs)
+            self._changes.types[event_type_ref] = event.type
+            self._event_type_refs[event.type] = event_type_ref
+
+        self._changes.data[event_type_ref] = event
+        self._events[event.type] = event
+
+        added_ref = common.LatestEventRef(event_type_ref)
+        removed_ref = ((previous_event.id, added_ref)
+                       if previous_event else None)
+        return AddResult(added_ref=added_ref,
+                         removed_ref=removed_ref)
+
+    def query(self,
+              params: common.QueryLatestParams
+              ) -> common.QueryResult:
+        event_types = (set(params.event_types)
+                       if params.event_types is not None
+                       else None)
+
+        if event_types is None or ('*', ) in event_types:
+            events = self._events.values()
+
+        elif any('*' in event_type or '?' in event_type
+                 for event_type in event_types):
+            subscription = common.create_subscription(event_types)
+            events = (event for event in self._events.values()
+                      if subscription.matches(event.type))
+
+        elif len(event_types) < len(self._events):
+            events = (self._events.get(event_type)
+                      for event_type in event_types)
+            events = (event for event in events if event)
+
+        else:
+            events = (event for event in self._events.values()
+                      if event.type in event_types)
+
+        return common.QueryResult(events=list(events),
+                                  more_follows=False)
+
+    def create_changes(self) -> Changes:
+        changes, self._changes = self._changes, Changes({}, {})
+        return changes
+
+    def ext_write(self,
+                  txn: lmdb.Transaction,
+                  changes: Changes):
+        self._env.ext_write(txn, common.DbType.LATEST_DATA,
+                            changes.data.items())
 
-from hat.event import mariner
-from hat.event.server import common
-
-
-mlog: logging.Logger = logging.getLogger(__name__)
-"""Module logger"""
-
-
-async def create_mariner_server(conf: json.Data,
-                                backend: common.Backend
-                                ) -> 'MarinerServer':
-    """Create mariner server
-
-    Args:
-        conf: configuration defined by
-            ``hat-event://main.yaml#/definitions/mariner_server``
-        backend: backend
-
-    """
-    url = urllib.parse.urlparse(conf['address'])
-    address = tcp.Address(url.hostname, url.port)
-
-    server = MarinerServer()
-    server._backend = backend
-
-    server._server = await mariner.listen(
-        address, server._on_connection,
-        subscriptions=[tuple(event_type)
-                       for event_type in conf['subscriptions']])
-
-    return server
-
-
-class MarinerServer(aio.Resource):
-
-    @property
-    def async_group(self) -> aio.Group:
-        """Async group"""
-        return self._server.async_group
-
-    async def _on_connection(self, conn):
-        try:
-            mlog.debug("new connection: %s", conn.client_id)
-
-            # TODO check client token, subscriptions
-
-            conn = _Connection(self._backend, conn)
-
-        except Exception as e:
-            mlog.error("on connection error: %s", e, exc_info=e)
-            conn.close()
-
-        # await conn.wait_closing()
-
-
-class _Connection(aio.Resource):
-
-    def __init__(self,
-                 backend: common.Backend,
-                 conn: mariner.ServerConnection):
-        self._backend = backend
-        self._conn = conn
-        self._last_event_ids = ({conn.last_event_id.server: conn.last_event_id}
-                                if conn.last_event_id else {})
-        self._subscription = self._conn.subscription
-
-        self.async_group.spawn(self._connection_loop)
-
-    @property
-    def async_group(self) -> aio.Group:
-        return self._conn.async_group
-
-    async def _connection_loop(self):
-        try:
-            mlog.debug("starting connection loop")
-
-            events_queue = aio.Queue()
-            with self._backend.register_flushed_events_cb(
-                    events_queue.put_nowait):
-
-                for last_event_id in self._last_event_ids.values():
-                    async for events in self._backend.query_flushed(
-                            last_event_id):
-                        await self._send_events(events)
-
-                while True:
-                    events = await events_queue.get()
-                    await self._send_events(events)
-
-        except ConnectionError:
-            pass
-
-        except Exception as e:
-            mlog.error("connection loop error: %s", e, exc_info=e)
-
-        finally:
-            mlog.debug("stopping connection loop")
-            self.close()
-
-    async def _send_events(self, events):
-        if not events:
-            return
-
-        last_event_id = self._last_event_ids.get(events[0].event_id.server)
-        if last_event_id:
-            if events[0].event_id.session < last_event_id.session:
-                return
-
-            if events[0].event_id.session == last_event_id.session:
-                events = (event for event in events
-                          if event.event_id > last_event_id)
-
-        events = [event for event in events
-                  if self._subscription.matches(event.event_type)]
-        if not events:
-            return
-
-        mlog.debug("sending events")
-        await self._conn.send_events(events)
-
-        last_event_id = events[-1].event_id
-        self._last_event_ids[last_event_id.server] = last_event_id
+        self._env.ext_write(txn, common.DbType.LATEST_TYPE,
+                            changes.types.items())
```

## Comparing `hat/event/server/backends/memory.py` & `hat/event/backends/memory.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,149 +1,152 @@
 """Simple memory backend
 
 All registered events are stored in single unsorted continuous event list.
 
-`query_flushed` returns empty iterable and registered flushed events callback
-is never notified.
-
 """
 
 import collections
-import typing
 
 from hat import aio
-from hat import json
-from hat import util
-
-from hat.event.server import common
-
-
-json_schema_id = None
-json_schema_repo = None
-
 
-async def create(conf: json.Data) -> 'MemoryBackend':
-    backend = MemoryBackend()
-    backend._async_group = aio.Group()
-    backend._events = collections.deque()
-    backend._registered_events_cbs = util.CallbackRegistry()
-    return backend
+from hat.event import common
 
 
 class MemoryBackend(common.Backend):
 
+    def __init__(self, conf, registered_events_cb, flushed_events_cb):
+        self._registered_events_cb = registered_events_cb
+        self._flushed_events_cb = flushed_events_cb
+        self._async_group = aio.Group()
+        self._events = collections.deque()
+
     @property
-    def async_group(self) -> aio.Group:
+    def async_group(self):
         return self._async_group
 
-    def register_registered_events_cb(self,
-                                      cb: typing.Callable[[list[common.Event]],
-                                                          None]
-                                      ) -> util.RegisterCallbackHandle:
-        return self._registered_events_cbs.register(cb)
-
-    def register_flushed_events_cb(self,
-                                   cb: typing.Callable[[list[common.Event]],
-                                                       None]
-                                   ) -> util.RegisterCallbackHandle:
-        return util.RegisterCallbackHandle(cancel=lambda: None)
-
-    async def get_last_event_id(self,
-                                server_id: int
-                                ) -> common.EventId:
+    async def get_last_event_id(self, server_id):
         event_ids = (e.event_id for e in self._events
                      if e.server == server_id)
-        key = lambda event_id: event_id.instance  # NOQA
         default = common.EventId(server=server_id, session=0, instance=0)
-        return max(event_ids, key=key, default=default)
+        return max(event_ids, default=default)
 
-    async def register(self,
-                       events: list[common.Event]
-                       ) -> list[common.Event | None]:
+    async def register(self, events):
         self._events.extend(events)
-        self._registered_events_cbs.notify(events)
+
+        if self._registered_events_cb:
+            await aio.call(self._registered_events_cb, events)
+
+        if self._flushed_events_cb:
+            await aio.call(self._flushed_events_cb, events)
+
         return events
 
-    async def query(self,
-                    data: common.QueryData
-                    ) -> list[common.Event]:
+    async def query(self, params):
+        if isinstance(params, common.QueryLatestParams):
+            return self._query_latest(params)
+
+        if isinstance(params, common.QueryTimeseriesParams):
+            return self._query_timeseries(params)
+
+        if isinstance(params, common.QueryServerParams):
+            return self._query_server(params)
+
+        raise ValueError('unsupported params type')
+
+    async def flush(self):
+        pass
+
+    def _query_latest(self, params):
         events = self._events
 
-        if data.server_id is not None:
-            events = _filter_server_id(events, data.server_id)
+        if params.event_types is not None:
+            events = _filter_event_types(events, params.event_types)
+
+        result = {}
+        for event in events:
+            previous = result.get(event.type)
+            if previous is None or previous < event:
+                result[event.type] = event
 
-        if data.event_ids is not None:
-            events = _filter_events_ids(events, data.event_ids)
+        return common.QueryResult(events=list(result.values()),
+                                  more_follows=False)
 
-        if data.event_types is not None:
-            events = _filter_event_types(events, data.event_types)
+    def _query_timeseries(self, params):
+        events = self._events
 
-        if data.t_from is not None:
-            events = _filter_t_from(events, data.t_from)
+        if params.event_types is not None:
+            events = _filter_event_types(events, params.event_types)
 
-        if data.t_to is not None:
-            events = _filter_t_to(events, data.t_to)
+        if params.t_from is not None:
+            events = _filter_t_from(events, params.t_from)
 
-        if data.source_t_from is not None:
-            events = _filter_source_t_from(events, data.source_t_from)
+        if params.t_to is not None:
+            events = _filter_t_to(events, params.t_to)
 
-        if data.source_t_to is not None:
-            events = _filter_source_t_to(events, data.source_t_to)
+        if params.source_t_from is not None:
+            events = _filter_source_t_from(events, params.source_t_from)
 
-        if data.payload is not None:
-            events = _filter_payload(events, data.payload)
+        if params.source_t_to is not None:
+            events = _filter_source_t_to(events, params.source_t_to)
 
-        if data.order_by == common.OrderBy.TIMESTAMP:
-            sort_key = lambda event: event.timestamp  # NOQA
-        elif data.order_by == common.OrderBy.SOURCE_TIMESTAMP:
-            sort_key = lambda event: event.source_timestamp  # NOQA
+        if params.order_by == common.OrderBy.TIMESTAMP:
+            sort_key = lambda event: event.timestamp, event  # NOQA
+        elif params.order_by == common.OrderBy.SOURCE_TIMESTAMP:
+            sort_key = lambda event: event.source_timestamp, event  # NOQA
         else:
             raise ValueError('invalid order by')
 
-        if data.order == common.Order.ASCENDING:
+        if params.order == common.Order.ASCENDING:
             sort_reverse = False
-        elif data.order == common.Order.DESCENDING:
+        elif params.order == common.Order.DESCENDING:
             sort_reverse = True
         else:
             raise ValueError('invalid order by')
 
         events = sorted(events, key=sort_key, reverse=sort_reverse)
 
-        if data.unique_type:
-            events = _filter_unique_type(events)
-
-        if data.max_results is not None:
-            events = _filter_max_results(events, data.max_results)
+        if params.last_event_id and events:
+            for i, event in enumerate(events):
+                if event.id == params.last_event_id:
+                    break
+            events = events[i+1:]
+
+        if params.max_results is not None and len(events) > params.max_results:
+            more_follows = True
+            events = events[:params.max_results]
+        else:
+            more_follows = False
 
-        return list(events)
+        return common.QueryResult(events=events,
+                                  more_follows=more_follows)
 
-    async def query_flushed(self,
-                            data: common.QueryData
-                            ) -> typing.AsyncIterable[list[common.Event]]:
-        for events in []:
-            yield events
-
-    async def flush(self):
-        pass
+    def _query_server(self, params):
+        events = sorted(_filter_server_id(self._events, params.server_id))
 
+        if params.last_event_id and events:
+            for i, event in enumerate(events):
+                if event.id > params.last_event_id:
+                    break
+            events = events[i:]
+
+        if params.max_results is not None and len(events) > params.max_results:
+            more_follows = True
+            events = events[:params.max_results]
+        else:
+            more_follows = False
 
-def _filter_server_id(events, server_id):
-    for event in events:
-        if event.event_id.server == server_id:
-            yield event
+        return common.QueryResult(events=events,
+                                  more_follows=more_follows)
 
 
-def _filter_events_ids(events, event_ids):
-    for event in events:
-        if event.event_id in event_ids:
-            yield event
+info = common.BackendInfo(MemoryBackend)
 
 
 def _filter_event_types(events, event_types):
-    subscription = common.Subscription(event_types)
+    subscription = common.create_subscription(event_types)
     for event in events:
         if subscription.matches(event.event_type):
             yield event
 
 
 def _filter_t_from(events, t_from):
     for event in events:
@@ -165,28 +168,11 @@
 
 def _filter_source_t_to(events, source_t_to):
     for event in events:
         if event.source_timestamp <= source_t_to:
             yield event
 
 
-def _filter_payload(events, payload):
+def _filter_server_id(events, server_id):
     for event in events:
-        if event.payload == payload:
+        if event.event_id.server == server_id:
             yield event
-
-
-def _filter_unique_type(events):
-    event_types = set()
-    for event in events:
-        event_type = tuple(event.event_type)
-        if event_type in event_types:
-            continue
-        event_types.add(event_type)
-        yield event
-
-
-def _filter_max_results(events, max_results):
-    for i, event in enumerate(events):
-        if i >= max_results:
-            break
-        yield events
```

## Comparing `hat/event/server/backends/lmdb/backend.py` & `hat/event/backends/lmdb/backend.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,278 +1,319 @@
+from collections.abc import Collection
 from pathlib import Path
 import asyncio
 import collections
+import contextlib
 import logging
 import typing
 
 from hat import aio
 from hat import json
-from hat import util
 
-from hat.event.server.backends.lmdb import common
-from hat.event.server.backends.lmdb import environment
-from hat.event.server.backends.lmdb import latestdb
-from hat.event.server.backends.lmdb import ordereddb
-from hat.event.server.backends.lmdb import refdb
-from hat.event.server.backends.lmdb import systemdb
-from hat.event.server.backends.lmdb.conditions import Conditions
+from hat.event.backends.lmdb import common
+from hat.event.backends.lmdb import environment
+from hat.event.backends.lmdb import latestdb
+from hat.event.backends.lmdb import refdb
+from hat.event.backends.lmdb import systemdb
+from hat.event.backends.lmdb import timeseriesdb
+from hat.event.backends.lmdb.conditions import Conditions
 
 
 mlog = logging.getLogger(__name__)
 
-cleanup_max_entries_remove = 100
+cleanup_max_results = 1024
 
+flush_queue_size = 4096
 
-async def create(conf: json.Data
+max_registered_count = 1024 * 256
+
+version = '0.9'
+
+
+class Databases(typing.NamedTuple):
+    system: systemdb.SystemDb
+    latest: latestdb.LatestDb
+    timeseries: timeseriesdb.TimeseriesDb
+    ref: refdb.RefDb
+
+
+class Changes(typing.NamedTuple):
+    system: systemdb.Changes
+    latest: latestdb.Changes
+    timeseries: timeseriesdb.Changes
+    ref: refdb.Changes
+
+
+async def create(conf: json.Data,
+                 registered_events_cb: common.BackendRegisteredEventsCb | None,
+                 flushed_events_cb: common.BackendFlushedEventsCb | None
                  ) -> 'LmdbBackend':
     backend = LmdbBackend()
-    backend._flush_lock = asyncio.Lock()
-    backend._flush_period = conf['flush_period']
-    backend._flushed_events_cbs = util.CallbackRegistry()
-    backend._registered_events_cbs = util.CallbackRegistry()
+    backend._registered_events_cb = registered_events_cb
+    backend._flushed_events_cb = flushed_events_cb
     backend._conditions = Conditions(conf['conditions'])
+    backend._loop = asyncio.get_running_loop()
+    backend._flush_queue = aio.Queue(flush_queue_size)
+    backend._registered_count = 0
+    backend._registered_queue = collections.deque()
     backend._async_group = aio.Group()
 
-    backend._env = await environment.create(db_path=Path(conf['db_path']),
-                                            max_db_size=conf['max_db_size'])
+    backend._env = await environment.create(Path(conf['db_path']))
+    backend.async_group.spawn(aio.call_on_done, backend._env.wait_closing(),
+                              backend.close)
 
     try:
-        await backend._env.execute(_ext_init, backend, conf)
+        latest_subscription = common.create_subscription(
+            tuple(i) for i in conf['latest']['subscriptions'])
+
+        timeseries_partitions = (
+            timeseriesdb.Partition(
+                order_by=common.OrderBy[i['order_by']],
+                subscription=common.create_subscription(
+                    tuple(event_type) for event_type in i['subscriptions']),
+                limit=(
+                    timeseriesdb.Limit(
+                        min_entries=i['limit'].get('min_entries'),
+                        max_entries=i['limit'].get('max_entries'),
+                        duration=i['limit'].get('duration'),
+                        size=i['limit'].get('size'))
+                    if 'limit' in i else None))
+            for i in conf['timeseries'])
+
+        backend._dbs = await backend._env.execute(
+            _ext_create_dbs, backend._env, conf['identifier'],
+            backend._conditions, latest_subscription, timeseries_partitions)
+
+        backend.async_group.spawn(backend._flush_loop, conf['flush_period'])
+        backend.async_group.spawn(backend._cleanup_loop,
+                                  conf['cleanup_period'])
 
     except BaseException:
         await aio.uncancellable(backend._env.async_close())
         raise
 
-    backend.async_group.spawn(aio.call_on_done, backend._env.wait_closing(),
-                              backend.close)
-    backend.async_group.spawn(backend._flush_loop)
-    backend.async_group.spawn(backend._cleanup_loop)
-
     return backend
 
 
-def _ext_init(backend, conf):
-    backend._ref_db = refdb.RefDb(backend._env)
-
-    backend._sys_db = systemdb.ext_create(backend._env)
-
-    backend._latest_db = latestdb.ext_create(
-        env=backend._env,
-        ref_db=backend._ref_db,
-        subscription=common.Subscription(
-            tuple(i) for i in conf['latest']['subscriptions']),
-        conditions=backend._conditions)
-
-    backend._ordered_dbs = [
-        ordereddb.ext_create(
-            env=backend._env,
-            ref_db=backend._ref_db,
-            subscription=common.Subscription(
-                tuple(et) for et in i['subscriptions']),
-            conditions=backend._conditions,
-            order_by=common.OrderBy[i['order_by']],
-            limit=i.get('limit'))
-        for i in conf['ordered']]
-
-    # TODO: maybe cleanup unused ordered partitions
-    # ordereddb.cleanup(
-    #     {ordered_db.partition_id for ordered_db in backend._ordered_dbs})
+def _ext_create_dbs(env, identifier, conditions, latest_subscription,
+                    timeseries_partitions):
+    with env.ext_begin(write=True) as txn:
+        system_db = systemdb.ext_create(env, txn, version, identifier)
+        latest_db = latestdb.ext_create(env, txn, conditions,
+                                        latest_subscription)
+        timeseries_db = timeseriesdb.ext_create(env, txn, conditions,
+                                                timeseries_partitions)
+        ref_db = refdb.RefDb(env)
+
+    return Databases(system=system_db,
+                     latest=latest_db,
+                     timeseries=timeseries_db,
+                     ref=ref_db)
 
 
 class LmdbBackend(common.Backend):
 
     @property
     def async_group(self) -> aio.Group:
         return self._async_group
 
-    def register_registered_events_cb(self,
-                                      cb: typing.Callable[[list[common.Event]],
-                                                          None]
-                                      ) -> util.RegisterCallbackHandle:
-        return self._registered_events_cbs.register(cb)
-
-    def register_flushed_events_cb(self,
-                                   cb: typing.Callable[[list[common.Event]],
-                                                       None]
-                                   ) -> util.RegisterCallbackHandle:
-        return self._flushed_events_cbs.register(cb)
-
     async def get_last_event_id(self,
                                 server_id: int
                                 ) -> common.EventId:
-        event_id, _ = self._sys_db.get_last_event_id_timestamp(server_id)
-        return event_id
+        if not self.is_open:
+            raise common.BackendClosedError()
+
+        return self._dbs.system.get_last_event_id(server_id)
 
     async def register(self,
-                       events: list[common.Event]
-                       ) -> list[common.Event]:
+                       events: Collection[common.Event]
+                       ) -> Collection[common.Event] | None:
+        if not self.is_open:
+            raise common.BackendClosedError()
+
         for event in events:
-            last_event_id, last_timestamp = \
-                self._sys_db.get_last_event_id_timestamp(event.event_id.server)
+            server_id = event.id.server
+
+            last_event_id = self._dbs.system.get_last_event_id(server_id)
+            last_timestamp = self._dbs.system.get_last_timestamp(server_id)
 
-            if last_event_id >= event.event_id:
+            if last_event_id >= event.id:
                 mlog.warning("event registration skipped: invalid event id")
                 continue
 
             if last_timestamp > event.timestamp:
                 mlog.warning("event registration skipped: invalid timestamp")
                 continue
 
             if not self._conditions.matches(event):
                 mlog.warning("event registration skipped: invalid conditions")
                 continue
 
-            registered = False
+            refs = collections.deque()
+
+            latest_result = self._dbs.latest.add(event)
 
-            if self._latest_db.add(event):
-                registered = True
+            if latest_result.added_ref:
+                refs.append(latest_result.added_ref)
 
-            for db in self._ordered_dbs:
-                if db.add(event):
-                    registered = True
+            if latest_result.removed_ref:
+                self._dbs.ref.remove(*latest_result.removed_ref)
 
-            if not registered:
+            refs.extend(self._dbs.timeseries.add(event))
+
+            if not refs:
                 continue
 
-            self._sys_db.set_last_event_id_timestamp(event.event_id,
-                                                     event.timestamp)
+            self._dbs.ref.add(event, refs)
+            self._dbs.system.set_last_event_id(event.id)
+            self._dbs.system.set_last_timestamp(server_id, event.timestamp)
+
+        self._registered_queue.append(events)
+        self._registered_count += len(events)
+
+        if self._registered_count > max_registered_count:
+            await self._flush_queue.put(self._loop.create_future())
+
+        if self._registered_events_cb:
+            await aio.call(self._registered_events_cb, events)
 
-        self._registered_events_cbs.notify(events)
         return events
 
     async def query(self,
-                    data: common.QueryData
-                    ) -> list[common.Event]:
-        if (data.server_id is None and
-                data.event_ids is None and
-                data.t_to is None and
-                data.source_t_from is None and
-                data.source_t_to is None and
-                data.payload is None and
-                data.order == common.Order.DESCENDING and
-                data.order_by == common.OrderBy.TIMESTAMP and
-                data.unique_type):
-            events = self._latest_db.query(event_types=data.event_types)
-
-            if data.t_from is not None:
-                events = (event for event in events
-                          if data.t_from <= event.timestamp)
-
-            events = sorted(events,
-                            key=lambda i: (i.timestamp, i.event_id),
-                            reverse=True)
-
-            if data.max_results is not None:
-                events = events[:data.max_results]
+                    params: common.QueryParams
+                    ) -> common.QueryResult:
+        if not self.is_open:
+            raise common.BackendClosedError()
 
-            return events
+        if isinstance(params, common.QueryLatestParams):
+            return self._dbs.latest.query(params)
 
-        subscription = (common.Subscription(data.event_types)
-                        if data.event_types is not None else None)
+        if isinstance(params, common.QueryTimeseriesParams):
+            return await self._dbs.timeseries.query(params)
 
-        for db in self._ordered_dbs:
-            if db.order_by != data.order_by:
-                continue
-            if subscription and subscription.isdisjoint(db.subscription):
-                continue
+        if isinstance(params, common.QueryServerParams):
+            return await self._dbs.ref.query(params)
 
-            events = await db.query(subscription=subscription,
-                                    server_id=data.server_id,
-                                    event_ids=data.event_ids,
-                                    t_from=data.t_from,
-                                    t_to=data.t_to,
-                                    source_t_from=data.source_t_from,
-                                    source_t_to=data.source_t_to,
-                                    payload=data.payload,
-                                    order=data.order,
-                                    unique_type=data.unique_type,
-                                    max_results=data.max_results)
-            return list(events)
-
-        return []
-
-    async def query_flushed(self,
-                            event_id: common.EventId
-                            ) -> typing.AsyncIterable[list[common.Event]]:
-        async for events in self._ref_db.query(event_id):
-            yield events
+        raise ValueError('unsupported params type')
 
     async def flush(self):
-        async with self._flush_lock:
-            if not self._env.is_open:
-                return
-
-            dbs = [self._sys_db,
-                   self._latest_db,
-                   *self._ordered_dbs]
-            ext_flush_fns = [db.create_ext_flush() for db in dbs]
-            sessions = await self._env.execute(_ext_flush, self._env,
-                                               ext_flush_fns)
+        try:
+            future = self._loop.create_future()
+            await self._flush_queue.put(future)
+            await future
+
+        except aio.QueueClosedError:
+            raise common.BackendClosedError()
+
+    async def _flush_loop(self, flush_period):
+        futures = collections.deque()
 
-            for session in sessions:
-                self._flushed_events_cbs.notify(session)
+        async def cleanup():
+            with contextlib.suppress(Exception):
+                await self._flush()
+
+            await self._env.async_close()
 
-    async def _flush_loop(self):
         try:
             while True:
-                await asyncio.sleep(self._flush_period)
-                await aio.uncancellable(self.flush())
+                try:
+                    future = await aio.wait_for(self._flush_queue.get(),
+                                                flush_period)
+                    futures.append(future)
+
+                except asyncio.TimeoutError:
+                    pass
+
+                except aio.CancelledWithResultError as e:
+                    if e.result:
+                        futures.append(e.result)
+
+                    raise
+
+                while not self._flush_queue.empty():
+                    futures.append(self._flush_queue.get_nowait())
+
+                await aio.uncancellable(self._flush())
+
+                while futures:
+                    future = futures.popleft()
+                    if not future.done():
+                        future.set_result(None)
 
         except Exception as e:
             mlog.error('backend flush error: %s', e, exc_info=e)
 
         finally:
             self.close()
-            await aio.uncancellable(self._close())
+            self._flush_queue.close()
+
+            while not self._flush_queue.empty():
+                futures.append(self._flush_queue.get_nowait())
+
+            for future in futures:
+                if not future.done():
+                    future.set_exception(common.BackendClosedError())
 
-    async def _cleanup_loop(self):
+            await aio.uncancellable(cleanup())
+
+    async def _cleanup_loop(self, cleanup_period):
         try:
             while True:
-                repeat = True
-                while repeat:
-                    repeat = False
-
-                    for ordered_db in self._ordered_dbs:
-                        await asyncio.sleep(0)
-
-                        entries_removed = await aio.uncancellable(
-                            self._env.execute(ordered_db.ext_apply_limit,
-                                              common.now(),
-                                              cleanup_max_entries_remove))
+                await asyncio.sleep(0)
 
-                        if entries_removed >= cleanup_max_entries_remove:
-                            repeat = True
+                repeat = await self._env.execute(_ext_cleanup, self._env,
+                                                 self._dbs, common.now())
+                if repeat:
+                    continue
 
-                await asyncio.sleep(self._flush_period)
+                await asyncio.sleep(cleanup_period)
 
         except Exception as e:
             mlog.error('backend cleanup error: %s', e, exc_info=e)
 
         finally:
             self.close()
 
-    async def _close(self):
-        await self.flush()
-        await self._env.async_close()
+    async def _flush(self):
+        if not self._env.is_open:
+            return
+
+        self._registered_count = 0
+        registered_queue, self._registered_queue = (self._registered_queue,
+                                                    collections.deque())
+
+        changes = Changes(system=self._dbs.system.create_changes(),
+                          latest=self._dbs.latest.create_changes(),
+                          timeseries=self._dbs.timeseries.create_changes(),
+                          ref=self._dbs.ref.create_changes())
 
+        # TODO lock period between create_changes and locking executor
+        #      (timeseries and ref must write changes before new queries are
+        #      allowed)
+
+        await self._env.execute(_ext_flush, self._env, self._dbs, changes)
+
+        if not self._flushed_events_cb:
+            return
+
+        while registered_queue:
+            events = registered_queue.popleft()
+            await aio.call(self._flushed_events_cb, events)
+
+
+def _ext_flush(env, dbs, changes):
+    with env.ext_begin(write=True) as txn:
+        dbs.system.ext_write(txn, changes.system)
+        dbs.latest.ext_write(txn, changes.latest)
+        dbs.timeseries.ext_write(txn, changes.timeseries)
+        dbs.ref.ext_write(txn, changes.ref)
 
-def _ext_flush(env, flush_fns):
-    events = {}
 
+def _ext_cleanup(env, dbs, now):
     with env.ext_begin(write=True) as txn:
-        for flush_fn in flush_fns:
-            for event in flush_fn(txn):
-                events[event.event_id] = event
-
-    sessions = collections.deque()
-    session = collections.deque()
-
-    for event_id in sorted(events.keys()):
-        if session and session[0].event_id.session != event_id.session:
-            sessions.append(list(session))
-            session = collections.deque()
-        session.append(events[event_id])
+        result = dbs.timeseries.ext_cleanup(txn, now, cleanup_max_results)
+        if not result:
+            return False
 
-    if session:
-        sessions.append(list(session))
+        dbs.ref.ext_cleanup(txn, result)
 
-    return sessions
+    return len(result) >= cleanup_max_results
```

## Comparing `hat/event/server/backends/lmdb/conditions.py` & `hat/event/backends/lmdb/conditions.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 from hat import json
 
-from hat.event.server.backends.lmdb import common
+from hat.event.backends.lmdb import common
 
 
 class Conditions:
 
     def __init__(self, conf: json.Data):
-        self._conditions = [(common.Subscription(i['subscriptions']),
+        self._conditions = [(common.create_subscription(i['subscriptions']),
                              _create_condition(i['condition']))
                             for i in conf]
 
     def matches(self, event: common.Event) -> bool:
         for subscription, condition in self._conditions:
-            if not subscription.matches(event.event_type):
+            if not subscription.matches(event.type):
                 continue
+
             if not condition.matches(event):
                 return False
+
         return True
 
 
 def _create_condition(conf):
     if conf['type'] == 'all':
         return _AllCondition(conf)
 
@@ -52,17 +54,15 @@
 
 class _JsonCondition:
 
     def __init__(self, conf):
         self._conf = conf
 
     def matches(self, event):
-        if event.payload is None:
-            return False
-        if event.payload.type != common.EventPayloadType.JSON:
+        if not isinstance(event.payload, common.EventPayloadJson):
             return False
 
         data_path = self._conf.get('data_path', [])
         data = json.get(event.payload.data, data_path)
 
         if 'data_type' in self._conf:
             data_type = self._conf['data_type']
```

## Comparing `hat/event/server/backends/lmdb/ordereddb.py` & `hat/event/backends/lmdb/timeseriesdb.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,466 +1,486 @@
+from collections.abc import Iterable
 import collections
-import functools
 import itertools
 import typing
 
-from hat import json
+import lmdb
 
-from hat.event.server.backends.lmdb import common
-from hat.event.server.backends.lmdb import encoder
-from hat.event.server.backends.lmdb import environment
-from hat.event.server.backends.lmdb import refdb
-from hat.event.server.backends.lmdb.conditions import Conditions
+from hat.event.backends.lmdb import common
+from hat.event.backends.lmdb import environment
+from hat.event.backends.lmdb.conditions import Conditions
 
 
-Changes: typing.TypeAlias = typing.Iterable[tuple[common.Timestamp,
-                                                  common.Event]]
+Changes: typing.TypeAlias = dict[common.PartitionId,
+                                 collections.deque[tuple[common.Timestamp,
+                                                         common.Event]]]
+
+
+class Limit(typing.NamedTuple):
+    min_entries: int | None = None
+    max_entries: int | None = None
+    duration: float | None = None
+    size: int | None = None
+
+
+class Partition(typing.NamedTuple):
+    order_by: common.OrderBy
+    subscription: common.Subscription
+    limit: Limit | None
 
 
 def ext_create(env: environment.Environment,
-               ref_db: refdb.RefDb,
-               subscription: common.Subscription,
+               txn: lmdb.Transaction,
                conditions: Conditions,
-               order_by: common.OrderBy,
-               limit: json.Data | None
-               ) -> 'OrderedDb':
-    db = OrderedDb()
+               partitions: Iterable[Partition],
+               max_results: int = 4096
+               ) -> 'TimeseriesDb':
+    db = TimeseriesDb()
     db._env = env
-    db._ref_db = ref_db
-    db._subscription = subscription
     db._conditions = conditions
-    db._order_by = order_by
-    db._limit = limit
-    db._changes = collections.deque()
-
-    db._partition_id = None
-    last_partition_id = 0
-    partition_data = {
-        'order': order_by.value,
-        'subscriptions': [list(i)
-                          for i in sorted(subscription.get_query_types())]}
-
-    with env.ext_begin(write=True) as txn:
-        with env.ext_cursor(txn, common.DbType.ORDERED_PARTITION) as cursor:
-            for encoded_key, encoded_value in cursor:
-                key = encoder.decode_ordered_partition_db_key(encoded_key)
-                value = encoder.decode_ordered_partition_db_value(
-                    encoded_value)
-
-                last_partition_id = key
-                if value == partition_data:
-                    db._partition_id = last_partition_id
-                    break
-
-            if db._partition_id is None:
-                db._partition_id = last_partition_id + 1
-
-                encoded_key = encoder.encode_ordered_partition_db_key(
-                    db._partition_id)
-                encoded_value = encoder.encode_ordered_partition_db_value(
-                    partition_data)
+    db._max_results = max_results
+    db._changes = collections.defaultdict(collections.deque)
 
-                cursor.put(encoded_key, encoded_value)
+    # depending on dict order
+    db._partitions = dict(_ext_init_partitions(env, txn, partitions))
 
     return db
 
 
-class OrderedDb(common.Flushable):
+class TimeseriesDb:
 
-    @property
-    def partition_id(self) -> int:
-        return self._partition_id
+    def add(self,
+            event: common.Event
+            ) -> Iterable[common.EventRef]:
+        for partition_id, partition in self._partitions.items():
+            if not partition.subscription.matches(event.type):
+                continue
 
-    @property
-    def subscription(self) -> common.Subscription:
-        return self._subscription
+            if partition.order_by == common.OrderBy.TIMESTAMP:
+                timestamp = event.timestamp
 
-    @property
-    def order_by(self) -> common.OrderBy:
-        return self._order_by
+            elif partition.order_by == common.OrderBy.SOURCE_TIMESTAMP:
+                if event.source_timestamp is None:
+                    continue
 
-    def add(self, event: common.Event) -> bool:
-        if not self._subscription.matches(event.event_type):
-            return False
-
-        if self._order_by == common.OrderBy.TIMESTAMP:
-            timestamp = event.timestamp
-
-        elif self._order_by == common.OrderBy.SOURCE_TIMESTAMP:
-            if event.source_timestamp is None:
-                return False
-            timestamp = event.source_timestamp
+                timestamp = event.source_timestamp
 
-        else:
-            raise ValueError('unsupported order by')
+            else:
+                raise ValueError('unsupported order by')
+
+            self._changes[partition_id].append((timestamp, event))
 
-        self._changes.append((timestamp, event))
-        return True
+            yield common.TimeseriesEventRef(
+                (partition_id, timestamp, event.id))
 
     async def query(self,
-                    subscription: common.Subscription | None,
-                    server_id: int | None,
-                    event_ids: list[common.EventId] | None,
-                    t_from: common.Timestamp | None,
-                    t_to: common.Timestamp | None,
-                    source_t_from: common.Timestamp | None,
-                    source_t_to: common.Timestamp | None,
-                    payload: common.EventPayload | None,
-                    order: common.Order,
-                    unique_type: bool,
-                    max_results: int | None
-                    ) -> typing.Iterable[common.Event]:
-        unique_types = set() if unique_type else None
+                    params: common.QueryTimeseriesParams
+                    ) -> common.QueryResult:
+        subscription = (common.create_subscription(params.event_types)
+                        if params.event_types is not None else None)
+
+        max_results = (params.max_results
+                       if params.max_results is not None and
+                       params.max_results < self._max_results
+                       else self._max_results)
+
+        for partition_id, partition in self._partitions.items():
+            if partition.order_by != params.order_by:
+                continue
+
+            if (subscription and
+                    subscription.isdisjoint(partition.subscription)):
+                continue
+
+            return await self._query_partition(partition_id, params,
+                                               subscription, max_results)
+
+        return common.QueryResult(events=[],
+                                  more_follows=False)
+
+    def create_changes(self) -> Changes:
+        changes, self._changes = (self._changes,
+                                  collections.defaultdict(collections.deque))
+        return changes
+
+    def ext_write(self,
+                  txn: lmdb.Transaction,
+                  changes: Changes):
+        data = (((partition_id, timestamp, event.id), event)
+                for partition_id, partition_changes in changes.items()
+                for timestamp, event in partition_changes)
+        self._env.ext_write(txn, common.DbType.TIMESERIES_DATA, data)
+
+        counts = ((partition_id, len(partition_changes))
+                  for partition_id, partition_changes in changes.items())
+        _ext_inc_partition_count(self._env, txn, counts)
+
+    def ext_cleanup(self,
+                    txn: lmdb.Transaction,
+                    now: common.Timestamp,
+                    max_results: int | None = None,
+                    ) -> collections.deque[tuple[common.EventId,
+                                                 common.EventRef]]:
+        result = collections.deque()
+
+        for partition_id, partition in self._partitions.items():
+            if not partition.limit:
+                continue
+
+            partition_max_results = (max_results - len(result)
+                                     if max_results is not None else None)
+            if partition_max_results is not None and partition_max_results < 1:
+                break
+
+            result.extend(_ext_cleanup_partition(self._env, txn, now,
+                                                 partition_id, partition.limit,
+                                                 partition_max_results))
+
+        return result
+
+    async def _query_partition(self, partition_id, params, subscription,
+                               max_results):
         events = collections.deque()
+        changes = self._changes
 
-        if order == common.Order.DESCENDING:
-            events.extend(self._query_changes(
-                subscription, server_id, event_ids, t_from, t_to,
-                source_t_from, source_t_to, payload, order, unique_types,
-                max_results))
-
-            if max_results is not None:
-                max_results -= len(events)
-                if max_results <= 0:
-                    return events
+        filter = _Filter(subscription=subscription,
+                         t_from=params.t_from,
+                         t_to=params.t_to,
+                         source_t_from=params.source_t_from,
+                         source_t_to=params.source_t_to,
+                         max_results=max_results + 1,
+                         last_event_id=params.last_event_id)
+
+        if params.order == common.Order.DESCENDING:
+            events.extend(_query_partition_changes(
+                changes[partition_id], params, filter))
+
+            if not filter.done:
+                events.extend(await self._env.execute(
+                    _ext_query_partition_events, self._env, self._conditions,
+                    partition_id, params, filter))
 
+        elif params.order == common.Order.ASCENDING:
             events.extend(await self._env.execute(
-                self._ext_query, subscription, server_id, event_ids, t_from,
-                t_to, source_t_from, source_t_to, payload, order, unique_types,
-                max_results))
+                _ext_query_partition_events, self._env, self._conditions,
+                partition_id, params, filter))
 
-        elif order == common.Order.ASCENDING:
-            events.extend(await self._env.execute(
-                self._ext_query, subscription, server_id, event_ids, t_from,
-                t_to, source_t_from, source_t_to, payload, order, unique_types,
-                max_results))
-
-            if max_results is not None:
-                max_results -= len(events)
-                if max_results <= 0:
-                    return events
-
-            events.extend(self._query_changes(
-                subscription, server_id, event_ids, t_from, t_to,
-                source_t_from, source_t_to, payload, order, unique_types,
-                max_results))
+            if not filter.done:
+                events.extend(_query_partition_changes(
+                    changes[partition_id], params, filter))
 
         else:
             raise ValueError('unsupported order')
 
-        return events
-
-    def create_ext_flush(self) -> common.ExtFlushCb:
-        changes, self._changes = self._changes, collections.deque()
-        return functools.partial(self._ext_flush, changes)
-
-    def ext_apply_limit(self,
-                        now: common.Timestamp,
-                        max_entries_remove: int | None = None,
-                        ) -> int:
-        if not self._limit:
-            return True
-
-        with self._env.ext_begin(write=True) as txn:
-            entries_count = self._ext_get_entries_count(txn)
-            new_entries_count = self._ext_apply_limit(txn, entries_count,
-                                                      max_entries_remove, now)
-            if new_entries_count != entries_count:
-                self._ext_set_entries_count(txn, new_entries_count)
-
-        return entries_count - new_entries_count
-
-    def _query_changes(self, subscription, server_id, event_ids, t_from, t_to,
-                       source_t_from, source_t_to, payload, order,
-                       unique_types, max_results):
-        if order == common.Order.DESCENDING:
-            events = (event for _, event in reversed(self._changes))
-
-            if (self._order_by == common.OrderBy.TIMESTAMP and
-                    t_to is not None):
-                events = itertools.dropwhile(
-                    lambda i: t_to < i.timestamp,
-                    events)
-
-            elif (self._order_by == common.OrderBy.SOURCE_TIMESTAMP and
-                    source_t_to is not None):
-                events = itertools.dropwhile(
-                    lambda i: source_t_to < i.source_timestamp,
-                    events)
-
-            if (self._order_by == common.OrderBy.TIMESTAMP and
-                    t_from is not None):
-                events = itertools.takewhile(
-                    lambda i: t_from <= i.timestamp,
-                    events)
-
-            elif (self._order_by == common.OrderBy.SOURCE_TIMESTAMP and
-                    source_t_from is not None):
-                events = itertools.takewhile(
-                    lambda i: source_t_from <= i.source_timestamp,
-                    events)
-
-        elif order == common.Order.ASCENDING:
-            events = (event for _, event in self._changes)
-
-            if (self._order_by == common.OrderBy.TIMESTAMP and
-                    t_from is not None):
-                events = itertools.dropwhile(
-                    lambda i: i.timestamp < t_from,
-                    events)
-
-            elif (self._order_by == common.OrderBy.SOURCE_TIMESTAMP and
-                    source_t_from is not None):
-                events = itertools.dropwhile(
-                    lambda i: i.source_timestamp < source_t_from,
-                    events)
-
-            if (self._order_by == common.OrderBy.TIMESTAMP and
-                    t_to is not None):
-                events = itertools.takewhile(
-                    lambda i: i.timestamp <= t_to,
-                    events)
-
-            elif (self._order_by == common.OrderBy.SOURCE_TIMESTAMP and
-                    source_t_to is not None):
-                events = itertools.takewhile(
-                    lambda i: i.source_timestamp <= source_t_to,
-                    events)
+        more_follows = len(events) > max_results
+        while len(events) > max_results:
+            events.pop()
+
+        return common.QueryResult(events=events,
+                                  more_follows=more_follows)
+
+
+def _ext_init_partitions(env, txn, partitions):
+    db_data = dict(env.ext_read(txn, common.DbType.TIMESERIES_PARTITION))
+    next_partition_ids = itertools.count(max(db_data.keys(), default=0) + 1)
+
+    for partition in partitions:
+        event_types = sorted(partition.subscription.get_query_types())
+        partition_data = {'order': partition.order_by.value,
+                          'subscriptions': [list(i) for i in event_types]}
+
+        for partition_id, i in db_data.items():
+            if i == partition_data:
+                break
 
         else:
-            raise ValueError('unsupported order')
+            partition_id = next(next_partition_ids)
+            db_data[partition_id] = partition_data
+            env.ext_write(txn, common.DbType.TIMESERIES_PARTITION,
+                          [(partition_id, partition_data)])
 
-        yield from _filter_events(events, subscription, server_id, event_ids,
-                                  t_from, t_to, source_t_from, source_t_to,
-                                  payload, unique_types, max_results)
-
-    def _ext_query(self, subscription, server_id, event_ids, t_from, t_to,
-                   source_t_from, source_t_to, payload, order,
-                   unique_types, max_results):
-        if self._order_by == common.OrderBy.TIMESTAMP:
-            events = self._ext_query_events(t_from, t_to, order)
+        yield partition_id, partition
 
-        elif self._order_by == common.OrderBy.SOURCE_TIMESTAMP:
-            events = self._ext_query_events(source_t_from, source_t_to, order)
 
-        else:
-            raise ValueError('unsupported order by')
+def _ext_query_partition_events(env, conditions, partition_id, params,
+                                filter):
+    if params.order_by == common.OrderBy.TIMESTAMP:
+        events = _ext_query_partition_events_range(
+            env, partition_id, params.t_from, params.t_to, params.order)
+
+    elif params.order_by == common.OrderBy.SOURCE_TIMESTAMP:
+        events = _ext_query_partition_events_range(
+            env, partition_id, params.source_t_from, params.source_t_to,
+            params.order)
+
+    else:
+        raise ValueError('unsupported order by')
+
+    events = (event for event in events if conditions.matches(event))
+    events = filter.process(events)
+    return collections.deque(events)
+
+
+def _ext_query_partition_events_range(env, partition_id, t_from, t_to, order):
+    db_def = common.db_defs[common.DbType.TIMESERIES_DATA]
+
+    if not t_from:
+        t_from = common.min_timestamp
 
-        events = (event for event in events if self._conditions.matches(event))
+    from_key = partition_id, t_from, common.EventId(0, 0, 0)
+    encoded_from_key = db_def.encode_key(from_key)
 
-        events = _filter_events(events, subscription, server_id, event_ids,
-                                t_from, t_to, source_t_from, source_t_to,
-                                payload, unique_types, max_results)
-        return list(events)
-
-    def _ext_query_events(self, t_from, t_to, order):
-        if not t_from:
-            t_from = common.Timestamp(s=-(1 << 63), us=0)
-        from_key = (self._partition_id,
-                    t_from,
-                    common.EventId(0, 0, 0))
-        encoded_from_key = encoder.encode_ordered_data_db_key(from_key)
-
-        if not t_to:
-            t_to = common.Timestamp(s=(1 << 63) - 1, us=int(1e6))
-        to_key = (self._partition_id,
-                  t_to,
-                  common.EventId((1 << 64) - 1, (1 << 64) - 1, (1 << 64) - 1))
-        encoded_to_key = encoder.encode_ordered_data_db_key(to_key)
-
-        with self._env.ext_begin() as txn:
-            with self._env.ext_cursor(txn,
-                                      common.DbType.ORDERED_DATA) as cursor:
-                if order == common.Order.DESCENDING:
-                    encoded_start_key, encoded_stop_key = (encoded_to_key,
-                                                           encoded_from_key)
-
-                    if cursor.set_range(encoded_start_key):
-                        more = cursor.prev()
-                    else:
-                        more = cursor.last()
-
-                    while more and encoded_stop_key <= bytes(cursor.key()):
-                        yield encoder.decode_ordered_data_db_value(
-                            cursor.value())
-                        more = cursor.prev()
-
-                elif order == common.Order.ASCENDING:
-                    encoded_start_key, encoded_stop_key = (encoded_from_key,
-                                                           encoded_to_key)
-
-                    more = cursor.set_range(encoded_start_key)
-
-                    while more and bytes(cursor.key()) < encoded_stop_key:
-                        yield encoder.decode_ordered_data_db_value(
-                            cursor.value())
-                        more = cursor.next()
+    if not t_to:
+        t_to = common.max_timestamp
 
+    to_key = (partition_id,
+              t_to,
+              common.EventId((1 << 64) - 1, (1 << 64) - 1, (1 << 64) - 1))
+    encoded_to_key = db_def.encode_key(to_key)
+
+    with env.ext_begin() as txn:
+        with env.ext_cursor(txn, common.DbType.TIMESERIES_DATA) as cursor:
+            if order == common.Order.DESCENDING:
+                encoded_start_key, encoded_stop_key = (encoded_to_key,
+                                                       encoded_from_key)
+
+                if cursor.set_range(encoded_start_key):
+                    more = cursor.prev()
                 else:
-                    raise ValueError('unsupported order')
+                    more = cursor.last()
 
-    def _ext_flush(self, changes, txn):
-        entries_count = self._ext_get_entries_count(txn)
-        new_entries_count = self._ext_add_changes(txn, entries_count, changes)
-        if new_entries_count != entries_count:
-            self._ext_set_entries_count(txn, new_entries_count)
-        return (i for _, i in changes)
-
-    def _ext_get_entries_count(self, txn):
-        with self._env.ext_cursor(txn, common.DbType.ORDERED_COUNT) as cursor:
-            key = self._partition_id
-            encoded_key = encoder.encode_ordered_count_db_key(key)
+                while more and encoded_stop_key <= bytes(cursor.key()):
+                    yield db_def.decode_value(cursor.value())
+                    more = cursor.prev()
 
-            encoded_value = cursor.get(encoded_key)
-            return (encoder.decode_ordered_count_db_value(encoded_value)
-                    if encoded_value else 0)
+            elif order == common.Order.ASCENDING:
+                encoded_start_key, encoded_stop_key = (encoded_from_key,
+                                                       encoded_to_key)
+
+                more = cursor.set_range(encoded_start_key)
+
+                while more and bytes(cursor.key()) <= encoded_stop_key:
+                    yield db_def.decode_value(cursor.value())
+                    more = cursor.next()
+
+            else:
+                raise ValueError('unsupported order')
+
+
+def _ext_get_partition_count(env, txn, partition_id):
+    db_def = common.db_defs[common.DbType.TIMESERIES_COUNT]
 
-    def _ext_set_entries_count(self, txn, entries_count):
-        with self._env.ext_cursor(txn, common.DbType.ORDERED_COUNT) as cursor:
-            key = self._partition_id
-            encoded_key = encoder.encode_ordered_count_db_key(key)
+    with env.ext_cursor(txn, common.DbType.TIMESERIES_COUNT) as cursor:
+        encoded_key = db_def.encode_key(partition_id)
+        encoded_value = cursor.get(encoded_key)
 
-            value = entries_count
-            encoded_value = encoder.encode_ordered_count_db_value(value)
+        return db_def.decode_value(encoded_value) if encoded_value else 0
 
+
+def _ext_set_partition_count(env, txn, partition_id, count):
+    env.ext_write(txn, common.DbType.TIMESERIES_COUNT, [(partition_id, count)])
+
+
+def _ext_inc_partition_count(env, txn, partition_counts):
+    db_def = common.db_defs[common.DbType.TIMESERIES_COUNT]
+
+    with env.ext_cursor(txn, common.DbType.TIMESERIES_COUNT) as cursor:
+        for partition_id, count in partition_counts:
+            encoded_key = db_def.encode_key(partition_id)
+            encoded_value = cursor.get(encoded_key)
+
+            value = db_def.decode_value(encoded_value) if encoded_value else 0
+            inc_value = value + count
+
+            encoded_value = db_def.encode_value(inc_value)
             cursor.put(encoded_key, encoded_value)
 
-    def _ext_add_changes(self, txn, entries_count, changes):
-        if not changes:
-            return entries_count
-
-        with self._env.ext_cursor(txn, common.DbType.ORDERED_DATA) as cursor:
-            for timestamp, event in changes:
-                key = self._partition_id, timestamp, event.event_id
-                encoded_key = encoder.encode_ordered_data_db_key(key)
-
-                value = event
-                encoded_value = encoder.encode_ordered_data_db_value(value)
-
-                cursor.put(encoded_key, encoded_value)
-                entries_count += 1
-
-                event_ref = common.OrderedEventRef(key)
-                self._ref_db.ext_add_event_ref(txn, event.event_id, event_ref)
-
-        return entries_count
-
-    def _ext_apply_limit(self, txn, entries_count, max_entries_remove, now):
-        if not self._limit:
-            return entries_count
-
-        timestamp = common.Timestamp(s=-(1 << 63), us=0)
-        start_key = (self._partition_id,
-                     timestamp,
-                     common.EventId(0, 0, 0))
-        stop_key = ((self._partition_id + 1),
-                    timestamp,
-                    common.EventId(0, 0, 0))
-
-        encoded_start_key = encoder.encode_ordered_data_db_key(start_key)
-        encoded_stop_key = encoder.encode_ordered_data_db_key(stop_key)
-
-        min_entries = self._limit.get('min_entries', 0)
-        max_entries = None
-        encoded_duration_key = None
-
-        if 'size' in self._limit:
-            stat = self._env.ext_stat(txn, common.DbType.ORDERED_DATA)
-
-            if stat['entries']:
-                total_size = stat['psize'] * (stat['branch_pages'] +
-                                              stat['leaf_pages'] +
-                                              stat['overflow_pages'])
-                entry_size = total_size / stat['entries']
-                max_entries = int(self._limit['size'] / entry_size)
-
-        if 'max_entries' in self._limit:
-            max_entries = (
-                self._limit['max_entries'] if max_entries is None
-                else min(max_entries, self._limit['max_entries']))
-
-        if 'duration' in self._limit:
-            duration_key = (self._partition_id,
-                            now.add(-self._limit['duration']),
-                            common.EventId(0, 0, 0))
-            encoded_duration_key = encoder.encode_ordered_data_db_key(
-                duration_key)
-
-        with self._env.ext_cursor(txn, common.DbType.ORDERED_DATA) as cursor:
-            more = cursor.set_range(encoded_start_key)
-            while more:
-                if max_entries_remove is not None and max_entries_remove < 1:
-                    break
-
-                if entries_count <= min_entries:
-                    break
-
-                encoded_key = bytes(cursor.key())
-                if encoded_key >= encoded_stop_key:
-                    break
-
-                if ((max_entries is None or
-                     entries_count <= max_entries) and
-                    (encoded_duration_key is None or
-                     encoded_key >= encoded_duration_key)):
-                    break
-
-                key = encoder.decode_ordered_data_db_key(encoded_key)
-                event_id = key[2]
-
-                more = cursor.delete()
-                entries_count -= 1
-                if max_entries_remove is not None:
-                    max_entries_remove -= 1
-
-                event_ref = common.OrderedEventRef(key)
-                self._ref_db.ext_remove_event_ref(txn, event_id, event_ref)
-
-        return entries_count
-
-
-def _filter_events(events, subscription, server_id, event_ids, t_from, t_to,
-                   source_t_from, source_t_to, payload, unique_types,
-                   max_results):
-    if max_results is not None and max_results <= 0:
-        return
-
-    for event in events:
-        if server_id is not None and event.event_id.server != server_id:
-            continue
-
-        if subscription and not subscription.matches(event.event_type):
-            continue
-
-        if event_ids is not None and event.event_id not in event_ids:
-            continue
-
-        if t_from is not None and event.timestamp < t_from:
-            continue
-
-        if t_to is not None and t_to < event.timestamp:
-            continue
-
-        if source_t_from is not None and (
-                event.source_timestamp is None or
-                event.source_timestamp < source_t_from):
-            continue
-
-        if source_t_to is not None and (
-                event.source_timestamp is None or
-                source_t_to < event.source_timestamp):
-            continue
 
-        if payload is not None and event.payload != payload:
-            continue
+def _ext_cleanup_partition(env, txn, now, partition_id, limit, max_results):
+    db_def = common.db_defs[common.DbType.TIMESERIES_DATA]
 
-        if unique_types is not None:
-            if event.event_type in unique_types:
-                continue
-            unique_types.add(event.event_type)
+    timestamp = common.min_timestamp
+    start_key = (partition_id,
+                 timestamp,
+                 common.EventId(0, 0, 0))
+    stop_key = ((partition_id + 1),
+                timestamp,
+                common.EventId(0, 0, 0))
+
+    encoded_start_key = db_def.encode_key(start_key)
+    encoded_stop_key = db_def.encode_key(stop_key)
+
+    min_entries = limit.min_entries or 0
+    max_entries = None
+    encoded_duration_key = None
+
+    if limit.size is not None:
+        stat = env.ext_stat(txn, common.DbType.TIMESERIES_DATA)
+
+        if stat['entries']:
+            total_size = stat['psize'] * (stat['branch_pages'] +
+                                          stat['leaf_pages'] +
+                                          stat['overflow_pages'])
+            entry_size = total_size / stat['entries']
+            max_entries = int(limit.size / entry_size)
+
+    if limit.max_entries is not None:
+        max_entries = (limit.max_entries if max_entries is None
+                       else min(max_entries, limit.max_entries))
+
+    if limit.duration is not None:
+        duration_key = (partition_id,
+                        now.add(-limit.duration),
+                        common.EventId(0, 0, 0))
+        encoded_duration_key = db_def.encode_key(duration_key)
+
+    result_count = 0
+    entries_count = _ext_get_partition_count(env, txn, partition_id)
+
+    with env.ext_cursor(txn, common.DbType.TIMESERIES_DATA) as cursor:
+        more = cursor.set_range(encoded_start_key)
+        while more:
+            if max_results is not None and result_count >= max_results:
+                break
+
+            if entries_count - result_count <= min_entries:
+                break
+
+            encoded_key = bytes(cursor.key())
+            if encoded_key >= encoded_stop_key:
+                break
+
+            if ((max_entries is None or
+                 entries_count - result_count <= max_entries) and
+                (encoded_duration_key is None or
+                 encoded_key >= encoded_duration_key)):
+                break
+
+            key = db_def.decode_key(encoded_key)
+            event_id = key[2]
+
+            more = cursor.delete()
+            result_count += 1
+
+            yield event_id, common.TimeseriesEventRef(key)
+
+    if result_count > 0:
+        _ext_set_partition_count(env, txn, partition_id,
+                                 entries_count - result_count)
+
+
+def _query_partition_changes(changes, params, filter):
+    if params.order == common.Order.DESCENDING:
+        events = (event for _, event in reversed(changes))
+
+        if (params.order_by == common.OrderBy.TIMESTAMP and
+                params.t_to is not None):
+            events = itertools.dropwhile(
+                lambda i: params.t_to < i.timestamp,
+                events)
+
+        elif (params.order_by == common.OrderBy.SOURCE_TIMESTAMP and
+                params.source_t_to is not None):
+            events = itertools.dropwhile(
+                lambda i: params.source_t_to < i.source_timestamp,
+                events)
+
+        if (params.order_by == common.OrderBy.TIMESTAMP and
+                params.t_from is not None):
+            events = itertools.takewhile(
+                lambda i: params.t_from <= i.timestamp,
+                events)
+
+        elif (params.order_by == common.OrderBy.SOURCE_TIMESTAMP and
+                params.source_t_from is not None):
+            events = itertools.takewhile(
+                lambda i: params.source_t_from <= i.source_timestamp,
+                events)
+
+    elif params.order == common.Order.ASCENDING:
+        events = (event for _, event in changes)
+
+        if (params.order_by == common.OrderBy.TIMESTAMP and
+                params.t_from is not None):
+            events = itertools.dropwhile(
+                lambda i: i.timestamp < params.t_from,
+                events)
+
+        elif (params.order_by == common.OrderBy.SOURCE_TIMESTAMP and
+                params.source_t_from is not None):
+            events = itertools.dropwhile(
+                lambda i: i.source_timestamp < params.source_t_from,
+                events)
+
+        if (params.order_by == common.OrderBy.TIMESTAMP and
+                params.t_to is not None):
+            events = itertools.takewhile(
+                lambda i: i.timestamp <= params.t_to,
+                events)
+
+        elif (params.order_by == common.OrderBy.SOURCE_TIMESTAMP and
+                params.source_t_to is not None):
+            events = itertools.takewhile(
+                lambda i: i.source_timestamp <= params.source_t_to,
+                events)
+
+    else:
+        raise ValueError('unsupported order')
+
+    return filter.process(events)
+
+
+class _Filter:
+
+    def __init__(self,
+                 subscription: common.Subscription,
+                 t_from: common.Timestamp | None,
+                 t_to: common.Timestamp | None,
+                 source_t_from: common.Timestamp | None,
+                 source_t_to: common.Timestamp | None,
+                 max_results: int,
+                 last_event_id: common.EventId | None):
+        self._subscription = subscription
+        self._t_from = t_from
+        self._t_to = t_to
+        self._source_t_from = source_t_from
+        self._source_t_to = source_t_to
+        self._max_results = max_results
+        self._last_event_id = last_event_id
 
-        yield event
+    @property
+    def done(self):
+        return self._max_results < 1
 
-        if max_results is not None:
-            max_results -= 1
-            if max_results <= 0:
+    def process(self, events: Iterable[common.Event]):
+        for event in events:
+            if self._max_results < 1:
                 return
+
+            if self._last_event_id:
+                if event.id == self._last_event_id:
+                    self._last_event_id = None
+
+                continue
+
+            if self._t_from is not None and event.timestamp < self._t_from:
+                continue
+
+            if self._t_to is not None and self._t_to < event.timestamp:
+                continue
+
+            if self._source_t_from is not None and (
+                    event.source_timestamp is None or
+                    event.source_timestamp < self._source_t_from):
+                continue
+
+            if self._source_t_to is not None and (
+                    event.source_timestamp is None or
+                    self._source_t_to < event.source_timestamp):
+                continue
+
+            if (self._subscription and
+                    not self._subscription.matches(event.type)):
+                continue
+
+            self._max_results -= 1
+            yield event
```

## Comparing `hat/event/server/backends/lmdb/convert/convert_v06_to_v07.py` & `hat/event/backends/lmdb/convert/convert_v06_to_v07.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,24 +1,12 @@
 from pathlib import Path
 import itertools
-import sys
 
-from hat.event.server.backends.lmdb.convert import v06
-from hat.event.server.backends.lmdb.convert import v07
-
-
-def main():
-    if len(sys.argv) != 3:
-        print("Usage: %s SRC_DB_PATH DST_DB_PATH", file=sys.stderr)
-        sys.exit(1)
-
-    src_path = Path(sys.argv[1])
-    dst_path = Path(sys.argv[2])
-
-    convert(src_path, dst_path)
+from hat.event.backends.lmdb.convert import v06
+from hat.event.backends.lmdb.convert import v07
 
 
 def convert(src_path: Path,
             dst_path: Path):
     with v06.create_env(src_path) as src_env:
         src_system_db = src_env.open_db(b'system')
         server_id = _get_server_id(src_env, src_system_db)
@@ -215,11 +203,7 @@
         type=v07.EventPayloadType[src_event_payload.type.name],
         data=src_event_payload.data)
 
 
 def _convert_timestamp(src_timestamp):
     return v07.Timestamp(s=src_timestamp.s,
                          us=src_timestamp.us)
-
-
-if __name__ == '__main__':
-    main()
```

## Comparing `hat/event/server/backends/lmdb/convert/v06.py` & `hat/event/backends/lmdb/convert/v06.py`

 * *Files 24% similar despite different names*

```diff
@@ -2,81 +2,46 @@
 import enum
 import struct
 import platform
 import typing
 
 import lmdb
 
-from hat import chatter
 from hat import json
 from hat import sbs
 
 
 EventType: typing.TypeAlias = typing.Tuple[str, ...]
-"""Event type"""
 
 
 EventPayloadType = enum.Enum('EventPayloadType', [
     'BINARY',
     'JSON',
     'SBS'])
 
 
 class EventId(typing.NamedTuple):
     server: int
-    """server identifier"""
     instance: int
-    """event instance identifier"""
 
 
 class SbsData(typing.NamedTuple):
     module: str | None
-    """SBS module name"""
     type: str
-    """SBS type name"""
     data: bytes
 
 
 class EventPayload(typing.NamedTuple):
     type: EventPayloadType
     data: bytes | json.Data | SbsData
 
 
 class Timestamp(typing.NamedTuple):
     s: int
-    """seconds since 1970-01-01 (can be negative)"""
     us: int
-    """microseconds added to timestamp seconds in range [0, 1e6)"""
-
-    def __lt__(self, other):
-        if not isinstance(other, Timestamp):
-            return NotImplemented
-        return self.s * 1000000 + self.us < other.s * 1000000 + other.us
-
-    def __gt__(self, other):
-        if not isinstance(other, Timestamp):
-            return NotImplemented
-        return self.s * 1000000 + self.us > other.s * 1000000 + other.us
-
-    def __eq__(self, other):
-        if not isinstance(other, Timestamp):
-            return NotImplemented
-        return self.s * 1000000 + self.us == other.s * 1000000 + other.us
-
-    def __ne__(self, other):
-        return not self == other
-
-    def __le__(self, other):
-        return self < other or self == other
-
-    def __ge__(self, other):
-        return self > other or self == other
-
-    def __hash__(self):
-        return self.s * 1000000 + self.us
 
 
 class Event(typing.NamedTuple):
     event_id: EventId
     event_type: EventType
     timestamp: Timestamp
     source_timestamp: Timestamp | None
@@ -103,30 +68,30 @@
 def decode_uint_timestamp_uint(x: bytes
                                ) -> tuple[int, Timestamp, int]:
     res = struct.unpack(">QQIQ", x)
     return res[0], Timestamp(res[1] - (1 << 63), res[2]), res[3]
 
 
 def decode_event(event_bytes: bytes) -> Event:
-    event_sbs = _sbs_repo.decode('HatEvent', 'Event', event_bytes)
+    event_sbs = _sbs_repo.decode('HatEvent.Event', event_bytes)
     return _event_from_sbs(event_sbs)
 
 
 def create_env(path: Path):
     max_dbs = 5
     max_db_size = (512 * 1024 * 1024 * 1024
                    if platform.architecture()[0] == '64bit'
                    else 1024 * 1024 * 1024)
     return lmdb.Environment(str(path),
                             map_size=max_db_size,
                             subdir=False,
                             max_dbs=max_dbs)
 
 
-_sbs_repo = sbs.Repository(chatter.sbs_repo, r"""
+_sbs_repo = sbs.Repository(r"""
 module HatEvent
 
 MsgSubscribe = Array(EventType)
 
 MsgNotify = Array(Event)
 
 MsgRegisterReq = Array(RegisterEvent)
@@ -161,15 +126,19 @@
 }
 
 EventType = Array(String)
 
 EventPayload = Choice {
     binary:  Bytes
     json:    String
-    sbs:     Hat.Data
+    sbs:     Record {
+        module:  Optional(String)
+        type:    String
+        data:    Bytes
+    }
 }
 
 Event = Record {
     id:               EventId
     type:             EventType
     timestamp:        Timestamp
     sourceTimestamp:  Optional(Timestamp)
```

## Comparing `hat/event/server/backends/lmdb/manager/common.py` & `hat/event/backends/lmdb/manager/common.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,39 +1,45 @@
-from hat.event.server.backends.lmdb.common import *  # NOQA
+from hat.event.backends.lmdb.common import *  # NOQA
 
 from hat import json
 
-from hat.event.server.common import EventId, Timestamp, Event, EventPayloadType
+from hat.event.backends.lmdb.common import (EventId,
+                                            Timestamp,
+                                            Event,
+                                            EventPayloadJson,
+                                            EventPayloadBinary)
 
 
 def event_id_to_json(event_id: EventId) -> json.Data:
     return {'server': event_id.server,
             'session': event_id.session,
             'instance': event_id.instance}
 
 
 def timestamp_to_json(timestamp: Timestamp) -> json.Data:
     return {'s': timestamp.s,
             'us': timestamp.us}
 
 
 def event_to_json(event: Event) -> json.Data:
-    return {'event_id': event_id_to_json(event.event_id),
-            'event_type': list(event.event_type),
+    return {'id': event_id_to_json(event.id),
+            'type': list(event.type),
             'timestamp': timestamp_to_json(event.timestamp),
             'source_timestamp': (timestamp_to_json(event.source_timestamp)
                                  if event.source_timestamp else None),
             'payload': _event_payload_to_json(event.payload)}
 
 
 def _event_payload_to_json(payload):
-    if payload.type == EventPayloadType.BINARY:
-        return {'type': 'BINARY'}
+    if payload is None:
+        return None
 
-    if payload.type == EventPayloadType.SBS:
-        return {'type': 'SBS'}
+    if isinstance(payload, EventPayloadBinary):
+        return {'type': 'BINARY',
+                'subtype': payload.type,
+                'data': bytes(payload.data).hex()}
 
-    if payload.type == EventPayloadType.JSON:
+    if isinstance(payload, EventPayloadJson):
         return {'type': 'JSON',
                 'data': payload.data}
 
     raise ValueError('unsupported payload type')
```

## Comparing `hat/event/server/backends/lmdb/manager/main.py` & `hat/event/backends/lmdb/manager/main.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,40 +1,34 @@
 import argparse
 import sys
 
-from hat import json
-
-from hat.event.server.backends.lmdb.manager import copy
-from hat.event.server.backends.lmdb.manager import query
+from hat.event.backends.lmdb.manager import copy
+from hat.event.backends.lmdb.manager import query
 
 
 def create_argument_parser() -> argparse.ArgumentParser:
     parser = argparse.ArgumentParser()
-    parser.add_argument('--indent', type=int, default=None)
     subparsers = parser.add_subparsers(dest='action', required=True)
 
     query.create_argument_parser(subparsers)
     copy.create_argument_parser(subparsers)
 
     return parser
 
 
 def main():
     parser = create_argument_parser()
     args = parser.parse_args()
 
     if args.action == 'query':
-        results = query.query(args)
+        query.query(args)
 
     elif args.action == 'copy':
-        results = copy.copy(args)
+        copy.copy(args)
 
     else:
         raise ValueError('unsupported action')
 
-    for result in results:
-        print(json.encode(result, indent=args.indent))
-
 
 if __name__ == '__main__':
     sys.argv[0] = 'hat-event-lmdb-manager'
     sys.exit(main())
```

## Comparing `hat_event-0.8.9.dist-info/LICENSE` & `hat_event-0.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `hat_event-0.8.9.dist-info/METADATA` & `hat_event-0.9.0.dist-info/METADATA`

 * *Files 24% similar despite different names*

```diff
@@ -1,35 +1,31 @@
 Metadata-Version: 2.1
 Name: hat-event
-Version: 0.8.9
+Version: 0.9.0
 Summary: Hat event
+Description-Content-Type: text/x-rst
 License: Apache-2.0
+Provides-Extra: dev
+Requires-Dist: appdirs ~=1.4.4
+Requires-Dist: hat-aio ~=0.7.9
+Requires-Dist: hat-drivers ~=0.7.10
+Requires-Dist: hat-json ~=0.5.26
+Requires-Dist: hat-monitor ~=0.8.5
+Requires-Dist: hat-sbs ~=0.7.1
+Requires-Dist: hat-util ~=0.6.13
+Requires-Dist: lmdb ~=1.4.1
+Requires-Dist: hat-doit ~=0.15.11; extra == 'dev'
+Requires-Dist: peru >=1.3.1; extra == 'dev'
+Requires-Dist: psutil >=5.9.5; extra == 'dev'
+Requires-Dist: sphinxcontrib-plantuml >=0.23; extra == 'dev'
+Requires-Dist: sphinxcontrib-programoutput >=0.17; extra == 'dev'
+Requires-Python: >=3.10
 Project-URL: Homepage, http://hat-open.com
 Project-URL: Repository, https://github.com/hat-open/hat-event.git
 Project-URL: Documentation, http://hat-event.hat-open.com
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: Apache Software License
-Requires-Python: >=3.10
-Description-Content-Type: text/x-rst
-License-File: LICENSE
-Requires-Dist: appdirs (~=1.4.4)
-Requires-Dist: hat-aio (~=0.7.8)
-Requires-Dist: hat-chatter (~=0.5.11)
-Requires-Dist: hat-drivers (~=0.7.0)
-Requires-Dist: hat-json (~=0.5.19)
-Requires-Dist: hat-monitor (<0.8.0,>=0.6.10)
-Requires-Dist: hat-sbs (~=0.6.4)
-Requires-Dist: hat-util (~=0.6.10)
-Requires-Dist: lmdb (~=1.4.1)
-Provides-Extra: dev
-Requires-Dist: hat-doit (~=0.14.6) ; extra == 'dev'
-Requires-Dist: peru (>=1.3.1) ; extra == 'dev'
-Requires-Dist: psutil (>=5.9.5) ; extra == 'dev'
-Requires-Dist: sphinxcontrib-plantuml (>=0.23) ; extra == 'dev'
-Requires-Dist: sphinxcontrib-programoutput (>=0.17) ; extra == 'dev'
 
 .. _online documentation: https://hat-event.hat-open.com
 .. _git repository: https://github.com/hat-open/hat-event.git
 .. _PyPI project: https://pypi.org/project/hat-event
 .. _pydoit: https://pydoit.org
 .. _Hat Open: https://hat-open.com
 .. _KonÄar Digital: https://www.koncar.hr/en
@@ -93,15 +89,15 @@
 Development of Hat Open and associated repositories is sponsored by
 `KonÄar Digital`_.
 
 
 License
 -------
 
-Copyright 2020-2023 Hat Open AUTHORS
+Copyright 2020-2024 Hat Open AUTHORS
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
 
     http://www.apache.org/licenses/LICENSE-2.0
```

## Comparing `hat_event-0.8.9.dist-info/RECORD` & `hat_event-0.9.0.dist-info/RECORD`

 * *Files 25% similar despite different names*

```diff
@@ -1,63 +1,62 @@
-hat/event/__init__.py,sha256=nIQrqyPevwuBP-5p0MrmV1iLnevMXoz8t5moAAl21HI,47
-hat/event/common/__init__.py,sha256=vnT2TYTXVSPRfvj2g1r-a443bkvc474LIJ-bqfuYl6s,2696
-hat/event/common/data.py,sha256=y0Cc2meBbs16a7WOLh8gO6M4zduwELRDZt9MMLEQ_6A,8535
-hat/event/common/json_schema_repo.json,sha256=-_KxbXErlJHsY68dO4iV9iTbJVFobCdEUv0K3xAqvNo,9158
-hat/event/common/sbs_repo.json,sha256=oBozeqtpz8EDg9oGYpT3avGNScD1T8Jf053m25_lfLQ,7842
-hat/event/common/timestamp.py,sha256=iBAx2DfReKUUygntIUqcMhlKB_odAHwap5n03qqjitU,4235
-hat/event/common/subscription/__init__.py,sha256=2Zf0yFdQeXHEJQcepUIzochi9KUdB_h0IlSd0_zyCUs,534
-hat/event/common/subscription/_csubscription.abi3.pyd,sha256=qZFAg9d9sIWAoE3oLahaEelD7P11f9vtJNmoE0ZmOgM,122941
-hat/event/common/subscription/common.py,sha256=rN8DfK-6cmObUFJRB7H77popuzKrHfp_e6giUEVWVv0,5758
-hat/event/common/subscription/csubscription.py,sha256=jon0XjECTZcJ68KZQ-hMFLahNALc62za1TRJzwzoMGQ,1530
-hat/event/common/subscription/pysubscription.py,sha256=olBvh753U3v4Sh5GdoFkuMAky6MqtCTt90TbZ8TRkaE,2670
-hat/event/eventer/__init__.py,sha256=r1o7PfU_dIoawdjuuKsX8RXvrX8w885fcfaKAeCsEWU,2550
-hat/event/eventer/client.py,sha256=yoLnLQSyOrS8AsG0yv9aPxQ4wNpR-Q8hes4nMgTfrQA,12692
-hat/event/eventer/server.py,sha256=fFXNgS63IrjmWvQPDV--MpRYDwAxaNf0mnnjVLnHTGo,6502
-hat/event/mariner/__init__.py,sha256=DRtrP_IgCnY6J2yvuTrC1dXYN-WE3T3anxdCs41FSlg,521
-hat/event/mariner/client.py,sha256=kOuBHNzcwX2aP8LmhjEJwdTTSxC7K8_goet-s6E1h2o,3940
-hat/event/mariner/common.py,sha256=Rp7lzgzkAT2ny5KbfH5p1nGh9l2uMmhSMygRL41N6q0,481
-hat/event/mariner/encoder.py,sha256=mNnCyK8QSbXXnGtEWknZY1HmOwjG-g0LGo0_z6SfDBE,6806
-hat/event/mariner/server.py,sha256=-wnGsWveQ4AvNudr9mqSR-TEkykfWC-BOHOyilUsB_s,5119
-hat/event/mariner/transport.py,sha256=eXpB4Nd6GYobWCrIA9-ZsjDjiH7LaAKXNopEn94761M,1509
+hat/event/common/common.py,sha256=SCQpgrroxmKTlD1LeidnEKXJ_71oxGOgvnaiWkCnYh0,4452
+hat/event/common/collection/__init__.py,sha256=9C3GQY2Z1k0JKl_ENpl_ZI1gjtbiTfoFywf45dKZ72w,317
+hat/event/server/engine.py,sha256=yqq9CeF1Eg0mUIY5GF5cSEGoksMss26wm9ZCb7Zu434,6950
+hat/event/common/collection/common.py,sha256=PRfaVHCNy7h4SvwTX7gY6FO3GbsJxnMZWsNm2tpWPM0,595
+hat/event/common/subscription/pysubscription.py,sha256=qh-PH3R7EcuEIi6zzKk_ytw3rX3c9uGFEgH4eupjqhI,2666
+hat/event/common/sbs_repo.json,sha256=EhJTaqxbhgPoYVmVyXGSRQUWEyIj_BcRDGMklNM8RYI,9464
+hat/event/backends/lmdb/__init__.py,sha256=u9mVnSROi2vpjvkWHhslUA5U7j6bpunVJcabdc4Dj28,301
+hat/event/backends/lmdb/manager/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+hat/event/eventer/server.py,sha256=IIUZcpGP93Lr43pJIa5DXVSOfCP7mXLU4MucXrSTJas,8711
+hat/event/backends/lmdb/common.py,sha256=x8Kc3RciJ5ulwifC4GfPEDj8JlPlRQsVPYbEsvyIw9k,8880
+hat/event/backends/lmdb/manager/copy.py,sha256=WBpcfD9ywiW7TrsF-AFDBDl2-BD2vg4SiVL3ycftR1w,3475
+hat/event/backends/memory.py,sha256=YHXHD7ruNL8a1RGZCM_CQDPzfniB1zgDQA3UuLaZHtw,5400
+hat/event/backends/dummy.py,sha256=b8K4dM0jpllR4XFEw19fURSfWiWkHXmLyPyTc5X-fks,1304
+hat/event/backends/lmdb/latestdb.py,sha256=PxKPm4bhRBmqGMFObGWwssj-25RJjKhSB3Dv5v4Ma8w,3926
+hat/event/backends/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 hat/event/server/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+hat/event/eventer/__init__.py,sha256=mSRDlpIqpVKml8_SbC6s1EI168OBb4dMgB7sz0uJ2Ks,924
+hat/event/common/collection/list.py,sha256=6_-rZFlMQnQ9KWAKXiMFu-5NBsYsIcyYChiareBTZVk,580
+hat/event/backends/lmdb/convert/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+hat/event/__init__.py,sha256=nIQrqyPevwuBP-5p0MrmV1iLnevMXoz8t5moAAl21HI,47
+hat/event/common/subscription/csubscription.py,sha256=0a2Ict21wXFYr6vyLCQfVAZyjV3v2da3oxqx_qqzN-0,1526
+hat/event/common/encoder.py,sha256=zJzf_qhcod7fych42L66YjLiMIkI-5an_PrVAaxh_r0,10715
+hat/event/server/eventer_client.py,sha256=UKLgPTxwdUA_tK-bEe27qCpKoAmVBGmSWQUmHB6Jlbs,4599
+hat/event/server/runner.py,sha256=Nf8nymd0SRa55buLzPi4CslDKqFwA0W1tDwGg_pLOFQ,11486
+hat/event/common/backend.py,sha256=5oPzQU9zHxFoSkpT1rqYIElOyItdD6gh_UZrkIiChwE,2493
+hat/event/backends/lmdb/backend.py,sha256=I88kJY2z9BQgOr8zPNni1IEX5mjMfEZccdMY9adur_E,10502
+hat/event/backends/lmdb/conditions.py,sha256=XQHFRn1fdFVTK6AphfaNgwmVLCDPf7hefjO57yC4mvg,2720
+hat/event/backends/lmdb/manager/__main__.py,sha256=RuvZt3NE2bZ4kxSOBZX136j_ZSERxkjGlMwrniNelLk,159
+hat/event/backends/lmdb/convert/convert_v06_to_v07.py,sha256=It6sRnB7zXzTQWZ3jnECgnTyVS4NW2QhBbgvwAhRkas,8373
+hat/event/backends/lmdb/manager/common.py,sha256=y6UkyobFGTQ1yh7OElOzT8xYyupM8hpiWaL_Km3Ehs4,1488
+hat/event/backends/lmdb/convert/version.py,sha256=ZLEUSXhcCStdclBzm4Ljn9KPngME1l4kllwOJXzxS8E,1682
+hat/event/common/subscription/__init__.py,sha256=FSY4gMwqiNPlBO_-d_D9m--oAG80HZznXpPOQWBc8Es,600
+hat/event/eventer/client.py,sha256=j4h7usssTTfJx1q34XXqok3jKYXUX4Gp1jGo8fN4ekY,8780
+hat/event/backends/lmdb/convert/v09.py,sha256=QDWdV-O5-pGZAf3JBG5veYYQyZw23pRR2SbPDDvxSMo,1346
+hat/event/component.py,sha256=Hg095u5PE5na5aMsmgEtr2GKEgYdSzPtTpZBAPtPiB4,9947
+hat/event/backends/lmdb/timeseriesdb.py,sha256=lF4xslHRuQABTDw7SAoJntfHW6a98eSiY68rVT5sP5c,17350
+hat/event/common/json_schema_repo.json,sha256=f4OCAPHRREd7vRneP-4FnN_dKpANrN7pEGP9tHDR5-4,5705
+hat/event/backends/lmdb/manager/query.py,sha256=NOjqDK0ucfUzgshV8A7ytuV6pQNSQDDPYsSS8wroeFc,7747
 hat/event/server/__main__.py,sha256=1Wul_omw9id6MncwkZAn-iFK-FaEirfTJ2JcxZzrcig,138
-hat/event/server/common.py,sha256=qhamdFWDUKkdOJ1tyE1DnzWQ2UkRcigTT_Xx0MhBNFY,5613
-hat/event/server/engine.py,sha256=Qk5yDnaPgc5RkNV9ZaGkJgYKFt-GV7hMimATx9Kc0Yg,6751
-hat/event/server/eventer_server.py,sha256=eEPIi3VSms27a2F4IYBmbDkiOUyhC5dEI_Q-zqutRkE,2095
-hat/event/server/main.py,sha256=W_4EJ-vnjeEKtxurRdmsh7grH0eRttqOxBW50J6A6LQ,2330
-hat/event/server/mariner_server.py,sha256=0ymFFcWA5qBY-T0x9IakPafpyGNrPvlDvAcu896Hoxo,3742
-hat/event/server/runner.py,sha256=t7A2LAN3WK1Ae0eIoJDGBFOu6x24Lb8qe7iLWZsxV0I,10721
-hat/event/server/syncer_client.py,sha256=cW72GmzEJMC4btf0-C5TABPJ4CRhi7eMnSLyL5Pe0L0,7577
-hat/event/server/syncer_server.py,sha256=EK-8NqYP6e8Jf9-CRxGdTjsSp7liT9GmcuKATJe1A20,2041
-hat/event/server/backends/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-hat/event/server/backends/dummy.py,sha256=hIgHgWCd0g7beQn4OycknCzWXvf5W3gkrh0E3eD8_4w,2156
-hat/event/server/backends/memory.py,sha256=dvhst047vAT3YLvciz9jheNqSLMICf8zgfQd4AhUN1c,5647
-hat/event/server/backends/lmdb/__init__.py,sha256=KUMfcLQw7ysMtj0MAYSzVIzGs04TXxt0N_JHelYHrn0,241
-hat/event/server/backends/lmdb/backend.py,sha256=I4SXfilwfEUXmr_tzhh6zDYKXnfmY7b0bvTGgXOoXzA,9528
-hat/event/server/backends/lmdb/common.py,sha256=7klgg5RXttPi1rLlBPJ5eehiED1aq6DvaB11qBYSkd4,2498
-hat/event/server/backends/lmdb/conditions.py,sha256=en-fcQEOwRyVjbVdhz3MuspYXzj6W2GgwH2CMlzL77Q,2779
-hat/event/server/backends/lmdb/encoder.py,sha256=CP_XBXDNcaNie1-DVVaoY99QKMH_yCJNAGrnCMc28MY,6427
-hat/event/server/backends/lmdb/environment.py,sha256=pg0ypzCBtDbAQNWEQASORRWGgCdjsUQsRYpHhxqIqJg,1594
-hat/event/server/backends/lmdb/latestdb.py,sha256=3OVPDxuJFwaaP4oC81eQVbhuddmkwrdaX8xhTkCCz58,4458
-hat/event/server/backends/lmdb/ordereddb.py,sha256=0xpJv9-knumxWfxiuQbMa67WGwIuZ6iUwtY03FngKj0,17619
-hat/event/server/backends/lmdb/refdb.py,sha256=kpcS9Brg_8Cv5yLZ43em9ghukG8VLoVoReKc2_jV-do,4947
-hat/event/server/backends/lmdb/systemdb.py,sha256=-VJNK_shfZHS3BhP4L3kbWIZ-lqUOSYOBnDuzuEOnYs,2158
-hat/event/server/backends/lmdb/convert/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-hat/event/server/backends/lmdb/convert/convert_v06_to_v07.py,sha256=eVz8lZG2MCAyx0CWgaVqwpTgNHqKJQK3sgCayYCS4Mo,8667
-hat/event/server/backends/lmdb/convert/v06.py,sha256=b0mAz0n8qOBO5XhBsveZ_uKSERhqBuhV0yTbZEclTr4,6153
-hat/event/server/backends/lmdb/convert/v07.py,sha256=0mHisr0AZUV-rKlC0uxKijV-0z87TQHEHUvwEcHq3zA,2444
-hat/event/server/backends/lmdb/manager/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-hat/event/server/backends/lmdb/manager/__main__.py,sha256=borAj1oZKFXG-BJLGbmXgdJU0-YsVgHND2Dr6Wtx4zA,166
-hat/event/server/backends/lmdb/manager/common.py,sha256=9mewEno7MiiwDCVmrPOV4xvEJAJkK2Mjm-A93e_Fzc4,1254
-hat/event/server/backends/lmdb/manager/copy.py,sha256=4gLKXi0gf6sdiDwYxHmBO4MST5FCA3xbEzxIelGYIug,1668
-hat/event/server/backends/lmdb/manager/main.py,sha256=aeq1Le7KSjFnlQz5Jlow2PX-KxqVR0UqP0O_NGLLrZo,951
-hat/event/server/backends/lmdb/manager/query.py,sha256=IY8myArfETVrsWhLT8JbwVGf-AwR8neHmA9a7bpgeng,5625
-hat/event/syncer/__init__.py,sha256=cyTxvmwLEVwykxDCWXJ9Z_yf8NFciWP7V5JU0vaNatU,618
-hat/event/syncer/client.py,sha256=p0SXgn8Sf5foWwI9NJbz0czhKj4jM-GWLrFyM2MOAKs,3787
-hat/event/syncer/common.py,sha256=VHdjvlUa52GkEdT6VshBDN5GZ4f7Qip4LuVXyayn9OA,1957
-hat/event/syncer/server.py,sha256=m7qWBQninY2oWm4toiwVn8GZeI0fhWni0cfLHGwDcqA,13536
-hat_event-0.8.9.dist-info/LICENSE,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
-hat_event-0.8.9.dist-info/METADATA,sha256=lmUSpBnXqS1likGPSOuzN3zY43Regqp4_g6pKsJXNu0,3080
-hat_event-0.8.9.dist-info/WHEEL,sha256=94Gg7OqAJVxvCyv0ZC9_MZaW4vHMFyhZ9QwqbepeqpQ,127
-hat_event-0.8.9.dist-info/entry_points.txt,sha256=VRGFweYHw7BjDWzrnHHSZ1MMIkXPnT3o-QLbQPhLE3o,138
-hat_event-0.8.9.dist-info/top_level.txt,sha256=3RuRoRsaXQZNKwr3T2RE9XepBRTk4YpnXUbMiH5nes8,4
-hat_event-0.8.9.dist-info/RECORD,,
+hat/event/eventer/common.py,sha256=-pA86GhbY-oDyHypRXmJmaotOflxyMH-TykIy1r48cM,737
+hat/event/server/eventer_server.py,sha256=9AhWfhysyczrAGI8jpvUulJqvgZzs2LSVkYmE73CJCM,3365
+hat/event/backends/lmdb/convert/__main__.py,sha256=q0OhzxbrgrDOwngAlACg9ZiiDBNiR4YT9e1jbs8E0pE,159
+hat/event/server/main.py,sha256=x0SnEvf0ri-HhQoyak2UaZy0QVILC7nT-iyJ3IdnMac,2207
+hat/event/common/module.py,sha256=MGlZKsxtmG9khYaPaGsS0jRgzwvJCm6Plh1ugy_xVjY,3184
+hat/event/common/matches.py,sha256=uV9juK5d2hNzFroFrrEbruVY0E30tTW9hpGoeTu3w_s,1556
+hat/event/backends/lmdb/systemdb.py,sha256=y5ZmrRryQMtBVKils7ImGkqMRcydC03MyEoQB_TNH_8,3227
+hat/event/common/subscription/_csubscription.abi3.pyd,sha256=5El4sW9yRzp6nLsu5YevIXUkvmVRlHuimErBVMsI-0I,116911
+hat/event/backends/lmdb/convert/main.py,sha256=oELuQcbjahVG6_Cln3XCfAR8u2TBO4EsOgvf_9P_awU,2407
+hat/event/backends/lmdb/refdb.py,sha256=NDj7e7YLSCwkD_n6PlHePWV9ebKf_iI_oBDGv49_2rU,7858
+hat/event/backends/lmdb/convert/v06.py,sha256=LZfRo7nrHYCHka_KvZAMJYJl00m4fWivFzrXnRnARcc,5116
+hat/event/backends/lmdb/manager/main.py,sha256=tMpK12a0LvPgQX0XjZYjKQCB5x6jU7-t10hpeOeo8I8,752
+hat/event/common/collection/tree.py,sha256=D6-qnmH2Psc-XVu6g5rhn5k-p5BiufWRzdSRKKZkp5Y,1570
+hat/event/common/subscription/common.py,sha256=lmjmhcs2LRewfKJVLgo7Bw5iZxAUT1xzukD2e7o50d8,4265
+hat/event/backends/lmdb/convert/convert_v07_to_v09.py,sha256=zicz9lMnjZEUp2xvRbr9ped0DMuGcREq3JfIzkre3Vs,6751
+hat/event/backends/lmdb/environment.py,sha256=b8DCflvMfjSVZo-F2kgfdZyYgcBFb1uEtqFKoMLfLIo,3194
+hat/event/common/__init__.py,sha256=M9kY8jS-5KJJboGmMXFfBPsLJ2KFeCPZEPUgxNQolXQ,5782
+hat/event/backends/lmdb/convert/v07.py,sha256=y4N1hQzQ4rIYs3vw6jzpQrrwZw_1gun6aFr6x1v1-zo,12740
+hat_event-0.9.0.dist-info/LICENSE,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
+hat_event-0.9.0.dist-info/entry_points.txt,sha256=73FNDDvZYfxZaclIlVN4iSpjBY7zpz6SOIpopR-L2a8,198
+hat_event-0.9.0.dist-info/METADATA,sha256=WBPE0zqoDwJY9VQB1TjXlAgskRuz089RUjyLcGs8lx4,2872
+hat_event-0.9.0.dist-info/WHEEL,sha256=N23xFUKRD3f7joY5siT6d5UtqlAEXInNx8BgrJBDFjw,137
+hat_event-0.9.0.dist-info/RECORD,,
```

