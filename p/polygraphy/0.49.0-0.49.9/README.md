# Comparing `tmp/polygraphy-0.49.0-py2.py3-none-any.whl.zip` & `tmp/polygraphy-0.49.9-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,159 +1,168 @@
-Zip file size: 327879 bytes, number of entries: 157
--rw-rw-r--  2.0 unx       49 b- defN 23-Jul-29 01:25 polygraphy/__init__.py
--rw-rw-r--  2.0 unx     2438 b- defN 23-Jun-13 19:50 polygraphy/config.py
+Zip file size: 346910 bytes, number of entries: 166
+-rw-rw-r--  2.0 unx       49 b- defN 24-Mar-19 19:47 polygraphy/__init__.py
+-rw-rw-r--  2.0 unx     2438 b- defN 23-Nov-16 18:51 polygraphy/config.py
 -rw-rw-r--  2.0 unx     1079 b- defN 23-Mar-28 16:37 polygraphy/constants.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Feb-19 22:21 polygraphy/backend/__init__.py
 -rw-rw-r--  2.0 unx       90 b- defN 21-Feb-19 22:21 polygraphy/backend/base/__init__.py
 -rw-rw-r--  2.0 unx     1377 b- defN 23-Mar-28 16:37 polygraphy/backend/base/loader.py
--rw-rw-r--  2.0 unx     9847 b- defN 23-Jun-14 20:36 polygraphy/backend/base/runner.py
--rw-rw-r--  2.0 unx     1990 b- defN 23-Jun-13 19:50 polygraphy/backend/base/util.py
+-rw-rw-r--  2.0 unx     9847 b- defN 23-Nov-16 18:51 polygraphy/backend/base/runner.py
+-rw-rw-r--  2.0 unx     1990 b- defN 23-Nov-16 18:51 polygraphy/backend/base/util.py
 -rw-rw-r--  2.0 unx       47 b- defN 21-Feb-19 22:21 polygraphy/backend/common/__init__.py
 -rw-rw-r--  2.0 unx     2925 b- defN 23-Mar-28 16:37 polygraphy/backend/common/loader.py
 -rw-rw-r--  2.0 unx       45 b- defN 21-Apr-29 12:43 polygraphy/backend/onnx/__init__.py
--rw-rw-r--  2.0 unx    38138 b- defN 23-Jul-29 01:25 polygraphy/backend/onnx/loader.py
--rw-rw-r--  2.0 unx    17261 b- defN 23-Jun-14 20:36 polygraphy/backend/onnx/util.py
+-rw-rw-r--  2.0 unx    38293 b- defN 23-Nov-29 17:37 polygraphy/backend/onnx/loader.py
+-rw-rw-r--  2.0 unx    17261 b- defN 23-Nov-16 18:51 polygraphy/backend/onnx/util.py
 -rw-rw-r--  2.0 unx       94 b- defN 23-May-04 21:16 polygraphy/backend/onnxrt/__init__.py
 -rw-rw-r--  2.0 unx     2796 b- defN 23-Mar-28 16:37 polygraphy/backend/onnxrt/loader.py
--rw-rw-r--  2.0 unx     3368 b- defN 23-Jun-13 19:50 polygraphy/backend/onnxrt/runner.py
+-rw-rw-r--  2.0 unx     3431 b- defN 23-Nov-29 17:51 polygraphy/backend/onnxrt/runner.py
 -rw-rw-r--  2.0 unx      664 b- defN 22-May-16 13:51 polygraphy/backend/pluginref/__init__.py
 -rw-rw-r--  2.0 unx     3471 b- defN 23-Mar-28 16:37 polygraphy/backend/pluginref/references.py
 -rw-rw-r--  2.0 unx     2712 b- defN 23-Mar-28 16:37 polygraphy/backend/pluginref/runner.py
 -rw-rw-r--  2.0 unx       44 b- defN 21-Feb-19 22:21 polygraphy/backend/pyt/__init__.py
--rw-rw-r--  2.0 unx     2830 b- defN 23-Jun-13 19:50 polygraphy/backend/pyt/runner.py
--rw-rw-r--  2.0 unx     1154 b- defN 22-Sep-01 16:06 polygraphy/backend/tf/__init__.py
+-rw-rw-r--  2.0 unx     2830 b- defN 23-Nov-16 18:51 polygraphy/backend/pyt/runner.py
+-rw-rw-r--  2.0 unx     1172 b- defN 23-Nov-29 17:37 polygraphy/backend/tf/__init__.py
 -rw-rw-r--  2.0 unx    18232 b- defN 23-Mar-28 16:37 polygraphy/backend/tf/loader.py
 -rw-rw-r--  2.0 unx     3731 b- defN 23-Mar-28 16:37 polygraphy/backend/tf/runner.py
--rw-rw-r--  2.0 unx     6966 b- defN 23-Jun-13 19:50 polygraphy/backend/tf/util.py
--rw-rw-r--  2.0 unx      323 b- defN 23-Jun-13 19:50 polygraphy/backend/trt/__init__.py
--rw-rw-r--  2.0 unx    14550 b- defN 23-Jun-13 19:50 polygraphy/backend/trt/algorithm_selector.py
--rw-rw-r--  2.0 unx    10266 b- defN 23-Jun-13 19:50 polygraphy/backend/trt/calibrator.py
--rw-rw-r--  2.0 unx    23294 b- defN 23-Jul-29 01:25 polygraphy/backend/trt/config.py
--rw-rw-r--  2.0 unx    31818 b- defN 23-Jun-23 17:56 polygraphy/backend/trt/loader.py
+-rw-rw-r--  2.0 unx     6966 b- defN 23-Nov-16 18:51 polygraphy/backend/tf/util.py
+-rw-rw-r--  2.0 unx      323 b- defN 23-Nov-16 18:51 polygraphy/backend/trt/__init__.py
+-rw-rw-r--  2.0 unx    14280 b- defN 23-Nov-29 17:37 polygraphy/backend/trt/algorithm_selector.py
+-rw-rw-r--  2.0 unx    10266 b- defN 23-Nov-16 18:51 polygraphy/backend/trt/calibrator.py
+-rw-rw-r--  2.0 unx    25025 b- defN 24-Feb-05 17:57 polygraphy/backend/trt/config.py
+-rw-rw-r--  2.0 unx    34597 b- defN 24-Feb-16 19:33 polygraphy/backend/trt/loader.py
 -rw-rw-r--  2.0 unx     8187 b- defN 23-Mar-28 16:37 polygraphy/backend/trt/profile.py
--rw-rw-r--  2.0 unx    14820 b- defN 23-Jun-23 17:56 polygraphy/backend/trt/runner.py
--rw-rw-r--  2.0 unx    31210 b- defN 23-Jul-29 01:25 polygraphy/backend/trt/util.py
+-rw-rw-r--  2.0 unx    21627 b- defN 24-Mar-19 19:47 polygraphy/backend/trt/runner.py
+-rw-rw-r--  2.0 unx    33219 b- defN 23-Dec-28 18:17 polygraphy/backend/trt/util.py
 -rw-rw-r--  2.0 unx       74 b- defN 23-May-09 17:07 polygraphy/common/__init__.py
 -rw-rw-r--  2.0 unx     6173 b- defN 23-Mar-28 16:37 polygraphy/common/interface.py
--rw-rw-r--  2.0 unx     6465 b- defN 23-Jun-14 16:51 polygraphy/common/struct.py
+-rw-rw-r--  2.0 unx     6504 b- defN 23-Nov-29 17:37 polygraphy/common/struct.py
 -rw-rw-r--  2.0 unx      230 b- defN 21-Feb-19 22:21 polygraphy/comparator/__init__.py
--rw-rw-r--  2.0 unx    18853 b- defN 23-Jun-13 19:50 polygraphy/comparator/comparator.py
--rw-rw-r--  2.0 unx    33310 b- defN 23-Jul-29 01:25 polygraphy/comparator/compare.py
--rw-rw-r--  2.0 unx    20292 b- defN 23-Jul-05 18:52 polygraphy/comparator/data_loader.py
--rw-rw-r--  2.0 unx     2531 b- defN 23-Jun-13 19:50 polygraphy/comparator/postprocess.py
--rw-rw-r--  2.0 unx    13689 b- defN 23-Jun-21 19:30 polygraphy/comparator/struct.py
--rw-rw-r--  2.0 unx    13976 b- defN 23-Jul-29 01:25 polygraphy/comparator/util.py
+-rw-rw-r--  2.0 unx    18853 b- defN 23-Nov-16 18:51 polygraphy/comparator/comparator.py
+-rw-rw-r--  2.0 unx    35137 b- defN 24-Mar-19 19:47 polygraphy/comparator/compare.py
+-rw-rw-r--  2.0 unx    24486 b- defN 23-Nov-30 18:08 polygraphy/comparator/data_loader.py
+-rw-rw-r--  2.0 unx     2531 b- defN 23-Nov-16 18:51 polygraphy/comparator/postprocess.py
+-rw-rw-r--  2.0 unx    13689 b- defN 23-Nov-16 18:51 polygraphy/comparator/struct.py
+-rw-rw-r--  2.0 unx    13976 b- defN 23-Nov-16 18:51 polygraphy/comparator/util.py
 -rw-rw-r--  2.0 unx       35 b- defN 21-May-17 13:51 polygraphy/cuda/__init__.py
--rw-rw-r--  2.0 unx    16567 b- defN 23-Jun-14 20:36 polygraphy/cuda/cuda.py
--rw-rw-r--  2.0 unx      246 b- defN 23-Jun-13 19:50 polygraphy/datatype/__init__.py
--rw-rw-r--  2.0 unx     9352 b- defN 23-Jul-05 18:52 polygraphy/datatype/datatype.py
--rw-rw-r--  2.0 unx     2127 b- defN 23-Jun-14 20:36 polygraphy/datatype/numpy.py
--rw-rw-r--  2.0 unx     2442 b- defN 23-Jul-05 18:52 polygraphy/datatype/onnx.py
--rw-rw-r--  2.0 unx     2237 b- defN 23-Jun-14 20:36 polygraphy/datatype/onnxrt.py
--rw-rw-r--  2.0 unx     2120 b- defN 23-Jun-14 20:36 polygraphy/datatype/tensorrt.py
--rw-rw-r--  2.0 unx     1901 b- defN 23-Jun-13 19:50 polygraphy/datatype/torch.py
+-rw-rw-r--  2.0 unx    16567 b- defN 23-Nov-16 18:51 polygraphy/cuda/cuda.py
+-rw-rw-r--  2.0 unx      246 b- defN 23-Nov-16 18:51 polygraphy/datatype/__init__.py
+-rw-rw-r--  2.0 unx     9555 b- defN 23-Nov-29 17:37 polygraphy/datatype/datatype.py
+-rw-rw-r--  2.0 unx     2150 b- defN 23-Nov-29 17:37 polygraphy/datatype/numpy.py
+-rw-rw-r--  2.0 unx     2470 b- defN 23-Nov-29 17:37 polygraphy/datatype/onnx.py
+-rw-rw-r--  2.0 unx     2237 b- defN 23-Nov-16 18:51 polygraphy/datatype/onnxrt.py
+-rw-rw-r--  2.0 unx     2213 b- defN 23-Nov-29 17:37 polygraphy/datatype/tensorrt.py
+-rw-rw-r--  2.0 unx     1930 b- defN 23-Nov-29 17:37 polygraphy/datatype/torch.py
 -rw-rw-r--  2.0 unx       45 b- defN 21-May-17 13:51 polygraphy/exception/__init__.py
--rw-rw-r--  2.0 unx     1681 b- defN 23-Jul-05 18:52 polygraphy/exception/exception.py
+-rw-rw-r--  2.0 unx     1681 b- defN 23-Nov-16 18:51 polygraphy/exception/exception.py
 -rw-rw-r--  2.0 unx       35 b- defN 21-May-17 13:51 polygraphy/func/__init__.py
 -rw-rw-r--  2.0 unx     6479 b- defN 23-Mar-28 16:37 polygraphy/func/func.py
 -rw-rw-r--  2.0 unx       36 b- defN 21-May-17 13:51 polygraphy/json/__init__.py
--rw-rw-r--  2.0 unx    13356 b- defN 23-Jul-29 01:25 polygraphy/json/serde.py
+-rw-rw-r--  2.0 unx    13440 b- defN 23-Nov-29 17:37 polygraphy/json/serde.py
 -rw-rw-r--  2.0 unx       55 b- defN 21-Apr-29 12:43 polygraphy/logger/__init__.py
--rw-rw-r--  2.0 unx    24130 b- defN 23-Jul-05 18:52 polygraphy/logger/logger.py
+-rw-rw-r--  2.0 unx    24130 b- defN 23-Nov-16 18:51 polygraphy/logger/logger.py
 -rw-rw-r--  2.0 unx      116 b- defN 21-Apr-29 12:43 polygraphy/mod/__init__.py
 -rw-rw-r--  2.0 unx    12827 b- defN 23-May-09 23:40 polygraphy/mod/exporter.py
--rw-rw-r--  2.0 unx    11004 b- defN 23-Jun-13 19:50 polygraphy/mod/importer.py
--rw-rw-r--  2.0 unx     1418 b- defN 23-Jul-29 01:25 polygraphy/mod/util.py
+-rw-rw-r--  2.0 unx    12917 b- defN 23-Dec-28 18:17 polygraphy/mod/importer.py
+-rw-rw-r--  2.0 unx     1418 b- defN 23-Nov-16 18:51 polygraphy/mod/util.py
 -rw-rw-r--  2.0 unx      125 b- defN 22-May-24 19:25 polygraphy/tools/__init__.py
--rw-rw-r--  2.0 unx     2534 b- defN 22-Oct-06 23:14 polygraphy/tools/_main.py
--rw-rw-r--  2.0 unx     2504 b- defN 23-Jul-29 01:25 polygraphy/tools/registry.py
+-rw-rw-r--  2.0 unx     2650 b- defN 24-Mar-19 19:47 polygraphy/tools/_main.py
+-rw-rw-r--  2.0 unx     2559 b- defN 24-Feb-05 17:57 polygraphy/tools/registry.py
 -rw-rw-r--  2.0 unx    15742 b- defN 23-Mar-29 18:21 polygraphy/tools/script.py
--rw-rw-r--  2.0 unx    13359 b- defN 23-Jul-29 01:25 polygraphy/tools/sparse.py
+-rw-rw-r--  2.0 unx    13789 b- defN 23-Dec-28 18:17 polygraphy/tools/sparse.py
 -rw-rw-r--  2.0 unx     1903 b- defN 23-Mar-28 16:37 polygraphy/tools/util.py
 -rw-rw-r--  2.0 unx      789 b- defN 23-Mar-09 23:53 polygraphy/tools/args/__init__.py
 -rw-rw-r--  2.0 unx     7412 b- defN 23-Mar-28 20:04 polygraphy/tools/args/base.py
 -rw-rw-r--  2.0 unx     9278 b- defN 23-Mar-28 20:04 polygraphy/tools/args/model.py
--rw-rw-r--  2.0 unx      920 b- defN 23-Jun-13 19:50 polygraphy/tools/args/backend/__init__.py
+-rw-rw-r--  2.0 unx      920 b- defN 23-Nov-16 18:51 polygraphy/tools/args/backend/__init__.py
 -rw-rw-r--  2.0 unx     3516 b- defN 23-Mar-28 16:37 polygraphy/tools/args/backend/runner_select.py
 -rw-rw-r--  2.0 unx      669 b- defN 22-May-24 19:25 polygraphy/tools/args/backend/onnx/__init__.py
--rw-rw-r--  2.0 unx    29611 b- defN 23-Jun-13 19:50 polygraphy/tools/args/backend/onnx/loader.py
+-rw-rw-r--  2.0 unx    29611 b- defN 23-Nov-16 18:51 polygraphy/tools/args/backend/onnx/loader.py
 -rw-rw-r--  2.0 unx      729 b- defN 22-May-24 19:25 polygraphy/tools/args/backend/onnxrt/__init__.py
--rw-rw-r--  2.0 unx     3172 b- defN 23-Jul-29 01:25 polygraphy/tools/args/backend/onnxrt/loader.py
+-rw-rw-r--  2.0 unx     3172 b- defN 23-Nov-16 18:51 polygraphy/tools/args/backend/onnxrt/loader.py
 -rw-rw-r--  2.0 unx     1395 b- defN 23-Mar-28 16:37 polygraphy/tools/args/backend/onnxrt/runner.py
 -rw-rw-r--  2.0 unx      674 b- defN 22-May-24 19:25 polygraphy/tools/args/backend/pluginref/__init__.py
 -rw-rw-r--  2.0 unx     1629 b- defN 23-Mar-28 16:37 polygraphy/tools/args/backend/pluginref/runner.py
 -rw-rw-r--  2.0 unx      775 b- defN 22-May-24 19:25 polygraphy/tools/args/backend/tf/__init__.py
 -rw-rw-r--  2.0 unx     2771 b- defN 23-Mar-28 16:37 polygraphy/tools/args/backend/tf/config.py
--rw-rw-r--  2.0 unx    10069 b- defN 23-Jun-13 19:50 polygraphy/tools/args/backend/tf/loader.py
+-rw-rw-r--  2.0 unx    10069 b- defN 23-Nov-16 18:51 polygraphy/tools/args/backend/tf/loader.py
 -rw-rw-r--  2.0 unx     2432 b- defN 23-Mar-28 16:37 polygraphy/tools/args/backend/tf/runner.py
 -rw-rw-r--  2.0 unx      778 b- defN 22-May-24 19:25 polygraphy/tools/args/backend/trt/__init__.py
--rw-rw-r--  2.0 unx    35966 b- defN 23-Jul-29 01:25 polygraphy/tools/args/backend/trt/config.py
+-rw-rw-r--  2.0 unx    37698 b- defN 24-Feb-05 17:57 polygraphy/tools/args/backend/trt/config.py
 -rw-rw-r--  2.0 unx     1181 b- defN 23-Mar-28 20:04 polygraphy/tools/args/backend/trt/helper.py
--rw-rw-r--  2.0 unx    27106 b- defN 23-Jun-23 17:56 polygraphy/tools/args/backend/trt/loader.py
--rw-rw-r--  2.0 unx     2126 b- defN 23-Mar-28 20:04 polygraphy/tools/args/backend/trt/runner.py
+-rw-rw-r--  2.0 unx    28365 b- defN 24-Feb-16 19:33 polygraphy/tools/args/backend/trt/loader.py
+-rw-rw-r--  2.0 unx     4428 b- defN 24-Mar-19 19:47 polygraphy/tools/args/backend/trt/runner.py
 -rw-rw-r--  2.0 unx      844 b- defN 22-May-24 19:25 polygraphy/tools/args/comparator/__init__.py
 -rw-rw-r--  2.0 unx    11098 b- defN 23-Mar-28 16:37 polygraphy/tools/args/comparator/comparator.py
--rw-rw-r--  2.0 unx    10757 b- defN 23-Jul-29 01:25 polygraphy/tools/args/comparator/compare.py
--rw-rw-r--  2.0 unx    12332 b- defN 23-Mar-28 16:37 polygraphy/tools/args/comparator/data_loader.py
+-rw-rw-r--  2.0 unx    10757 b- defN 23-Nov-16 18:51 polygraphy/tools/args/comparator/compare.py
+-rw-rw-r--  2.0 unx    12943 b- defN 23-Nov-29 17:37 polygraphy/tools/args/comparator/data_loader.py
 -rw-rw-r--  2.0 unx     4070 b- defN 23-Mar-28 16:37 polygraphy/tools/args/comparator/postprocess.py
 -rw-rw-r--  2.0 unx      663 b- defN 22-May-24 19:25 polygraphy/tools/args/logger/__init__.py
 -rw-rw-r--  2.0 unx     6675 b- defN 23-Mar-28 16:37 polygraphy/tools/args/logger/logger.py
 -rw-rw-r--  2.0 unx       46 b- defN 21-Feb-19 22:21 polygraphy/tools/args/util/__init__.py
--rw-rw-r--  2.0 unx    12719 b- defN 23-Jun-13 19:50 polygraphy/tools/args/util/util.py
+-rw-rw-r--  2.0 unx    12719 b- defN 23-Nov-16 18:51 polygraphy/tools/args/util/util.py
 -rw-rw-r--  2.0 unx       41 b- defN 21-Feb-23 16:54 polygraphy/tools/base/__init__.py
 -rw-rw-r--  2.0 unx     7257 b- defN 23-Mar-28 16:37 polygraphy/tools/base/tool.py
--rw-rw-r--  2.0 unx       47 b- defN 23-Jul-29 01:25 polygraphy/tools/check/__init__.py
--rw-rw-r--  2.0 unx     1020 b- defN 23-Jul-29 01:25 polygraphy/tools/check/check.py
--rw-rw-r--  2.0 unx       53 b- defN 23-Jul-29 01:25 polygraphy/tools/check/subtool/__init__.py
--rw-rw-r--  2.0 unx    41086 b- defN 23-Jul-29 01:25 polygraphy/tools/check/subtool/lint.py
+-rw-rw-r--  2.0 unx       47 b- defN 23-Nov-16 18:51 polygraphy/tools/check/__init__.py
+-rw-rw-r--  2.0 unx     1020 b- defN 23-Nov-16 18:51 polygraphy/tools/check/check.py
+-rw-rw-r--  2.0 unx       53 b- defN 23-Nov-16 18:51 polygraphy/tools/check/subtool/__init__.py
+-rw-rw-r--  2.0 unx    41313 b- defN 24-Feb-05 17:57 polygraphy/tools/check/subtool/lint.py
 -rw-rw-r--  2.0 unx       53 b- defN 21-Jan-20 15:55 polygraphy/tools/convert/__init__.py
 -rw-rw-r--  2.0 unx     4062 b- defN 23-Mar-28 20:04 polygraphy/tools/convert/convert.py
 -rw-rw-r--  2.0 unx       44 b- defN 22-May-16 13:51 polygraphy/tools/data/__init__.py
 -rw-rw-r--  2.0 unx     1062 b- defN 23-Mar-28 16:37 polygraphy/tools/data/data.py
 -rw-rw-r--  2.0 unx       59 b- defN 22-May-16 13:51 polygraphy/tools/data/subtool/__init__.py
 -rw-rw-r--  2.0 unx     3025 b- defN 23-Mar-28 16:37 polygraphy/tools/data/subtool/to_input.py
 -rw-rw-r--  2.0 unx       47 b- defN 21-Feb-11 22:55 polygraphy/tools/debug/__init__.py
--rw-rw-r--  2.0 unx     3480 b- defN 23-Jun-13 19:50 polygraphy/tools/debug/debug.py
+-rw-rw-r--  2.0 unx     3480 b- defN 23-Nov-16 18:51 polygraphy/tools/debug/debug.py
 -rw-rw-r--  2.0 unx      232 b- defN 22-Sep-22 23:48 polygraphy/tools/debug/subtool/__init__.py
--rw-rw-r--  2.0 unx     5123 b- defN 23-Jun-13 19:50 polygraphy/tools/debug/subtool/base.py
+-rw-rw-r--  2.0 unx     5123 b- defN 23-Nov-16 18:51 polygraphy/tools/debug/subtool/base.py
 -rw-rw-r--  2.0 unx     2072 b- defN 23-Mar-28 16:37 polygraphy/tools/debug/subtool/build.py
 -rw-rw-r--  2.0 unx    31725 b- defN 23-Mar-28 16:37 polygraphy/tools/debug/subtool/iterative_debug_args.py
--rw-rw-r--  2.0 unx     9283 b- defN 23-Jun-13 19:50 polygraphy/tools/debug/subtool/precision.py
--rw-rw-r--  2.0 unx    24308 b- defN 23-Jun-14 20:36 polygraphy/tools/debug/subtool/reduce.py
+-rw-rw-r--  2.0 unx     9283 b- defN 23-Nov-16 18:51 polygraphy/tools/debug/subtool/precision.py
+-rw-rw-r--  2.0 unx    24308 b- defN 23-Nov-16 18:51 polygraphy/tools/debug/subtool/reduce.py
 -rw-rw-r--  2.0 unx     1378 b- defN 23-Mar-28 16:37 polygraphy/tools/debug/subtool/repeat.py
 -rw-rw-r--  2.0 unx       53 b- defN 20-Sep-08 21:48 polygraphy/tools/inspect/__init__.py
--rw-rw-r--  2.0 unx     1204 b- defN 23-Jul-29 01:25 polygraphy/tools/inspect/inspect.py
--rw-rw-r--  2.0 unx      373 b- defN 23-Jul-29 01:25 polygraphy/tools/inspect/subtool/__init__.py
--rw-rw-r--  2.0 unx    10325 b- defN 23-Jun-13 19:50 polygraphy/tools/inspect/subtool/capability.py
+-rw-rw-r--  2.0 unx     1204 b- defN 23-Nov-16 18:51 polygraphy/tools/inspect/inspect.py
+-rw-rw-r--  2.0 unx      373 b- defN 23-Nov-16 18:51 polygraphy/tools/inspect/subtool/__init__.py
+-rw-rw-r--  2.0 unx    14372 b- defN 23-Dec-28 18:17 polygraphy/tools/inspect/subtool/capability.py
 -rw-rw-r--  2.0 unx     5413 b- defN 23-May-11 00:11 polygraphy/tools/inspect/subtool/data.py
--rw-rw-r--  2.0 unx     4814 b- defN 23-Jun-13 19:50 polygraphy/tools/inspect/subtool/diff_tactics.py
+-rw-rw-r--  2.0 unx     4814 b- defN 23-Nov-16 18:51 polygraphy/tools/inspect/subtool/diff_tactics.py
 -rw-rw-r--  2.0 unx     5954 b- defN 23-Mar-28 20:04 polygraphy/tools/inspect/subtool/model.py
--rw-rw-r--  2.0 unx     1564 b- defN 23-Jul-29 01:25 polygraphy/tools/inspect/subtool/sparsity.py
+-rw-rw-r--  2.0 unx     1564 b- defN 23-Nov-16 18:51 polygraphy/tools/inspect/subtool/sparsity.py
 -rw-rw-r--  2.0 unx     1377 b- defN 23-Mar-28 16:37 polygraphy/tools/inspect/subtool/tactics.py
+-rw-rw-r--  2.0 unx       50 b- defN 24-Feb-05 17:57 polygraphy/tools/plugin/__init__.py
+-rw-rw-r--  2.0 unx     1096 b- defN 24-Feb-05 17:57 polygraphy/tools/plugin/plugin.py
+-rw-rw-r--  2.0 unx      252 b- defN 24-Feb-05 17:57 polygraphy/tools/plugin/subtool/__init__.py
+-rw-rw-r--  2.0 unx     1177 b- defN 24-Feb-05 17:57 polygraphy/tools/plugin/subtool/list_plugins.py
+-rw-rw-r--  2.0 unx     1404 b- defN 24-Feb-05 17:57 polygraphy/tools/plugin/subtool/match.py
+-rw-rw-r--  2.0 unx     5125 b- defN 24-Feb-05 17:57 polygraphy/tools/plugin/subtool/plugin_base.py
+-rw-rw-r--  2.0 unx     3810 b- defN 24-Feb-05 17:57 polygraphy/tools/plugin/subtool/replace.py
 -rw-rw-r--  2.0 unx       41 b- defN 20-Sep-08 21:48 polygraphy/tools/run/__init__.py
--rw-rw-r--  2.0 unx     7909 b- defN 23-Jun-13 19:50 polygraphy/tools/run/run.py
+-rw-rw-r--  2.0 unx     7909 b- defN 23-Nov-16 18:51 polygraphy/tools/run/run.py
 -rw-rw-r--  2.0 unx       53 b- defN 20-Sep-08 21:48 polygraphy/tools/surgeon/__init__.py
--rw-rw-r--  2.0 unx     1183 b- defN 23-Jun-13 19:50 polygraphy/tools/surgeon/surgeon.py
--rw-rw-r--  2.0 unx      240 b- defN 23-Jun-13 19:50 polygraphy/tools/surgeon/subtool/__init__.py
--rw-rw-r--  2.0 unx     3370 b- defN 23-Jul-29 01:25 polygraphy/tools/surgeon/subtool/base.py
--rw-rw-r--  2.0 unx     8488 b- defN 23-Jul-29 01:25 polygraphy/tools/surgeon/subtool/extract.py
--rw-rw-r--  2.0 unx     6427 b- defN 23-Jul-29 01:25 polygraphy/tools/surgeon/subtool/insert.py
--rw-rw-r--  2.0 unx     1979 b- defN 23-Jul-29 01:25 polygraphy/tools/surgeon/subtool/prune.py
--rw-rw-r--  2.0 unx    10051 b- defN 23-Jul-29 01:25 polygraphy/tools/surgeon/subtool/sanitize.py
+-rw-rw-r--  2.0 unx     1285 b- defN 24-Feb-05 17:57 polygraphy/tools/surgeon/surgeon.py
+-rw-rw-r--  2.0 unx      397 b- defN 24-Feb-05 17:57 polygraphy/tools/surgeon/subtool/__init__.py
+-rw-rw-r--  2.0 unx     3370 b- defN 23-Nov-16 18:51 polygraphy/tools/surgeon/subtool/base.py
+-rw-rw-r--  2.0 unx     8488 b- defN 23-Nov-16 18:51 polygraphy/tools/surgeon/subtool/extract.py
+-rw-rw-r--  2.0 unx     6427 b- defN 23-Nov-16 18:51 polygraphy/tools/surgeon/subtool/insert.py
+-rw-rw-r--  2.0 unx     1979 b- defN 23-Nov-16 18:51 polygraphy/tools/surgeon/subtool/prune.py
+-rw-rw-r--  2.0 unx    10051 b- defN 23-Nov-16 18:51 polygraphy/tools/surgeon/subtool/sanitize.py
+-rw-rw-r--  2.0 unx     4063 b- defN 24-Feb-05 17:57 polygraphy/tools/surgeon/subtool/weight_reconstruct.py
+-rw-rw-r--  2.0 unx    16064 b- defN 24-Feb-16 19:33 polygraphy/tools/surgeon/subtool/weight_strip.py
 -rw-rw-r--  2.0 unx       56 b- defN 21-Feb-02 18:45 polygraphy/tools/template/__init__.py
 -rw-rw-r--  2.0 unx     1104 b- defN 23-Mar-28 16:37 polygraphy/tools/template/template.py
 -rw-rw-r--  2.0 unx      197 b- defN 22-Oct-06 23:14 polygraphy/tools/template/subtool/__init__.py
 -rw-rw-r--  2.0 unx     1033 b- defN 23-Mar-28 16:37 polygraphy/tools/template/subtool/base.py
 -rw-rw-r--  2.0 unx     3058 b- defN 23-Mar-28 16:37 polygraphy/tools/template/subtool/onnx_gs.py
 -rw-rw-r--  2.0 unx     2298 b- defN 23-Mar-28 16:37 polygraphy/tools/template/subtool/trt_config.py
 -rw-rw-r--  2.0 unx     2680 b- defN 23-Mar-28 16:37 polygraphy/tools/template/subtool/trt_network.py
--rw-rw-r--  2.0 unx       64 b- defN 23-Jun-13 19:50 polygraphy/util/__init__.py
--rw-rw-r--  2.0 unx    34780 b- defN 23-Jul-29 01:25 polygraphy/util/array.py
--rw-rw-r--  2.0 unx    31070 b- defN 23-Jun-14 20:36 polygraphy/util/util.py
--rw-rw-r--  2.0 unx    10764 b- defN 23-Jul-29 01:38 polygraphy-0.49.0.dist-info/LICENSE.txt
--rw-rw-r--  2.0 unx     5823 b- defN 23-Jul-29 01:38 polygraphy-0.49.0.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 23-Jul-29 01:38 polygraphy-0.49.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx       54 b- defN 23-Jul-29 01:38 polygraphy-0.49.0.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx       11 b- defN 23-Jul-29 01:38 polygraphy-0.49.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx        1 b- defN 23-Jul-29 01:38 polygraphy-0.49.0.dist-info/zip-safe
-?rw-rw-r--  2.0 unx    14445 b- defN 23-Jul-29 01:38 polygraphy-0.49.0.dist-info/RECORD
-157 files, 1031810 bytes uncompressed, 304739 bytes compressed:  70.5%
+-rw-rw-r--  2.0 unx       64 b- defN 23-Nov-16 18:51 polygraphy/util/__init__.py
+-rw-rw-r--  2.0 unx    35314 b- defN 23-Dec-28 18:17 polygraphy/util/array.py
+-rw-rw-r--  2.0 unx    31070 b- defN 24-Feb-05 17:57 polygraphy/util/util.py
+-rw-rw-r--  2.0 unx    10764 b- defN 24-Mar-19 19:47 polygraphy-0.49.9.dist-info/LICENSE.txt
+-rw-rw-r--  2.0 unx     5803 b- defN 24-Mar-19 19:47 polygraphy-0.49.9.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 24-Mar-19 19:47 polygraphy-0.49.9.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       53 b- defN 24-Mar-19 19:47 polygraphy-0.49.9.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       11 b- defN 24-Mar-19 19:47 polygraphy-0.49.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx        1 b- defN 24-Mar-19 19:47 polygraphy-0.49.9.dist-info/zip-safe
+?rw-rw-r--  2.0 unx    15344 b- defN 24-Mar-19 19:47 polygraphy-0.49.9.dist-info/RECORD
+166 files, 1099026 bytes uncompressed, 322310 bytes compressed:  70.7%
```

## zipnote {}

```diff
@@ -384,14 +384,35 @@
 
 Filename: polygraphy/tools/inspect/subtool/sparsity.py
 Comment: 
 
 Filename: polygraphy/tools/inspect/subtool/tactics.py
 Comment: 
 
+Filename: polygraphy/tools/plugin/__init__.py
+Comment: 
+
+Filename: polygraphy/tools/plugin/plugin.py
+Comment: 
+
+Filename: polygraphy/tools/plugin/subtool/__init__.py
+Comment: 
+
+Filename: polygraphy/tools/plugin/subtool/list_plugins.py
+Comment: 
+
+Filename: polygraphy/tools/plugin/subtool/match.py
+Comment: 
+
+Filename: polygraphy/tools/plugin/subtool/plugin_base.py
+Comment: 
+
+Filename: polygraphy/tools/plugin/subtool/replace.py
+Comment: 
+
 Filename: polygraphy/tools/run/__init__.py
 Comment: 
 
 Filename: polygraphy/tools/run/run.py
 Comment: 
 
 Filename: polygraphy/tools/surgeon/__init__.py
@@ -414,14 +435,20 @@
 
 Filename: polygraphy/tools/surgeon/subtool/prune.py
 Comment: 
 
 Filename: polygraphy/tools/surgeon/subtool/sanitize.py
 Comment: 
 
+Filename: polygraphy/tools/surgeon/subtool/weight_reconstruct.py
+Comment: 
+
+Filename: polygraphy/tools/surgeon/subtool/weight_strip.py
+Comment: 
+
 Filename: polygraphy/tools/template/__init__.py
 Comment: 
 
 Filename: polygraphy/tools/template/template.py
 Comment: 
 
 Filename: polygraphy/tools/template/subtool/__init__.py
@@ -444,29 +471,29 @@
 
 Filename: polygraphy/util/array.py
 Comment: 
 
 Filename: polygraphy/util/util.py
 Comment: 
 
-Filename: polygraphy-0.49.0.dist-info/LICENSE.txt
+Filename: polygraphy-0.49.9.dist-info/LICENSE.txt
 Comment: 
 
-Filename: polygraphy-0.49.0.dist-info/METADATA
+Filename: polygraphy-0.49.9.dist-info/METADATA
 Comment: 
 
-Filename: polygraphy-0.49.0.dist-info/WHEEL
+Filename: polygraphy-0.49.9.dist-info/WHEEL
 Comment: 
 
-Filename: polygraphy-0.49.0.dist-info/entry_points.txt
+Filename: polygraphy-0.49.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: polygraphy-0.49.0.dist-info/top_level.txt
+Filename: polygraphy-0.49.9.dist-info/top_level.txt
 Comment: 
 
-Filename: polygraphy-0.49.0.dist-info/zip-safe
+Filename: polygraphy-0.49.9.dist-info/zip-safe
 Comment: 
 
-Filename: polygraphy-0.49.0.dist-info/RECORD
+Filename: polygraphy-0.49.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## polygraphy/__init__.py

```diff
@@ -1,3 +1,3 @@
 import polygraphy.config
 
-__version__ = "0.49.0"
+__version__ = "0.49.9"
```

## polygraphy/backend/onnx/loader.py

```diff
@@ -392,16 +392,17 @@
             model = gs.export_onnx(graph.cleanup(), do_type_check=False)
             del graph
 
             if self.fold_shapes and self.do_shape_inference:
                 model = infer_shapes(model, allow_onnxruntime=self.allow_onnxruntime_shape_inference)
             return model
 
+        # Need to manually trigger the autoinstall this since it's used by ONNX-GS, which does not have an autoinstall mechanism.
         mod.autoinstall(onnxrt)
-        if not mod.has_mod("onnxruntime"):
+        if not onnxrt.is_installed() or not onnxrt.is_importable():
             G_LOGGER.error(
                 f"ONNX-Runtime is not installed, so constant folding may be suboptimal or not work at all.\n"
                 f"Consider installing ONNX-Runtime: {sys.executable} -m pip install onnxruntime"
             )
 
         model = self.load()
```

## polygraphy/backend/onnxrt/runner.py

```diff
@@ -42,15 +42,19 @@
     def activate_impl(self):
         self.sess, _ = util.invoke_if_callable(self._sess)
 
     @util.check_called_by("get_input_metadata")
     def get_input_metadata_impl(self):
         meta = TensorMetadata()
         for node in self.sess.get_inputs():
-            meta.add(node.name, dtype=DataType.from_dtype(node.type, "onnxruntime"), shape=node.shape)
+            meta.add(
+                node.name,
+                dtype=DataType.from_dtype(node.type, "onnxruntime"),
+                shape=node.shape,
+            )
         return meta
 
     @util.check_called_by("infer")
     def infer_impl(self, feed_dict):
         """
         Implementation for running inference with ONNX-Runtime.
         Do not call this method directly - use ``infer()`` instead,
```

## polygraphy/backend/tf/__init__.py

```diff
@@ -7,15 +7,15 @@
 
     def set_tf_logging_level(severity_trie):
         import os
         from polygraphy import mod
 
         tf = mod.lazy_import("tensorflow<2.0")
 
-        if not mod.has_mod("tensorflow"):
+        if not tf.is_installed() or not tf.is_importable():
             return
 
         sev = severity_trie.get(G_LOGGER.module_path(os.path.dirname(__file__)))
         if sev > G_LOGGER.WARNING:
             tf_sev = tf.compat.v1.logging.ERROR
             tf_logging_level = "3"
         elif sev > G_LOGGER.INFO:
```

## polygraphy/backend/trt/algorithm_selector.py

```diff
@@ -33,15 +33,17 @@
 #
 # NOTE: Modifying the structure of the data classes below will break backwards compatiblity
 #
 
 
 def check_is_instance(obj, cls, name):
     if not isinstance(obj, cls):
-        G_LOGGER.critical(f"'{name}' must be an instance of {cls.__name__}, but is: {obj}.")
+        G_LOGGER.critical(
+            f"'{name}' must be an instance of {cls.__name__}, but is: {obj}."
+        )
 
 
 @mod.export()
 class TensorInfo:
     """
     Tracks information about a tensor, such as format and data type.
     """
@@ -54,70 +56,66 @@
         Args:
             io_info (trt.IAlgorithmIOInfo): The algorithm I/O information.
 
         Returns:
             TensorInfo
         """
         return TensorInfo(
-            io_info.tensor_format,
             io_info.dtype,
             tuple(io_info.strides),
             # These fields were added in 8.6
             util.try_getattr(io_info, "vectorized_dim"),
             util.try_getattr(io_info, "components_per_element"),
         )
 
-    def __init__(self, tensor_format, dtype, strides, vectorized_dim, components_per_element):
+    def __init__(self, dtype, strides, vectorized_dim, components_per_element):
         """
         Args:
-            tensor_format (trt.TensorFormat): The tensor format.
             dtype (trt.DataType): The data type.
             strides (Sequence[int]): The strides.
             vectorized_dim (int): The index of the vectorized dimensions.
             components_per_element (int): The number of components per element.
         """
-        check_is_instance(tensor_format, trt.TensorFormat, "tensor_format")
         check_is_instance(dtype, trt.DataType, "dtype")
         check_is_instance(strides, Sequence, "strides")
         if vectorized_dim is not None:
             check_is_instance(vectorized_dim, int, "vectorized_dim")
         if components_per_element is not None:
             check_is_instance(components_per_element, int, "components_per_element")
 
-        self.tensor_format = tensor_format
         self.dtype = dtype
         self.strides = tuple(strides)
         self.vectorized_dim = vectorized_dim
         self.components_per_element = components_per_element
 
     def __eq__(self, other):
         return self.__dict__ == other.__dict__
 
     def __repr__(self):
-        return f"TensorInfo({str(self.tensor_format)}, {str(self.dtype)}, {self.strides}, {self.vectorized_dim}, {self.components_per_element})"
+        return f"TensorInfo({str(self.dtype)}, {self.strides}, {self.vectorized_dim}, {self.components_per_element})"
 
     def __hash__(self):
-        return hash((self.tensor_format, self.dtype, self.strides, self.vectorized_dim, self.components_per_element))
+        return hash(
+            (self.dtype, self.strides, self.vectorized_dim, self.components_per_element)
+        )
 
 
 @Encoder.register(TensorInfo)
 def encode(tensor_info):
     return {
-        "tensor_format": str(tensor_info.tensor_format),
         "dtype": str(tensor_info.dtype),
         "strides": tensor_info.strides,
         "vectorized_dim": tensor_info.vectorized_dim,
         "components_per_element": tensor_info.components_per_element,
     }
 
 
 @Decoder.register(TensorInfo)
 def decode(dct):
     return TensorInfo(
-        util.getattr_nested(trt, dct["tensor_format"]),
         util.getattr_nested(trt, dct["dtype"]),
         dct["strides"],
         dct["vectorized_dim"],
         dct["components_per_element"],
     )
 
 
@@ -142,15 +140,18 @@
 
         Returns:
             Algorithm
         """
 
         implementation = algorithm.algorithm_variant.implementation
         tactic = algorithm.algorithm_variant.tactic
-        inputs = tuple(TensorInfo.from_trt(algorithm.get_algorithm_io_info(i)) for i in range(context.num_inputs))
+        inputs = tuple(
+            TensorInfo.from_trt(algorithm.get_algorithm_io_info(i))
+            for i in range(context.num_inputs)
+        )
         outputs = tuple(
             TensorInfo.from_trt(algorithm.get_algorithm_io_info(i))
             for i in range(context.num_inputs, context.num_inputs + context.num_outputs)
         )
         return Algorithm(implementation, tactic, inputs, outputs)
 
     def __init__(self, implementation, tactic, inputs, outputs):
@@ -229,15 +230,18 @@
             TacticReplayData: self, to allow for method chaining.
         """
         self[name] = algorithm
         return self
 
     def __str__(self):
         return "\n".join(
-            [f"Layer: {name}\n{constants.TAB}Algorithm: {algorithm}" for (name, algorithm) in self.items()]
+            [
+                f"Layer: {name}\n{constants.TAB}Algorithm: {algorithm}"
+                for (name, algorithm) in self.items()
+            ]
         )
 
 
 @Encoder.register(TacticReplayData)
 def encode(replay):
     return {"replay": replay.dct}
```

## polygraphy/backend/trt/config.py

```diff
@@ -46,27 +46,30 @@
         tactic_sources=None,
         restricted=None,
         use_dla=None,
         allow_gpu_fallback=None,
         profiling_verbosity=None,
         memory_pool_limits=None,
         refittable=None,
+        strip_plan=None,
         preview_features=None,
         engine_capability=None,
         direct_io=None,
         builder_optimization_level=None,
         fp8=None,
         hardware_compatibility_level=None,
         max_aux_streams=None,
         version_compatible=None,
         exclude_lean_runtime=None,
         quantization_flags=None,
         error_on_timing_cache_miss=None,
         bf16=None,
         disable_compilation_cache=None,
+        progress_monitor=None,
+        weight_streaming=None,
     ):
         """
         Creates a TensorRT IBuilderConfig that can be used by EngineFromNetwork.
 
         Args:
             tf32 (bool):
                     Whether to build the engine with TF32 precision enabled.
@@ -131,14 +134,17 @@
                     Defaults to ``trt.ProfilingVerbosity.VERBOSE``.
             memory_pool_limits (Dict[trt.MemoryPoolType, int]):
                     Limits for different memory pools.
                     This should be a mapping of pool types to their respective limits in bytes.
             refittable (bool):
                     Enables the engine to be refitted with new weights after it is built.
                     Defaults to False.
+            strip_plan (bool):
+                    Strips the refittable weights from the engine plan file.
+                    Defaults to False.
             preview_features (List[trt.PreviewFeature]):
                     The preview features to enable.
                     Use an empty list to disable all preview features.
                     Defaults to TensorRT's default preview features.
             engine_capability (trt.EngineCapability):
                     The engine capability to build for.
                     Defaults to the default TensorRT engine capability.
@@ -180,25 +186,30 @@
                     Defaults to False.
             bf16 (bool):
                     Whether to build the engine with BF16 precision enabled.
                     Defaults to False.
             disable_compilation_cache (bool):
                     Whether to disable caching JIT-compiled code.
                     Defaults to False.
+            progress_monitor (trt.IProgressMonitor):
+                    A progress monitor. Allow users to view engine building progress through CLI.
+            weight_streaming (bool):
+                    TWhether to enable weight streaming for the TensorRT Engine.
         """
         self.tf32 = util.default(tf32, False)
         self.fp16 = util.default(fp16, False)
         self.bf16 = util.default(bf16, False)
         self.int8 = util.default(int8, False)
         self.fp8 = util.default(fp8, False)
         self.profiles = util.default(profiles, [Profile()])
         self.calibrator = calibrator
         self.precision_constraints = precision_constraints
         self.restricted = util.default(restricted, False)
         self.refittable = util.default(refittable, False)
+        self.strip_plan = util.default(strip_plan, False)
         self.timing_cache_path = load_timing_cache
         self.algorithm_selector = algorithm_selector
         self.sparse_weights = util.default(sparse_weights, False)
         self.tactic_sources = tactic_sources
         self.use_dla = util.default(use_dla, False)
         self.allow_gpu_fallback = util.default(allow_gpu_fallback, False)
         self.profiling_verbosity = profiling_verbosity
@@ -208,16 +219,20 @@
         self.direct_io = util.default(direct_io, False)
         self.builder_optimization_level = builder_optimization_level
         self.hardware_compatibility_level = hardware_compatibility_level
         self.max_aux_streams = max_aux_streams
         self.version_compatible = version_compatible
         self.exclude_lean_runtime = exclude_lean_runtime
         self.quantization_flags = quantization_flags
-        self.error_on_timing_cache_miss = util.default(error_on_timing_cache_miss, False)
+        self.error_on_timing_cache_miss = util.default(
+            error_on_timing_cache_miss, False
+        )
         self.disable_compilation_cache = util.default(disable_compilation_cache, False)
+        self.progress_monitor = progress_monitor
+        self.weight_streaming = weight_streaming
 
         if self.calibrator is not None and not self.int8:
             G_LOGGER.warning(
                 "A calibrator was provided to `CreateConfig`, but int8 mode was not enabled. "
                 "Did you mean to set `int8=True` to enable building with int8 precision?"
             )
 
@@ -239,20 +254,25 @@
         def try_run(func, name):
             try:
                 return func()
             except AttributeError:
                 trt_util.fail_unavailable(f"{name} in CreateConfig")
 
         def try_set_flag(flag_name):
-            return try_run(lambda: config.set_flag(getattr(trt.BuilderFlag, flag_name)), flag_name.lower())
+            return try_run(
+                lambda: config.set_flag(getattr(trt.BuilderFlag, flag_name)),
+                flag_name.lower(),
+            )
 
         if self.preview_features is not None:
             for preview_feature in trt.PreviewFeature.__members__.values():
                 try_run(
-                    lambda: config.set_preview_feature(preview_feature, preview_feature in self.preview_features),
+                    lambda: config.set_preview_feature(
+                        preview_feature, preview_feature in self.preview_features
+                    ),
                     "preview_features",
                 )
 
         G_LOGGER.verbose("Setting TensorRT Optimization Profiles")
         profiles = copy.deepcopy(self.profiles)
         for profile in profiles:
             # Last profile is used for set_calibration_profile.
@@ -282,14 +302,17 @@
             )
         if self.restricted:
             try_set_flag("SAFETY_SCOPE")
 
         if self.refittable:
             try_set_flag("REFIT")
 
+        if self.strip_plan:
+            try_set_flag("STRIP_PLAN")
+
         if self.direct_io:
             try_set_flag("DIRECT_IO")
 
         if self.tf32:
             try_set_flag("TF32")
         else:  # TF32 is on by default
             with contextlib.suppress(AttributeError):
@@ -302,25 +325,35 @@
             try_set_flag("BF16")
 
         if self.fp8:
             try_set_flag("FP8")
 
         if self.int8:
             try_set_flag("INT8")
-            if not network.has_explicit_precision:
+            # No Q/DQ layers means that we will need to calibrate.
+            if not any(
+                layer.type in [trt.LayerType.QUANTIZE, trt.LayerType.DEQUANTIZE]
+                for layer in network
+            ):
                 if self.calibrator is not None:
                     config.int8_calibrator = self.calibrator
                     try:
-                        config.set_calibration_profile(calib_profile.to_trt(builder, network))
+                        config.set_calibration_profile(
+                            calib_profile.to_trt(builder, network)
+                        )
                         G_LOGGER.info(f"Using calibration profile: {calib_profile}")
                     except AttributeError:
-                        G_LOGGER.extra_verbose("Cannot set calibration profile on TensorRT 7.0 and older.")
+                        G_LOGGER.extra_verbose(
+                            "Cannot set calibration profile on TensorRT 7.0 and older."
+                        )
 
                     trt_util.try_setup_polygraphy_calibrator(
-                        config, network, calib_profile=calib_profile.to_trt(builder, network)
+                        config,
+                        network,
+                        calib_profile=calib_profile.to_trt(builder, network),
                     )
                 else:
                     G_LOGGER.warning(
                         "Network does not have explicit precision and no calibrator was provided. Please ensure "
                         "that tensors in the network have dynamic ranges set, or provide a calibrator in order to use int8 mode."
                     )
 
@@ -338,34 +371,42 @@
 
             def set_profiling_verbosity():
                 config.profiling_verbosity = self.profiling_verbosity
 
             try_run(set_profiling_verbosity, name="profiling_verbosity")
         else:
             try:
-                config.profiling_verbosity = trt.ProfilingVerbosity.VERBOSE
+                config.profiling_verbosity = trt.ProfilingVerbosity.DETAILED
             except AttributeError:
                 pass
 
         if self.memory_pool_limits is not None:
             for pool_type, pool_size in self.memory_pool_limits.items():
-                try_run(lambda: config.set_memory_pool_limit(pool_type, pool_size), name="memory_pool_limits")
+                try_run(
+                    lambda: config.set_memory_pool_limit(pool_type, pool_size),
+                    name="memory_pool_limits",
+                )
 
         if self.tactic_sources is not None:
             tactic_sources_flag = 0
             for source in self.tactic_sources:
                 tactic_sources_flag |= 1 << int(source)
-            try_run(lambda: config.set_tactic_sources(tactic_sources_flag), name="tactic_sources")
+            try_run(
+                lambda: config.set_tactic_sources(tactic_sources_flag),
+                name="tactic_sources",
+            )
 
         try:
             cache = None
             if self.timing_cache_path:
                 try:
                     with util.LockFile(self.timing_cache_path):
-                        timing_cache_data = util.load_file(self.timing_cache_path, description="tactic timing cache")
+                        timing_cache_data = util.load_file(
+                            self.timing_cache_path, description="tactic timing cache"
+                        )
                         cache = config.create_timing_cache(timing_cache_data)
                 except FileNotFoundError:
                     G_LOGGER.warning(
                         "Timing cache file {} not found, falling back to empty timing cache.".format(
                             self.timing_cache_path
                         )
                     )
@@ -383,15 +424,17 @@
 
             def set_algo_selector():
                 config.algorithm_selector = self.algorithm_selector
 
             try_run(set_algo_selector, name="algorithm_selector")
 
             if not self.timing_cache_path:
-                G_LOGGER.warning("Disabling tactic timing cache because algorithm selector is enabled.")
+                G_LOGGER.warning(
+                    "Disabling tactic timing cache because algorithm selector is enabled."
+                )
                 try_set_flag("DISABLE_TIMING_CACHE")
 
         if self.engine_capability is not None:
 
             def set_engine_cap():
                 config.engine_capability = self.engine_capability
 
@@ -412,15 +455,17 @@
             try_run(set_hardware_compatibility_level, "hardware_compatibility_level")
 
         if self.version_compatible:
             try_set_flag("VERSION_COMPATIBLE")
 
         if self.exclude_lean_runtime:
             if not self.version_compatible:
-                G_LOGGER.critical(f"Cannot set EXCLUDE_LEAN_RUNTIME if version compatibility is not enabled. ")
+                G_LOGGER.critical(
+                    f"Cannot set EXCLUDE_LEAN_RUNTIME if version compatibility is not enabled. "
+                )
             try_set_flag("EXCLUDE_LEAN_RUNTIME")
 
         if self.hardware_compatibility_level is not None or self.version_compatible:
             G_LOGGER.info(
                 "Version or hardware compatibility was enabled. "
                 "If you are using an ONNX model, please set the NATIVE_INSTANCENORM ONNX parser flag, e.g. `--onnx-flags NATIVE_INSTANCENORM`"
             )
@@ -443,18 +488,28 @@
                     try_run(
                         lambda: config.clear_quantization_flag(quantization_flag),
                         "quantization_flag",
                     )
 
         if self.error_on_timing_cache_miss:
             try_set_flag("ERROR_ON_TIMING_CACHE_MISS")
-        
+
         if self.disable_compilation_cache:
             try_set_flag("DISABLE_COMPILATION_CACHE")
 
+        if self.progress_monitor is not None:
+
+            def set_progress_monitor():
+                config.progress_monitor = self.progress_monitor
+
+            try_run(set_progress_monitor, name="progress_monitor")
+
+        if self.weight_streaming:
+            try_set_flag("WEIGHT_STREAMING")
+
         return config
 
 
 @mod.export(funcify=True)
 class PostprocessConfig(BaseLoader):
     """
     [EXPERIMENTAL] Functor that applies a given post-processing function to a TensorRT ``IBuilderConfig``.
@@ -471,15 +526,17 @@
                     A callable which takes a builder, network, and config parameter and modifies the config in place.
         """
 
         self._config = config
 
         # Sanity-check that the function passed in is callable
         if not callable(func):
-            G_LOGGER.critical(f"Object {func} (of type {type(func)}) is not a callable.")
+            G_LOGGER.critical(
+                f"Object {func} (of type {type(func)}) is not a callable."
+            )
 
         self._func = func
 
     @util.check_called_by("__call__")
     def call_impl(self, builder, network):
         """
         Args:
```

## polygraphy/backend/trt/loader.py

```diff
@@ -87,93 +87,109 @@
             explicit_batch (bool):
                     Whether to create the network with explicit batch mode.
                     Defaults to True.
             strongly_typed (bool):
                     Whether to mark the network as being strongly typed.
                     Defaults to False.
         """
-        self.explicit_batch = util.default(explicit_batch, True)
+        self.explicit_batch = util.default(explicit_batch, True if mod.version(trt.__version__) < mod.version("10.0") else None)
         self.strongly_typed = util.default(strongly_typed, False)
 
     @util.check_called_by("__call__")
     def call_impl(self):
         """
         Returns:
             (trt.Builder, trt.INetworkDefinition): The builder and empty network.
         """
         builder = trt.Builder(trt_util.get_trt_logger())
         network_flags = 0
 
         if self.explicit_batch:
-            network_flags |= 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
+            try:
+                network_flags |= 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
+            except AttributeError:
+                trt_util.fail_unavailable("explicit_batch")
 
         if self.strongly_typed:
             try:
                 network_flags |= 1 << int(trt.NetworkDefinitionCreationFlag.STRONGLY_TYPED)
             except AttributeError:
                 trt_util.fail_unavailable("strongly_typed")
 
         network = builder.create_network(flags=network_flags)
         if network is None:
             G_LOGGER.critical("Invalid network. See logging output above for details.")
         return builder, network
 
 
 class BaseNetworkFromOnnx(BaseLoader):
-    def __init__(self, flags=None, strongly_typed=None):
+    def __init__(self, flags=None, plugin_instancenorm=None, strongly_typed=None):
         """
         Args:
             flags (List[trt.OnnxParserFlag]):
                     A list of ``OnnxParserFlag`` s to modify the default parsing
                     behavior of the ONNX parser.
                     Defaults to None.
+            plugin_instancenorm (bool):
+                    Whether to force usage of the plugin implementation of ONNX
+                    InstanceNorm by clearing the NATIVE_INSTANCENORM flag in the parser.
+                    Defaults to False
             strongly_typed (bool):
                     Whether to mark the network as being strongly typed.
                     Defaults to False.
         """
         self.flags = flags
+        self.plugin_instancenorm=util.default(plugin_instancenorm, False)
         self.strongly_typed = util.default(strongly_typed, False)
 
     @util.check_called_by("__call__")
     def call_impl(self):
         builder, network = create_network(strongly_typed=self.strongly_typed)
+        # Initialize plugin library for the parser.
+        trt.init_libnvinfer_plugins(trt_util.get_trt_logger(), "")
         parser = trt.OnnxParser(network, trt_util.get_trt_logger())
-        # Set flags if applicable
+        # Set flags if applicable.
         if mod.version(trt.__version__) >= mod.version("8.6"):
             if self.flags:
                 masked_flags = 0
                 for f in self.flags:
                     masked_flags |= 1 << int(f)
                 parser.flags = masked_flags
+            if self.plugin_instancenorm:
+                parser.clear_flag(trt.OnnxParserFlag.NATIVE_INSTANCENORM)
         return builder, network, parser
 
 
 @mod.export(funcify=True)
 class NetworkFromOnnxBytes(BaseNetworkFromOnnx):
     """
     Functor that parses an ONNX model to create a trt.INetworkDefinition.
     """
 
-    def __init__(self, model_bytes, flags=None, strongly_typed=None):
+    def __init__(self, model_bytes, flags=None, plugin_instancenorm=None, strongly_typed=None):
         """
         Parses an ONNX model.
 
         Args:
             model_bytes (Union[bytes, Callable() -> bytes]):
                     A serialized ONNX model or a callable that returns one.
 
             flags (List[trt.OnnxParserFlag])
                     A list of ``OnnxParserFlag`` s to modify the default parsing
                     behavior of the ONNX parser.
                     Defaults to None.
+            plugin_instancenorm (bool):
+                    Whether to force usage of the plugin implementation of ONNX
+                    InstanceNorm by clearing the NATIVE_INSTANCENORM flag in the parser.
+                    Defaults to False
             strongly_typed (bool):
                     Whether to mark the network as being strongly typed.
                     Defaults to False.
         """
-        super().__init__(flags=flags, strongly_typed=strongly_typed)
+        super().__init__(flags=flags, plugin_instancenorm=plugin_instancenorm, strongly_typed=strongly_typed)
         self._model_bytes = model_bytes
 
     @util.check_called_by("__call__")
     def call_impl(self):
         """
         Returns:
             (trt.IBuilder, trt.INetworkDefinition, trt.OnnxParser):
@@ -189,30 +205,34 @@
 @mod.export(funcify=True)
 class NetworkFromOnnxPath(BaseNetworkFromOnnx):
     """
     Functor that parses an ONNX model to create a trt.INetworkDefinition.
     This loader supports models with weights stored in an external location.
     """
 
-    def __init__(self, path, flags=None, strongly_typed=None):
+    def __init__(self, path, flags=None, plugin_instancenorm=None, strongly_typed=None):
         """
         Parses an ONNX model from a file.
 
         Args:
             path (str): The path from which to load the model.
 
             flags (List[trt.OnnxParserFlag]):
                     A list of ``OnnxParserFlag`` s to modify the default parsing
                     behavior of the ONNX parser.
                     Defaults to None.
+            plugin_instancenorm (bool):
+                    Whether to force usage of the plugin implementation of ONNX
+                    InstanceNorm by clearing the NATIVE_INSTANCENORM flag in the parser.
+                    Defaults to False
             strongly_typed (bool):
                     Whether to mark the network as being strongly typed.
                     Defaults to False.
         """
-        super().__init__(flags=flags, strongly_typed=strongly_typed)
+        super().__init__(flags=flags, plugin_instancenorm=plugin_instancenorm, strongly_typed=strongly_typed)
         self.path = path
 
     @util.check_called_by("__call__")
     def call_impl(self):
         """
         Returns:
             (trt.IBuilder, trt.INetworkDefinition, trt.OnnxParser):
@@ -798,7 +818,43 @@
                 attrs[name] = attr
 
             nodes.append(gs.Node(name=layer.name, op=op_name, attrs=attrs, inputs=node_inputs, outputs=node_outputs))
 
         graph = gs.Graph(name=network.name, inputs=graph_inputs, outputs=graph_outputs, nodes=nodes)
 
         return gs.export_onnx(graph)
+
+@mod.export(funcify=True)
+class MarkDebug(PostprocessNetwork):
+    """
+    Functor that mark tensors as debug tensors in a TensorRT ``INetworkDefinition``.
+    """
+
+    @staticmethod
+    def _apply(network, mark_debug):
+        tensor_map = trt_util.get_all_tensors(network)
+        util.check_sequence_contains(
+            tensor_map.keys(),
+            mark_debug,
+            name="the network",
+            items_name="tensors",
+            check_extra=False,
+            log_func=G_LOGGER.warning,
+        )
+
+        for name in mark_debug:
+            network.mark_debug(tensor_map[name])
+        return network
+
+    def __init__(self, network, mark_debug):
+        """
+        Mark tensors as debug tensors in a TensorRT ``INetworkDefinition``.
+
+        Args:
+            network (Union[Tuple[trt.Builder, trt.INetworkDefinition, Optional[parser]], Callable() -> Tuple[trt.Builder, trt.INetworkDefinition, Optional[parser]]):
+                    A tuple containing a TensorRT builder, network and optionally parser or a callable that returns one.
+                    To omit the parser, return a tuple containing just the builder and network.
+            mark_debug (List[str]):
+                    List of tensor names to mark as debug tensors.
+        """
+        func = lambda network: MarkDebug._apply(network, mark_debug)
+        super().__init__(network, func, "MarkDebug")
```

## polygraphy/backend/trt/runner.py

```diff
@@ -25,14 +25,37 @@
 from polygraphy.logger import G_LOGGER
 
 np = mod.lazy_import("numpy")
 torch = mod.lazy_import("torch>=1.13.0")
 trt = mod.lazy_import("tensorrt>=8.5")
 
 
+def _make_debug_listener():
+    class DebugTensorWriter(trt.IDebugListener):
+        def __init__(self):
+            trt.IDebugListener.__init__(self)
+            self.debug_tensor_outputs = {}
+
+        def process_debug_tensor(self, addr, location, type, shape, name, stream):
+            cuda.wrapper().stream_synchronize(stream)
+            datatype = DataType.from_dtype(type)
+            size = util.volume(shape)
+            buffer = np.zeros(shape, dtype=DataType.to_dtype(datatype, "numpy"))
+            buffer = util.array.resize_or_reallocate(buffer, size)
+            cuda.wrapper().memcpy(
+                dst=util.array.data_ptr(buffer),
+                src=addr,
+                nbytes=size*datatype.itemsize,
+                kind=cuda.MemcpyKind.DeviceToHost,
+                stream_ptr=stream)
+            cuda.wrapper().stream_synchronize(stream)
+            self.debug_tensor_outputs[name] = util.array.resize_or_reallocate(buffer, shape)
+
+    return DebugTensorWriter()
+
 def _make_output_allocator(use_torch):
     class OutputAllocator(trt.IOutputAllocator):
         def __init__(self):
             trt.IOutputAllocator.__init__(self)
             self.buffers = {}
             self.shapes = {}
 
@@ -106,53 +129,112 @@
     """
     Runs inference using TensorRT.
 
     Note that runners are not designed for production deployment and should generally
     be used only for prototyping, testing, and debugging.
     """
 
-    def __init__(self, engine, name: str = None, optimization_profile: int = None):
+    def __init__(self, engine, name: str = None, optimization_profile: int = None, allocation_strategy: str = None, weight_streaming_budget: int = None, weight_streaming_percent: float = None):
         """
         Args:
             engine (Union[Union[trt.ICudaEngine, trt.IExecutionContext], Callable() -> Union[trt.ICudaEngine, trt.IExecutionContext]]):
                     A TensorRT engine or execution context or a callable that returns one.
                     If an engine is provided, the runner will create a context automatically.
 
             name (str):
                     The human-readable name prefix to use for this runner.
                     A runner count and timestamp will be appended to this prefix.
             optimization_profile (int):
                     The index of the optimization profile to set each time this runner is activated.
                     When this is not provided, the profile is not set explicitly and will default to the 0th profile.
                     You can also change the profile after the runner is active using the ``set_profile()`` method.
+            allocation_strategy (str):
+                    The way device memory (internal activation and scratch memory) is allocated for the execution context. The value of this argument can be:
+                        - "static": The default value. The execution context will pre-allocate a block of memory that is sufficient for any possible input size across all profiles.
+                        - "profile": Allocate device memory enough for the current profile based on profile max shapes.
+                        - "runtime": Allocate device meomry enough for the current input shapes.
+            weight_streaming_budget (int):
+                    The amount of GPU memory that TensorRT can use for weights at runtime. Tt can take on the following values:
+                        None or 0: Disables weight streaming at runtime.
+                        -1: TensorRT will decide the streaming budget automatically.
+                        > 0: The maximum amount of GPU memory TensorRT is allowed to use for weights in bytes.
+            weight_streaming_percent (float):
+                    The percentage of weights that TRT will stream from CPU to GPU. It can take on the following values:
+                        None or 0: Disables weight streaming at runtime.
+                        [0 to 100]: The percentage of weights TRT will stream. 100 will stream the maximum number of weights.
         """
         super().__init__(name=name, prefix="trt-runner")
         self._engine_or_context = engine
         self.optimization_profile = optimization_profile
+        self.allocation_strategy = allocation_strategy
+        self.weight_streaming_budget = weight_streaming_budget
+        self.weight_streaming_percent = weight_streaming_percent
 
     @util.check_called_by("activate")
     def activate_impl(self):
         engine_or_context, _ = util.invoke_if_callable(self._engine_or_context)
 
         if isinstance(engine_or_context, trt.ICudaEngine):
             self.engine = engine_or_context
-            self.context = self.engine.create_execution_context()
+
+            # Setup weight streaming if applicable
+            if self.weight_streaming_budget != None and self.weight_streaming_percent != None:
+                G_LOGGER.critical(f"Cannot specify the weight streaming budget both in bytes and percentage.")
+
+            budget_bytes = None
+            if self.weight_streaming_budget is not None:
+                assert self.weight_streaming_budget == -1 or self.weight_streaming_budget >= 0
+                budget_bytes = self.weight_streaming_budget
+            elif self.weight_streaming_percent is not None:
+                assert 0 <= self.weight_streaming_percent <= 100
+                if self.weight_streaming_percent == 0:
+                    budget_bytes = 0 # Disable weight streaming
+                else:
+                    min_budget = self.engine.minimum_weight_streaming_budget
+                    max_budget = self.engine.streamable_weights_size
+                    budget_bytes = (1 - self.weight_streaming_percent / 100.0) * (max_budget - min_budget) + min_budget
+            if budget_bytes is not None:
+                budget_bytes = int(budget_bytes)
+                self.engine.weight_streaming_budget = budget_bytes
+                if self.engine.weight_streaming_budget != budget_bytes:
+                    G_LOGGER.critical(f"Failed to set weight streaming budget to {budget_bytes}!")
+                if budget_bytes == 0:
+                    G_LOGGER.info(f"Weight streaming is disabled.")
+                elif budget_bytes == -1:
+                    G_LOGGER.info(f"Weight streaming is enabled with TensorRT automatically determiing the budget.")
+                else:
+                    G_LOGGER.info(f"Weight streaming is enabled with a memory budget of {budget_bytes} bytes.")
+            
+            allocation_strategy = util.default(self.allocation_strategy, "static")
+            if allocation_strategy == 'static':
+                self.context = self.engine.create_execution_context()
+            elif allocation_strategy in ['profile', 'runtime']:
+                # Device memory will be managed by polygraphy
+                self.context = self.engine.create_execution_context(trt.ExecutionContextAllocationStrategy.USER_MANAGED)
+            else:
+                 G_LOGGER.critical("Invalid allocation strategy specified.")
             if not self.context:
                 G_LOGGER.critical("Invalid Context. See error log for details.")
         elif isinstance(engine_or_context, trt.IExecutionContext):
             self.context = engine_or_context
             self.engine = self.context.engine
+            if self.allocation_strategy is not None:
+                G_LOGGER.warning(
+                    "An allocation strategy was specified. Please ensure the provided execution context uses the same strategy."
+                )
+
         else:
             G_LOGGER.critical(
                 "Invalid Engine or Context. Please ensure the engine was built correctly. See error log for details."
             )
 
         self.device_input_buffers = OrderedDict()
         self.host_output_buffers = OrderedDict()
         self.stream = cuda.Stream()
+        self.context_memory_buffer = None
 
         if self.optimization_profile is not None:
             self.set_profile(self.optimization_profile)
 
     def set_profile(self, index: int):
         """
         Sets the active optimization profile for this runner.
@@ -230,20 +312,45 @@
                 if not self.context.set_input_shape(name, array_shape):
                     G_LOGGER.critical(f"For input: {name}, failed to set shape to: {array_shape}")
 
             if self.context.get_tensor_address(name) != ptr:
                 if not self.context.set_tensor_address(name, ptr):
                     G_LOGGER.critical(f"For input: {name}, failed to set tensor address to: {ptr}")
 
+        try:
+            self.context.set_all_tensors_debug_state
+        except AttributeError:
+            pass
+        else:
+            # Set up the debug listener before running inference.
+            debug_listener = _make_debug_listener()
+            self.context.set_all_tensors_debug_state(True)
+            if not self.context.set_debug_listener(debug_listener):
+                G_LOGGER.critical(f"Failed to set debug listener.")
+
         # Set up the output allocator before running inference.
         output_allocator = _make_output_allocator(use_torch and torch.cuda.is_available())
         for name in get_io(trt.TensorIOMode.OUTPUT):
             if not self.context.set_output_allocator(name, output_allocator):
                 G_LOGGER.critical(f"For output: {name}, failed to set output allocator")
 
+        if self.allocation_strategy in ["profile", "runtime"]:
+            if self.allocation_strategy == "profile":
+                # Perform per-profile allocation.
+                size_to_allocate = self.engine.get_device_memory_size_for_profile(self.context.active_optimization_profile)
+            elif self.allocation_strategy =="runtime":
+                # Perform runtime allocation.
+                size_to_allocate = self.context.update_device_memory_size_for_shapes()
+
+            if self.context_memory_buffer is None:
+                self.context_memory_buffer = cuda.DeviceArray.raw((size_to_allocate,))
+
+            self.context_memory_buffer.resize((size_to_allocate,))
+            self.context.device_memory = self.context_memory_buffer.ptr
+
         if not self.context.execute_async_v3(self.stream.ptr):
             G_LOGGER.critical("`execute_async_v3()` failed. Please see the logging output above for details.")
 
         output_buffers = OrderedDict()
         for name in get_io(trt.TensorIOMode.OUTPUT):
             # If we're dealing with vectorized formats, we need to return a FormattedArray.
             # Otherwise, we create a view instead with the correct shape/dtype.
@@ -271,14 +378,23 @@
             if using_vectorized_format:
                 array = FormattedArray(raw_array, shape=shape)
             else:
                 array = util.array.view(raw_array, dtype, shape)
             output_buffers[name] = array
 
         self.stream.synchronize()
+
+        try:
+            self.context.set_all_tensors_debug_state
+        except AttributeError:
+            pass
+        else:
+            if debug_listener.debug_tensor_outputs:
+                output_buffers.update(debug_listener.debug_tensor_outputs)
+
         return output_buffers
 
     @util.check_called_by("infer")
     def infer_impl(self, feed_dict, copy_outputs_to_host=None):
         """
         Implementation for running inference with TensorRT.
         Do not call this method directly - use ``infer()`` instead,
@@ -311,16 +427,19 @@
         self.inference_time = end - start
 
         return output_buffers
 
     @util.check_called_by("deactivate")
     def deactivate_impl(self):
         [buf.free() for buf in self.device_input_buffers.values()]
+        if self.context_memory_buffer is not None:
+            self.context_memory_buffer.free()
         self.stream.free()
 
         del (
             self.engine,
             self.context,
             self.device_input_buffers,
             self.host_output_buffers,
             self.stream,
+            self.context_memory_buffer,
         )
```

## polygraphy/backend/trt/util.py

```diff
@@ -52,15 +52,17 @@
         def log(self, severity, msg):
             try:
                 log_func = {
                     # This function cannot throw, so `critical` should not be used here!
                     trt.Logger.INTERNAL_ERROR: G_LOGGER.error,
                     trt.Logger.ERROR: G_LOGGER.error,
                     # Reduce warning spam from TRT.
-                    trt.Logger.WARNING: lambda msg: G_LOGGER.warning(msg, mode=LogMode.ONCE),
+                    trt.Logger.WARNING: lambda msg: G_LOGGER.warning(
+                        msg, mode=LogMode.ONCE
+                    ),
                     trt.Logger.INFO: G_LOGGER.verbose,
                     trt.Logger.VERBOSE: G_LOGGER.extra_verbose,
                 }.get(severity, G_LOGGER.super_verbose)
 
                 log_func(msg)
             except KeyboardInterrupt:
                 # `log()` is `noexcept` so we need to convert exceptions to signals so that
@@ -79,27 +81,31 @@
 def check_onnx_parser_errors(parser, success):
     if parser.num_errors > 0:
         for index in range(parser.num_errors):
             G_LOGGER.error(parser.get_error(index))
         G_LOGGER.critical("Could not parse ONNX correctly")
 
     if not success:
-        G_LOGGER.critical("Failed to parse ONNX model. Does the model file exist and contain a valid ONNX model?")
+        G_LOGGER.critical(
+            "Failed to parse ONNX model. Does the model file exist and contain a valid ONNX model?"
+        )
 
 
 def get_layer_class_mapping():
     layer_class_mapping = {}
 
     def try_add(layer_type, layer_cls):
         try:
             layer_type = getattr(trt.LayerType, layer_type)
             layer_cls = getattr(trt, layer_cls)
         except AttributeError:
             if config.INTERNAL_CORRECTNESS_CHECKS:
-                G_LOGGER.warning(f"Could not find layer type: {layer_type} or layer class: {layer_cls}")
+                G_LOGGER.warning(
+                    f"Could not find layer type: {layer_type} or layer class: {layer_cls}"
+                )
         else:
             layer_class_mapping[layer_type] = layer_cls
 
     try_add("CONVOLUTION", "IConvolutionLayer")
     try_add("FULLY_CONNECTED", "IFullyConnectedLayer")
     try_add("ACTIVATION", "IActivationLayer")
     try_add("POOLING", "IPoolingLayer")
@@ -154,25 +160,33 @@
 
 def get_network_input_names_meta(network):
     names = []
     meta = TensorMetadata()
     for i in range(network.num_inputs):
         tensor = network.get_input(i)
         names.append(tensor.name)
-        meta.add(name=tensor.name, dtype=DataType.from_dtype(tensor.dtype, "tensorrt"), shape=tensor.shape)
+        meta.add(
+            name=tensor.name,
+            dtype=DataType.from_dtype(tensor.dtype, "tensorrt"),
+            shape=tensor.shape,
+        )
     return names, meta
 
 
 def get_network_output_names_meta(network):
     names = []
     meta = TensorMetadata()
     for i in range(network.num_outputs):
         tensor = network.get_output(i)
         names.append(tensor.name)
-        meta.add(name=tensor.name, dtype=DataType.from_dtype(tensor.dtype, "tensorrt"), shape=tensor.shape)
+        meta.add(
+            name=tensor.name,
+            dtype=DataType.from_dtype(tensor.dtype, "tensorrt"),
+            shape=tensor.shape,
+        )
     return names, meta
 
 
 def get_layer_input_names_meta(layer):
     names = []
     meta = TensorMetadata()
     for i in range(layer.num_inputs):
@@ -194,15 +208,22 @@
     return names, meta
 
 
 def str_from_layer(layer, index):
     input_names, input_meta = get_layer_input_names_meta(layer)
     output_names, output_meta = get_layer_output_names_meta(layer)
     return util.str_from_layer(
-        "Layer", index, layer.name, layer.type, input_names, input_meta, output_names, output_meta
+        "Layer",
+        index,
+        layer.name,
+        layer.type,
+        input_names,
+        input_meta,
+        output_names,
+        output_meta,
     )
 
 
 def get_layer_attribute_names(layer):
     def is_special_attribute(attr):
         return attr.startswith("__") and attr.endswith("__")
 
@@ -222,15 +243,17 @@
             if layer.num_inputs > 1:
                 return attr not in ["shape", "start", "stride"]
         return True
 
     return [
         attr
         for attr in dir(layer)
-        if not is_special_attribute(attr) and not hasattr(trt.ILayer, attr) and is_valid_attribute(attr, layer)
+        if not is_special_attribute(attr)
+        and not hasattr(trt.ILayer, attr)
+        and is_valid_attribute(attr, layer)
     ]
 
 
 def str_from_network(network, show_layers=None, show_attrs=None, show_weights=None):
     """
     Converts a TensorRT network to a human-readable representation
 
@@ -245,21 +268,25 @@
     """
     show_layers = util.default(show_layers, False)
     show_attrs = util.default(show_attrs, False)
     show_weights = util.default(show_weights, False)
 
     LAYER_TYPE_CLASS_MAPPING = get_layer_class_mapping()
 
-    network_str = f"Name: {network.name} | {'Implicit' if hasattr(network, 'has_implicit_batch_dimension') and network.has_implicit_batch_dimension else 'Explicit'} Batch Network{' with Explicit Precision ' if hasattr(network, 'has_explicit_precision') and network.has_explicit_precision else ''}\n"
+    network_str = f"Name: {network.name} | {'Implicit' if hasattr(network, 'has_implicit_batch_dimension') and network.has_implicit_batch_dimension else 'Explicit'} Batch{' Strongly Typed' if hasattr(network, 'get_flag') and network.get_flag(trt.NetworkDefinitionCreationFlag.STRONGLY_TYPED) else ''} Network\n"
     network_str += "\n"
 
     _, input_metadata = get_network_input_names_meta(network)
-    network_str += f"---- {len(input_metadata)} Network Input(s) ----\n{input_metadata}\n\n"
+    network_str += (
+        f"---- {len(input_metadata)} Network Input(s) ----\n{input_metadata}\n\n"
+    )
     _, output_metadata = get_network_output_names_meta(network)
-    network_str += f"---- {len(output_metadata)} Network Output(s) ----\n{output_metadata}\n\n"
+    network_str += (
+        f"---- {len(output_metadata)} Network Output(s) ----\n{output_metadata}\n\n"
+    )
     network_str += f"---- {network.num_layers} Layer(s) ----\n"
     if show_layers:
         for index, layer in enumerate(network):
             if layer.type in LAYER_TYPE_CLASS_MAPPING:
                 layer.__class__ = LAYER_TYPE_CLASS_MAPPING[layer.type]
 
             network_str += str_from_layer(layer, index)
@@ -275,15 +302,17 @@
                             val = getattr(layer, attr)
                         except Exception as err:
                             val = f"<Error: could not retrieve layer attribute: {attr}. Note: Error was: {err}>"
                     if show_weights or not isinstance(val, np.ndarray):
                         attr_str = ""
                         if layer.name:
                             attr_str += f"{layer.name}."
-                        network_str += util.indent_block(f"{attr_str}{attr} = {val}") + "\n"
+                        network_str += (
+                            util.indent_block(f"{attr_str}{attr} = {val}") + "\n"
+                        )
             network_str += "\n"
 
     return util.indent_block(network_str, level=0)
 
 
 def get_all_tensors(network):
     all_tensors = set()
@@ -304,15 +333,19 @@
         network (trt.INetworkDefinition): The network in which to mark outputs.
         outputs (Sequence[str]): The names of tensors to mark as outputs.
     """
     outputs = util.unique_list(outputs)
 
     tensor_map = get_all_tensors(network)
     util.check_sequence_contains(
-        tensor_map.keys(), outputs, name="the network", items_name="outputs", check_extra=False
+        tensor_map.keys(),
+        outputs,
+        name="the network",
+        items_name="outputs",
+        check_extra=False,
     )
 
     for tensor in tensor_map.values():
         # Clear all old outputs
         if tensor.is_network_output:
             network.unmark_output(tensor)
 
@@ -321,16 +354,24 @@
         network.mark_output(tensor_map[name])
 
 
 def mark_layerwise(network):
     # Layers within loops cannot be marked as network outputs.
     LOOP_START_NAMES = ["TRIP_LIMIT", "ITERATOR", "RECURRENCE"]
     LOOP_END_NAMES = ["LOOP_OUTPUT"]
-    LOOP_START_LAYERS = [getattr(trt.LayerType, attr) for attr in LOOP_START_NAMES if hasattr(trt.LayerType, attr)]
-    LOOP_END_LAYERS = [getattr(trt.LayerType, attr) for attr in LOOP_END_NAMES if hasattr(trt.LayerType, attr)]
+    LOOP_START_LAYERS = [
+        getattr(trt.LayerType, attr)
+        for attr in LOOP_START_NAMES
+        if hasattr(trt.LayerType, attr)
+    ]
+    LOOP_END_LAYERS = [
+        getattr(trt.LayerType, attr)
+        for attr in LOOP_END_NAMES
+        if hasattr(trt.LayerType, attr)
+    ]
     EXCLUDE_LAYERS = [trt.LayerType.SHAPE, trt.LayerType.CONSTANT]
     outputs = []
     in_loop = False
     for layer in network:
         if layer.type in LOOP_START_LAYERS:
             G_LOGGER.warning(
                 "Loop detected. Please ensure the network is topologically sorted so that layers within "
@@ -353,15 +394,19 @@
 
 
 def unmark_outputs(network, outputs):
     outputs = util.unique_list(outputs)
 
     tensor_map = get_all_tensors(network)
     util.check_sequence_contains(
-        tensor_map.keys(), outputs, name="the network", items_name="outputs", check_extra=False
+        tensor_map.keys(),
+        outputs,
+        name="the network",
+        items_name="outputs",
+        check_extra=False,
     )
 
     for name in outputs:
         tensor = tensor_map[name]
         if tensor.is_network_output:
             network.unmark_output(tensor)
 
@@ -378,18 +423,24 @@
         return "[" + ", ".join(lst) + "]"
 
     def add_line(title, line):
         lines.append((f"{title:{22}} | " + line).strip())
 
     def get_enabled_enum_vals(EnumType, is_enabled):
         # is_enabled is a Callable[[enum_val], bool] which reports whether to include the enum value.
-        return [name for name, enum_val in EnumType.__members__.items() if is_enabled(enum_val)]
+        return [
+            name
+            for name, enum_val in EnumType.__members__.items()
+            if is_enabled(enum_val)
+        ]
 
     # Flags
-    enabled_builder_flags = get_enabled_enum_vals(trt.BuilderFlag, lambda flag: config.get_flag(flag))
+    enabled_builder_flags = get_enabled_enum_vals(
+        trt.BuilderFlag, lambda flag: config.get_flag(flag)
+    )
     add_line("Flags", f"{str_from_list(enabled_builder_flags)}")
 
     # Engine Capability
     with contextlib.suppress(AttributeError):
         add_line("Engine Capability", str(config.engine_capability))
 
     # Memory Pools
@@ -400,51 +451,66 @@
             # Only show DLA memory pools when DLA is in use
             if (not name.startswith("DLA") or using_dla)
         ]
         add_line("Memory Pools", f"{str_from_list(mem_pool_limits)}")
 
     # Tactic Sources
     with contextlib.suppress(AttributeError):
-        source_vals = get_enabled_enum_vals(trt.TacticSource, lambda val: (1 << int(val)) & config.get_tactic_sources())
+        source_vals = get_enabled_enum_vals(
+            trt.TacticSource, lambda val: (1 << int(val)) & config.get_tactic_sources()
+        )
         add_line("Tactic Sources", f"{str_from_list(source_vals)}")
 
     # DLA
     if using_dla:
-        add_line("DLA", f"Default Device Type: {config.default_device_type}, Core: {config.DLA_core}")
+        add_line(
+            "DLA",
+            f"Default Device Type: {config.default_device_type}, Core: {config.DLA_core}",
+        )
 
     # Profiling Verbosity
     with contextlib.suppress(AttributeError):
         add_line("Profiling Verbosity", f"{config.profiling_verbosity}")
 
     # Optimization Profiles
-    if config.num_optimization_profiles > 1:  # Not particularly interesting unless there are multiple.
-        add_line("Optimization Profiles", f"{config.num_optimization_profiles} profile(s)")
+    if (
+        config.num_optimization_profiles > 1
+    ):  # Not particularly interesting unless there are multiple.
+        add_line(
+            "Optimization Profiles", f"{config.num_optimization_profiles} profile(s)"
+        )
 
     # Preview Features
     with contextlib.suppress(AttributeError):
-        feature_vals = get_enabled_enum_vals(trt.PreviewFeature, lambda val: config.get_preview_feature(val))
+        feature_vals = get_enabled_enum_vals(
+            trt.PreviewFeature, lambda val: config.get_preview_feature(val)
+        )
         if feature_vals:
             add_line("Preview Features", f"{str_from_list(feature_vals)}")
 
     # Calibrator
     if config.int8_calibrator:
         add_line("Calibrator", f"{config.int8_calibrator}")
 
     # Quantization Flags
     with contextlib.suppress(AttributeError):
-        quantization_flags = get_enabled_enum_vals(trt.QuantizationFlag, lambda val: config.get_quantization_flag(val))
+        quantization_flags = get_enabled_enum_vals(
+            trt.QuantizationFlag, lambda val: config.get_quantization_flag(val)
+        )
         if quantization_flags:
             add_line("Quantization Flags", f"{str_from_list(quantization_flags)}")
 
     return "\n".join(lines)
 
 
 def check_profile(profile):
     if not bool(profile):
-        G_LOGGER.critical(f"Profile is not valid, please provide profile data.\nNote: profile was: {profile}")
+        G_LOGGER.critical(
+            f"Profile is not valid, please provide profile data.\nNote: profile was: {profile}"
+        )
     return profile
 
 
 def str_from_tensor(tensor, is_shape_tensor):
     ret = "Input "
     if is_shape_tensor:
         ret += "shape-tensor"
@@ -519,30 +585,35 @@
 def try_setup_polygraphy_calibrator(config, network, calib_profile=None):
     """
     Tries to call setup methods specific to Polygraphy calibrators.
     Returns early if there is no calibrator or if it is not a Polygraphy calibrator.
     """
     calibrator = config.int8_calibrator
     if calibrator is None or not (
-        hasattr(calibrator, "is_polygraphy_calibrator") and calibrator.is_polygraphy_calibrator
+        hasattr(calibrator, "is_polygraphy_calibrator")
+        and calibrator.is_polygraphy_calibrator
     ):
         # No calibrator or not a Polygraphy calibrator.
         return
 
     if calib_profile is None:
         try:
             calib_profile = config.get_calibration_profile()
         except AttributeError:
-            G_LOGGER.extra_verbose("Cannot get calibration profile on TensorRT 7.0 and older.")
+            G_LOGGER.extra_verbose(
+                "Cannot get calibration profile on TensorRT 7.0 and older."
+            )
             # Return early so we don't emit extraneous warnings on TRT 7.0 and older.
             return
 
     try:
         # TensorRT does not currently support shapes other than the OPT shape.
-        input_metadata = get_input_metadata_from_network(network, calib_profile, force_opt_shapes=True)
+        input_metadata = get_input_metadata_from_network(
+            network, calib_profile, force_opt_shapes=True
+        )
     except PolygraphyException as err:
         G_LOGGER.warning(
             "Could not determine input_metadata to provide to the calibrator because no calibration profile is set. "
             "Please either set a calibration profile in the config or call `calibrator.set_input_metadata()` manually. "
             f"\nNote: Error was:\n{err}",
             mode=LogMode.ONCE,
         )
@@ -577,58 +648,88 @@
             continue
 
         shape = engine.get_tensor_shape(name)
         # If the input format is HWC, make sure the input is shaped accordingly
         if get_tensor_format(engine, context, name) == trt.TensorFormat.HWC:
             shape = get_hwc_shape_from_chw(shape, context.get_tensor_strides(name))
 
-        meta.add(name=name, dtype=DataType.from_dtype(engine.get_tensor_dtype(name), "tensorrt"), shape=shape)
+        meta.add(
+            name=name,
+            dtype=DataType.from_dtype(engine.get_tensor_dtype(name), "tensorrt"),
+            shape=shape,
+        )
     return meta
 
 
 def str_from_engine(engine, context, show_layers=None, show_attrs=None):
     show_layers = util.default(show_layers, False)
     show_attrs = util.default(show_attrs, False)
 
     num_io_tensors = engine.num_io_tensors
 
     engine_str = f"Name: {engine.name} | {'Refittable ' if engine.refittable else ''}{'Implicit' if hasattr(engine, 'has_implicit_batch_dimension') and engine.has_implicit_batch_dimension else 'Explicit'} Batch Engine\n"
     engine_str += "\n"
 
     # Show metadata for the first profile (i.e. the dynamic shapes)
-    input_metadata = get_metadata_from_engine(engine, context, mode=trt.TensorIOMode.INPUT)
-    output_metadata = get_metadata_from_engine(engine, context, mode=trt.TensorIOMode.OUTPUT)
+    input_metadata = get_metadata_from_engine(
+        engine, context, mode=trt.TensorIOMode.INPUT
+    )
+    output_metadata = get_metadata_from_engine(
+        engine, context, mode=trt.TensorIOMode.OUTPUT
+    )
 
-    engine_str += f"---- {len(input_metadata)} Engine Input(s) ----\n{input_metadata}\n\n"
-    engine_str += f"---- {len(output_metadata)} Engine Output(s) ----\n{output_metadata}\n\n"
+    engine_str += (
+        f"---- {len(input_metadata)} Engine Input(s) ----\n{input_metadata}\n\n"
+    )
+    engine_str += (
+        f"---- {len(output_metadata)} Engine Output(s) ----\n{output_metadata}\n\n"
+    )
 
-    engine_str += f"---- Memory ----\nDevice Memory: {engine.device_memory_size} bytes\n\n"
+    engine_str += (
+        f"---- Memory ----\nDevice Memory: {engine.device_memory_size} bytes\n\n"
+    )
 
     engine_str += f"---- {engine.num_optimization_profiles} Profile(s) ({num_io_tensors} Tensor(s) Each) ----\n"
     for profile_index in range(engine.num_optimization_profiles):
         engine_str += f"- Profile: {profile_index}\n"
 
-        max_width = max([len(engine.get_tensor_name(idx)) for idx in range(engine.num_io_tensors)]) + 8
+        max_width = (
+            max(
+                [
+                    len(engine.get_tensor_name(idx))
+                    for idx in range(engine.num_io_tensors)
+                ]
+            )
+            + 8
+        )
 
         for idx in range(num_io_tensors):
             name = engine.get_tensor_name(idx)
-            binding_type = " (Input)" if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT else "(Output)"
-            engine_str += util.indent_block(f"Tensor: {name:<{max_width}} {binding_type}, Index: {idx}")
+            binding_type = (
+                " (Input)"
+                if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT
+                else "(Output)"
+            )
+            engine_str += util.indent_block(
+                f"Tensor: {name:<{max_width}} {binding_type}, Index: {idx}"
+            )
 
             if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
-                min_shape, opt_shape, max_shape = engine.get_tensor_profile_shape(name, profile_index)
-                engine_str += f" | Shapes: min={min_shape}, opt={opt_shape}, max={max_shape}\n"
+                min_shape, opt_shape, max_shape = engine.get_tensor_profile_shape(
+                    name, profile_index
+                )
+                engine_str += (
+                    f" | Shapes: min={min_shape}, opt={opt_shape}, max={max_shape}\n"
+                )
             else:
                 engine_str += f" | Shape: {engine.get_tensor_shape(name)}\n"
         engine_str += "\n"
 
     layers_per_profile = engine.num_layers // engine.num_optimization_profiles
-    engine_str += (
-        f"---- {layers_per_profile} Layer(s){' Per Profile' if engine.num_optimization_profiles > 1 else ''} ----\n"
-    )
+    engine_str += f"---- {layers_per_profile} Layer(s){' Per Profile' if engine.num_optimization_profiles > 1 else ''} ----\n"
     if show_layers:
         try:
             inspector = engine.create_engine_inspector()
         except AttributeError:
             G_LOGGER.warning(
                 f"Cannot show layer information because IEngineInspector is not available in this version of TensorRT ({trt.__version__})"
             )
@@ -639,15 +740,17 @@
                 if engine.num_optimization_profiles >= 1:
                     indent_level = 1
                     engine_str += f"- Profile: {profile_idx}\n"
 
                 offset = profile_idx * layers_per_profile
                 for index in range(layers_per_profile):
                     layer_info = json.loads(
-                        inspector.get_layer_information(offset + index, trt.LayerInformationFormat.JSON)
+                        inspector.get_layer_information(
+                            offset + index, trt.LayerInformationFormat.JSON
+                        )
                     )
 
                     op = "Unknown"
                     input_names, input_meta = [], TensorMetadata()
                     output_names, output_meta = [], TensorMetadata()
                     origin = "Unknown"
                     tactic = "Unknown"
@@ -668,15 +771,17 @@
                                     "BOOL": DataType.BOOL,
                                     "N/A": None,
                                 }
 
                                 for key, val in mapping.items():
                                     if key in contents:
                                         return val
-                                G_LOGGER.internal_error(f"Could not determine data type from format string: {contents}")
+                                G_LOGGER.internal_error(
+                                    f"Could not determine data type from format string: {contents}"
+                                )
                                 return None
 
                             names = []
                             meta = TensorMetadata()
                             info = layer_info.get(key)
                             if info is None:
                                 return meta
@@ -692,35 +797,55 @@
                                 )
                             return names, meta
 
                         input_names, input_meta = names_meta_from_inspector("Inputs")
                         output_names, output_meta = names_meta_from_inspector("Outputs")
                         origin = layer_info.get("Origin", "Unknown")
                         tactic = layer_info.get("TacticValue", "Unknown")
+                        # For Myelin layers, use `TacticName` instead of `TacticValue`
+                        if "TacticValue" not in layer_info:
+                            tactic = layer_info.get("TacticName", "Unknown")
+
                     else:
                         G_LOGGER.warning(
                             f"This engine was created with a profiling verbosity of: {engine.profiling_verbosity}. Some layer information may be missing. Try setting a higher profiling verbosity to see more detailed layer information. ",
                             mode=LogMode.ONCE,
                         )
                         name = layer_info
 
                     engine_str += (
                         util.indent_block(
                             util.str_from_layer(
-                                "Layer", index, name, op, input_names, input_meta, output_names, output_meta
+                                "Layer",
+                                index,
+                                name,
+                                op,
+                                input_names,
+                                input_meta,
+                                output_names,
+                                output_meta,
                             ),
                             indent_level,
                         )
                         + "\n"
                     )
 
                     if show_attrs:
-                        engine_str += util.indent_block("---- Attributes ----", indent_level + 1) + "\n"
-                        engine_str += util.indent_block(f"Origin = {origin}", indent_level + 1) + "\n"
-                        engine_str += util.indent_block(f"Tactic = {tactic}", indent_level + 1) + "\n"
+                        engine_str += (
+                            util.indent_block("---- Attributes ----", indent_level + 1)
+                            + "\n"
+                        )
+                        engine_str += (
+                            util.indent_block(f"Origin = {origin}", indent_level + 1)
+                            + "\n"
+                        )
+                        engine_str += (
+                            util.indent_block(f"Tactic = {tactic}", indent_level + 1)
+                            + "\n"
+                        )
 
                     engine_str += "\n"
 
     return util.indent_block(engine_str, level=0)
 
 
 def _get_array_on_gpu(arr, name, device_buffers, stream=None):
```

## polygraphy/common/struct.py

```diff
@@ -88,23 +88,23 @@
 
     @staticmethod
     def from_feed_dict(feed_dict):
         """
         Constructs a new TensorMetadata using information from the provided feed_dict.
 
         Args:
-            feed_dict (OrderedDict[str, numpy.ndarray]):
-                    A mapping of input tensor names to corresponding input NumPy arrays.
+            feed_dict (OrderedDict[str, Union[numpy.ndarray, torch.tensor]]):
+                    A mapping of input tensor names to corresponding input arrays.
 
         Returns:
             TensorMetadata
         """
         meta = TensorMetadata()
         for name, arr in feed_dict.items():
-            meta.add(name, arr.dtype, arr.shape)
+            meta.add(name, util.array.dtype(arr), util.array.shape(arr))
         return meta
 
     def add(self, name, dtype, shape, min_shape=None, max_shape=None, docstring=None):
         """
         Convenience function for adding entries.
 
         Args:
```

## polygraphy/comparator/compare.py

```diff
@@ -10,14 +10,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
+import copy
 import functools
 from collections import OrderedDict
 
 from polygraphy import mod, util
 from polygraphy.comparator import util as comp_util
 from polygraphy.datatype import DataType
 from polygraphy.logger import G_LOGGER, LogMode
@@ -28,15 +29,26 @@
 @mod.export()
 class OutputCompareResult:
     """
     Represents the result of comparing a single output of a single iteration
     between two runners.
     """
 
-    def __init__(self, passed, max_absdiff, max_reldiff, mean_absdiff, mean_reldiff, median_absdiff, median_reldiff, quantile_absdiff, quantile_reldiff):
+    def __init__(
+        self,
+        passed,
+        max_absdiff,
+        max_reldiff,
+        mean_absdiff,
+        mean_reldiff,
+        median_absdiff,
+        median_reldiff,
+        quantile_absdiff,
+        quantile_reldiff,
+    ):
         """
         Records the required tolerances and other statistics gathered during comparison.
 
         Args:
             passed (bool):
                     Whether the error was within acceptable limits.
             max_absdiff (float):
@@ -85,28 +97,28 @@
         return None
     elif found_name != output_name:
         exact_match = util.find_str_in_iterable(found_name, base_iter_result.keys())
         if exact_match == found_name:
             G_LOGGER.verbose(
                 f"Will not compare {found_name} with {output_name}, since the former already has an exact match: {exact_match}"
             )
-            return (
-                None  # If the found output is being compared against another output already, skip this non-exact match
-            )
+            return None  # If the found output is being compared against another output already, skip this non-exact match
         G_LOGGER.warning(
             f"Output names did not match exactly. Assuming {iter_result.runner_name} output: {found_name} corresponds to output: {output_name}"
         )
     return [found_name]
 
 
 def run_comparison(func, fail_fast, iter_result0, iter_result1, find_output_func):
     """
     Iterates over all the generated outputs and runs `func` to compare them.
     """
-    output_status = OrderedDict()  # OrderedDict[str, bool] Maps output names to whether they matched.
+    output_status = (
+        OrderedDict()
+    )  # OrderedDict[str, bool] Maps output names to whether they matched.
 
     for index, (out0_name, output0) in enumerate(iter_result0.items()):
         out1_names = util.default(find_output_func(out0_name, index, iter_result1), [])
 
         if len(out1_names) > 1:
             G_LOGGER.info(
                 f"Will attempt to compare output: '{out0_name}' [{iter_result0.runner_name}] with multiple outputs: '{list(out1_names)}' [{iter_result1.runner_name}]"
@@ -125,19 +137,23 @@
                 f"Comparing Output: '{out0_name}' (dtype={util.array.dtype(output0)}, shape={util.array.shape(output0)}) with '{out1_name}' (dtype={util.array.dtype(output1)}, shape={util.array.shape(output1)})"
             )
             with G_LOGGER.indent():
                 output_status[out0_name] = func(out0_name, output0, out1_name, output1)
                 if fail_fast and not output_status[out0_name]:
                     return output_status
 
-    mismatched_output_names = [name for name, matched in output_status.items() if not matched]
+    mismatched_output_names = [
+        name for name, matched in output_status.items() if not matched
+    ]
     if mismatched_output_names:
         G_LOGGER.error(f"FAILED | Mismatched outputs: {mismatched_output_names}")
     else:
-        G_LOGGER.finish(f"PASSED | All outputs matched | Outputs: {list(output_status.keys())}")
+        G_LOGGER.finish(
+            f"PASSED | All outputs matched | Outputs: {list(output_status.keys())}"
+        )
 
     # This is useful for catching cases were Polygraphy does something wrong with the runner output buffers
     if not output_status and (bool(iter_result0.keys()) or bool(iter_result1.keys())):
         r0_name = iter_result0.runner_name
         r0_outs = list(iter_result0.keys())
         r1_name = iter_result1.runner_name
         r1_outs = list(iter_result1.keys())
@@ -164,15 +180,15 @@
         find_output_func=None,
         check_error_stat=None,
         infinities_compare_equal=None,
         save_heatmaps=None,
         show_heatmaps=None,
         save_error_metrics_plot=None,
         show_error_metrics_plot=None,
-        error_quantile=None
+        error_quantile=None,
     ):
         """
         Creates a function that compares two IterationResults, and can be used as the `compare_func` argument
         in ``Comparator.compare_accuracy``.
 
         Args:
             check_shapes (bool):
@@ -262,15 +278,15 @@
             out1,
             out1_name,
             per_out_rtol,
             per_out_atol,
             per_out_err_stat,
             runner0_name,
             runner1_name,
-            per_out_quantile
+            per_out_quantile,
         ):
             """
             Checks whether two outputs matched.
 
             Args:
                 out0 (Union[np.array, torch.Tensor]): The first output.
                 out0_name (str): The name of the first output.
@@ -296,38 +312,46 @@
                 f"{runner0_name:35} | Output: {out0_name} (dtype={util.array.dtype(out0)}, shape={util.array.shape(out0)}):\n{util.indent_block(out0)}"
             )
             G_LOGGER.super_verbose(
                 f"{runner1_name:35} | Output: {out1_name} (dtype={util.array.dtype(out1)}, shape={util.array.shape(out1)}):\n{util.indent_block(out1)}"
             )
 
             # Check difference vs. tolerances
-            if util.array.dtype(out0) == DataType.BOOL and util.array.dtype(out1) == DataType.BOOL:
+            if (
+                util.array.dtype(out0) == DataType.BOOL
+                and util.array.dtype(out1) == DataType.BOOL
+            ):
                 absdiff = util.array.logical_xor(out0, out1)
             else:
-                absdiff = util.array.abs(util.array.subtract(comp_util.cast_up(out0), comp_util.cast_up(out1)))
+                absdiff = util.array.abs(
+                    util.array.subtract(
+                        comp_util.cast_up(out0), comp_util.cast_up(out1)
+                    )
+                )
                 if infinities_compare_equal:
                     out0_infinite = util.array.isinf(out0)
                     cond = util.array.logical_and(out0_infinite, out0 == out1)
                     absdiff = util.array.where(cond, 0, absdiff)
 
             # Add a small epsilon (2e-16) to zero values in the array to prevent NaN in relative error.
-            cast_up_out1 = comp_util.cast_up(out1)
+            out1_with_eps = copy.copy(comp_util.cast_up(out1))
 
-            if util.array.dtype(cast_up_out1).is_floating:
-                if util.array.any(cast_up_out1 == 0):
+            if util.array.dtype(out1_with_eps).is_floating:
+                if util.array.any(out1_with_eps == 0):
                     G_LOGGER.warning(
                         f"{runner1_name:35} | Output: {out1_name}: Some values are 0. "
                         f"Will add a small epsilon quantity to these when computing relative difference. "
                         f"Note that this may cause some relative differences to be extremely high. ",
                         mode=LogMode.ONCE,
                     )
                 EPSILON = 2.220446049250313e-16
-                cast_up_out1[cast_up_out1 == 0] += EPSILON
+                out1_with_eps[out1_with_eps == 0] += EPSILON
 
-            reldiff = util.array.divide(absdiff, util.array.abs(cast_up_out1))
+            # TODO: Only evaluate this if actually needed like we do for quantile_*.
+            reldiff = util.array.divide(absdiff, util.array.abs(out1_with_eps))
             min_reldiff = comp_util.compute_min(reldiff)
             max_reldiff = comp_util.compute_max(reldiff)
             mean_reldiff = comp_util.compute_mean(reldiff)
             median_reldiff = comp_util.compute_median(reldiff)
             quantile_reldiff = None
 
             min_absdiff = comp_util.compute_min(absdiff)
@@ -336,47 +360,71 @@
             median_absdiff = comp_util.compute_median(absdiff)
             quantile_absdiff = None
 
             def stat_failed(diff, tol):
                 return util.array.isnan(diff) or diff > tol
 
             if per_out_err_stat == "mean":
-                failed = stat_failed(mean_absdiff, per_out_atol) and stat_failed(mean_reldiff, per_out_rtol)
+                failed = stat_failed(mean_absdiff, per_out_atol) and stat_failed(
+                    mean_reldiff, per_out_rtol
+                )
             elif per_out_err_stat == "median":
-                failed = stat_failed(median_absdiff, per_out_atol) and stat_failed(median_reldiff, per_out_rtol)
+                failed = stat_failed(median_absdiff, per_out_atol) and stat_failed(
+                    median_reldiff, per_out_rtol
+                )
             elif per_out_err_stat == "max":
-                failed = stat_failed(max_absdiff, per_out_atol) and stat_failed(max_reldiff, per_out_rtol)
+                failed = stat_failed(max_absdiff, per_out_atol) and stat_failed(
+                    max_reldiff, per_out_rtol
+                )
             elif per_out_err_stat == "quantile":
                 quantile_reldiff = comp_util.compute_quantile(reldiff, per_out_quantile)
                 quantile_absdiff = comp_util.compute_quantile(absdiff, per_out_quantile)
-                failed = stat_failed(quantile_absdiff, per_out_atol) and stat_failed(quantile_reldiff, per_out_rtol)
+                failed = stat_failed(quantile_absdiff, per_out_atol) and stat_failed(
+                    quantile_reldiff, per_out_rtol
+                )
             else:
                 assert (
                     per_out_err_stat == "elemwise"
                 ), "This branch should be unreachable unless per_out_err_stat is 'elemwise'"
-                mismatches = (util.array.greater(absdiff, per_out_atol) | util.array.isnan(absdiff)) & (
-                    util.array.greater(reldiff, per_out_rtol) | util.array.isnan(reldiff)
+                mismatches = (
+                    util.array.greater(absdiff, per_out_atol)
+                    | util.array.isnan(absdiff)
+                ) & (
+                    util.array.greater(reldiff, per_out_rtol)
+                    | util.array.isnan(reldiff)
                 )
 
                 failed = util.array.any(mismatches)
                 try:
                     with G_LOGGER.indent():
-                        G_LOGGER.super_verbose(f"Mismatched indices:\n{util.array.argwhere(mismatches)}")
-                        G_LOGGER.extra_verbose(f"{runner0_name:35} | Mismatched values:\n{out0[mismatches]}")
-                        G_LOGGER.extra_verbose(f"{runner1_name:35} | Mismatched values:\n{out1[mismatches]}")
+                        G_LOGGER.super_verbose(
+                            lambda: f"Mismatched indices:\n{util.array.argwhere(mismatches)}"
+                        )
+                        G_LOGGER.extra_verbose(
+                            lambda: f"{runner0_name:35} | Mismatched values:\n{out0[mismatches]}"
+                        )
+                        G_LOGGER.extra_verbose(
+                            lambda: f"{runner1_name:35} | Mismatched values:\n{out1[mismatches]}"
+                        )
                 except Exception as err:
-                    G_LOGGER.warning(f"Failing to log mismatches.\nNote: Error was: {err}")
+                    G_LOGGER.warning(
+                        f"Failing to log mismatches.\nNote: Error was: {err}"
+                    )
 
             # Log information about the outputs
             hist_bin_range = (
                 min(comp_util.compute_min(out0), comp_util.compute_min(out1)),
                 max(comp_util.compute_max(out0), comp_util.compute_max(out1)),
             )
-            comp_util.log_output_stats(out0, failed, f"{runner0_name}: {out0_name}", hist_range=hist_bin_range)
-            comp_util.log_output_stats(out1, failed, f"{runner1_name}: {out1_name}", hist_range=hist_bin_range)
+            comp_util.log_output_stats(
+                out0, failed, f"{runner0_name}: {out0_name}", hist_range=hist_bin_range
+            )
+            comp_util.log_output_stats(
+                out1, failed, f"{runner1_name}: {out1_name}", hist_range=hist_bin_range
+            )
 
             G_LOGGER.info(f"Error Metrics: {out0_name}")
             with G_LOGGER.indent():
 
                 def req_tol(mean_diff, median_diff, max_diff, quantile_diff):
                     return {
                         "mean": mean_diff,
@@ -420,21 +468,31 @@
                                 use_lognorm=use_lognorm,
                             )
 
                 comp_util.log_output_stats(absdiff, failed, "Absolute Difference")
                 build_heatmaps(absdiff, min_absdiff, max_absdiff, "Absolute")
 
                 comp_util.log_output_stats(reldiff, failed, "Relative Difference")
-                build_heatmaps(reldiff, min_reldiff, max_reldiff, "Relative", use_lognorm=True)
+                build_heatmaps(
+                    reldiff, min_reldiff, max_reldiff, "Relative", use_lognorm=True
+                )
 
             G_LOGGER.extra_verbose(
-                f"Finished comparing: '{out0_name}' (dtype={util.array.dtype(out0)}, shape={util.array.shape(out0)}) [{runner0_name}] and '{out1_name}' (dtype={util.array.dtype(out1)}, shape={util.array.shape(out1)}) [{runner1_name}]"
+                lambda: f"Finished comparing: '{out0_name}' (dtype={util.array.dtype(out0)}, shape={util.array.shape(out0)}) [{runner0_name}] and '{out1_name}' (dtype={util.array.dtype(out1)}, shape={util.array.shape(out1)}) [{runner1_name}]"
             )
             return OutputCompareResult(
-                not failed, max_absdiff, max_reldiff, mean_absdiff, mean_reldiff, median_absdiff, median_reldiff, quantile_absdiff, quantile_reldiff
+                not failed,
+                max_absdiff,
+                max_reldiff,
+                mean_absdiff,
+                mean_reldiff,
+                median_absdiff,
+                median_reldiff,
+                quantile_absdiff,
+                quantile_reldiff,
             )
 
         def compare_output(iter_result0, iter_result1):
             """
             Compare the outputs of two runners from a single iteration.
 
             This function will always iterate over the output names of the first IterationResult,
@@ -467,51 +525,67 @@
 
             check_dict(rtol, "the rtol dictionary")
             check_dict(atol, "the atol dictionary")
             check_dict(check_error_stat, "the check_error_stat dictionary")
             check_dict(error_quantile, "the quantile dictionary")
 
             if not check_shapes:
-                G_LOGGER.info("Strict shape checking disabled. Will attempt to match output shapes before comparisons")
+                G_LOGGER.info(
+                    "Strict shape checking disabled. Will attempt to match output shapes before comparisons"
+                )
 
             def match(out0_name, output0, out1_name, output1):
                 per_out_atol = util.value_or_from_dict(atol, out0_name, default_atol)
                 per_out_rtol = util.value_or_from_dict(rtol, out0_name, default_rtol)
-                per_out_err_stat = util.value_or_from_dict(check_error_stat, out0_name, default_error_stat)
-                per_out_quantile = util.value_or_from_dict(error_quantile, out0_name, default_quantile)
+                per_out_err_stat = util.value_or_from_dict(
+                    check_error_stat, out0_name, default_error_stat
+                )
+                per_out_quantile = util.value_or_from_dict(
+                    error_quantile, out0_name, default_quantile
+                )
 
                 G_LOGGER.info(
                     f"Tolerance: [abs={per_out_atol:.5g}, rel={per_out_rtol:.5g}] | Checking {per_out_err_stat} error"
                 )
-                G_LOGGER.extra_verbose(f"Note: Comparing {iter_result0.runner_name} vs. {iter_result1.runner_name}")
+                G_LOGGER.extra_verbose(
+                    f"Note: Comparing {iter_result0.runner_name} vs. {iter_result1.runner_name}"
+                )
 
-                if check_shapes and util.array.shape(output0) != util.array.shape(output1):
+                if check_shapes and util.array.shape(output0) != util.array.shape(
+                    output1
+                ):
                     G_LOGGER.error(
-                        f"Will not compare outputs of different shapes. Note: Output shapes are {util.array.shape(output0)} and {util.array.shape(output1)}."
+                        f"FAILED | Output: `{out0_name}` | Will not compare outputs of different shapes.\n"
+                        f"Note: Output shapes are {util.array.shape(output0)} and {util.array.shape(output1)}."
                     )
                     G_LOGGER.error(
-                        "Note: Use --no-shape-check or set check_shapes=False to " "attempt to compare values anyway.",
+                        "Note: Use --no-shape-check or set check_shapes=False to "
+                        "attempt to compare values anyway.",
                         mode=LogMode.ONCE,
                     )
-                    outputs_matched = False
-                else:
-                    output1 = util.try_match_shape(output1, util.array.shape(output0))
-                    output0 = util.array.view(output0, DataType.from_dtype(util.array.dtype(output0)), util.array.shape(output1))
-                    outputs_matched = check_outputs_match(
-                        output0,
-                        out0_name,
-                        output1,
-                        out1_name,
-                        per_out_rtol=per_out_rtol,
-                        per_out_atol=per_out_atol,
-                        per_out_err_stat=per_out_err_stat,
-                        runner0_name=iter_result0.runner_name,
-                        runner1_name=iter_result1.runner_name,
-                        per_out_quantile=per_out_quantile
-                    )
+                    return False
+
+                output1 = util.try_match_shape(output1, util.array.shape(output0))
+                output0 = util.array.view(
+                    output0,
+                    DataType.from_dtype(util.array.dtype(output0)),
+                    util.array.shape(output1),
+                )
+                outputs_matched = check_outputs_match(
+                    output0,
+                    out0_name,
+                    output1,
+                    out1_name,
+                    per_out_rtol=per_out_rtol,
+                    per_out_atol=per_out_atol,
+                    per_out_err_stat=per_out_err_stat,
+                    runner0_name=iter_result0.runner_name,
+                    runner1_name=iter_result1.runner_name,
+                    per_out_quantile=per_out_quantile,
+                )
 
                 # Finally show summary.
                 if not outputs_matched:
                     G_LOGGER.error(
                         f"FAILED | Output: '{out0_name}' | Difference exceeds tolerance (rel={per_out_rtol}, abs={per_out_atol})"
                     )
                 else:
@@ -519,17 +593,22 @@
                         f"PASSED | Output: '{out0_name}' | Difference is within tolerance (rel={per_out_rtol}, abs={per_out_atol})"
                     )
 
                 return outputs_matched
 
             nonlocal find_output_func
             find_output_func = util.default(
-                find_output_func, functools.partial(default_find_output_func, base_iter_result=iter_result0)
+                find_output_func,
+                functools.partial(
+                    default_find_output_func, base_iter_result=iter_result0
+                ),
+            )
+            return run_comparison(
+                match, fail_fast, iter_result0, iter_result1, find_output_func
             )
-            return run_comparison(match, fail_fast, iter_result0, iter_result1, find_output_func)
 
         return compare_output
 
     @staticmethod
     def indices(index_tolerance=None, fail_fast=None):
         """
         Creates a function that compares two IterationResults containing indices, and can be used as the `compare_func` argument
@@ -597,15 +676,17 @@
                         and whether they matched. If an output name is not found, it is omitted from this dictionary.
 
             Raises:
                 PolygraphyException: If all output names are skipped, and thus no outputs are compared.
             """
 
             def match(out0_name, output0, out1_name, output1):
-                per_out_index_tol = util.value_or_from_dict(index_tolerance, out0_name, 0)
+                per_out_index_tol = util.value_or_from_dict(
+                    index_tolerance, out0_name, 0
+                )
 
                 if util.array.shape(output0) != util.array.shape(output1):
                     G_LOGGER.error("Cannot compare outputs of different shapes.")
                     return False
 
                 passed = True
                 for batch in np.ndindex(util.array.shape(output0)[:-1]):
@@ -614,49 +695,65 @@
                         out0_vals = out0_vals[:-per_out_index_tol]
                     out1_vals = output1[batch]
 
                     for index0, val0 in enumerate(out0_vals):
                         if val0 == out1_vals[index0]:
                             continue
 
-                        index1 = util.array.ravel(util.array.argwhere(out1_vals == val0))
+                        index1 = util.array.ravel(
+                            util.array.argwhere(out1_vals == val0)
+                        )
                         if util.array.size(index1) < 1:
-                            G_LOGGER.error(f"FAILED | Value: {val0} not found in output")
+                            G_LOGGER.error(
+                                f"FAILED | Value: {val0} not found in output"
+                            )
                             passed = False
                             if fail_fast:
                                 return False
                             continue
 
                         index1 = index1[0]
 
                         if abs(index1 - index0) > per_out_index_tol:
-                            G_LOGGER.error(f"FAILED | Difference exceeds index tolerance ({per_out_index_tol})")
+                            G_LOGGER.error(
+                                f"FAILED | Difference exceeds index tolerance ({per_out_index_tol})"
+                            )
                             passed = False
                             if fail_fast:
                                 return False
                             continue
 
                 # Log information about the outputs
                 hist_bin_range = (
                     min(comp_util.compute_min(output0), comp_util.compute_min(output1)),
                     max(comp_util.compute_max(output0), comp_util.compute_max(output1)),
                 )
                 comp_util.log_output_stats(
-                    output0, not passed, f"{iter_result0.runner_name}: {out0_name}", hist_range=hist_bin_range
+                    output0,
+                    not passed,
+                    f"{iter_result0.runner_name}: {out0_name}",
+                    hist_range=hist_bin_range,
                 )
                 comp_util.log_output_stats(
-                    output1, not passed, f"{iter_result1.runner_name}: {out1_name}", hist_range=hist_bin_range
+                    output1,
+                    not passed,
+                    f"{iter_result1.runner_name}: {out1_name}",
+                    hist_range=hist_bin_range,
                 )
 
                 if passed:
-                    G_LOGGER.finish(f"PASSED | Difference is within index tolerance ({per_out_index_tol})")
+                    G_LOGGER.finish(
+                        f"PASSED | Difference is within index tolerance ({per_out_index_tol})"
+                    )
                 return passed
 
             return run_comparison(
                 match,
                 fail_fast,
                 iter_result0,
                 iter_result1,
-                functools.partial(default_find_output_func, base_iter_result=iter_result0),
+                functools.partial(
+                    default_find_output_func, base_iter_result=iter_result0
+                ),
             )
 
         return compare_output
```

## polygraphy/comparator/data_loader.py

```diff
@@ -21,24 +21,114 @@
 from polygraphy.comparator.struct import RunResults
 from polygraphy.datatype import DataType
 from polygraphy.exception import DataTypeConversionException, PolygraphyException
 from polygraphy.json import save_json
 from polygraphy.logger import G_LOGGER, LogMode
 
 np = mod.lazy_import("numpy")
+torch = mod.lazy_import("torch")
+
+
+class ArraySampler:
+    def __init__(self, data_loader_backend_module, seed):
+        """
+        Args:
+            data_loader_backend_module (str):
+                    The module specifying the array type to use to generate arrays.
+                    Can be either "numpy" or "torch".
+            seed (int):
+                    The seed to use when generating random inputs.
+        """
+        self.rng = None
+        VALID_ARRAY_MODULES = ["numpy", "torch"]
+        if data_loader_backend_module not in VALID_ARRAY_MODULES:
+            G_LOGGER.critical(
+                f"Invalid `data_loader_backend_module`. Note: got: {data_loader_backend_module} but valid modules are: {VALID_ARRAY_MODULES}"
+            )
+
+        self.data_loader_backend_module = data_loader_backend_module
+
+
+        if self.data_loader_backend_module == "numpy":
+            self.rng = np.random.RandomState(seed)
+        elif self.data_loader_backend_module == "torch":
+            self.rng = torch.Generator()
+            self.rng.manual_seed(seed)
+
+
+    def sample_integer(self, shape, dtype, low, high):
+        """
+        Samples an array containing integral values in the range [low, high], inclusive
+        """
+        dtype = (
+            DataType.to_dtype(DataType.from_dtype(dtype), self.data_loader_backend_module)
+            if dtype is not None
+            else dtype
+        )
+        if self.data_loader_backend_module == "numpy":
+            return np.array(
+                self.rng.randint(low=low, high=high + 1, size=shape, dtype=dtype)
+            )
+        elif self.data_loader_backend_module == "torch":
+            return torch.randint(low, high + 1, shape, generator=self.rng, dtype=dtype)
+
+    def sample_float(self, shape, dtype, fmin, fmax):
+        """
+        Samples an array containing float values in the range [fmin, fmax], inclusive
+        """
+        # Special handling for infinite lower/upper bounds
+        # Without this, two infinities will collapse into a NaN, resulting in no infinities
+        # in the final output.
+        scale = fmax - fmin
+        shift = fmin
+        if util.is_inf(fmin):
+            scale = fmin
+            shift = 0
+        if util.is_inf(fmax):
+            scale = fmax
+
+        dtype = (
+            DataType.to_dtype(DataType.from_dtype(dtype), self.data_loader_backend_module)
+            if dtype is not None
+            else dtype
+        )
+        if self.data_loader_backend_module == "numpy":
+            return np.array(
+                (self.rng.random_sample(size=shape) * scale + shift).astype(dtype)
+            )
+        elif self.data_loader_backend_module == "torch":
+            return torch.rand(shape, generator=self.rng, dtype=dtype)
+
+    def constant_array(self, shape, dtype):
+        dtype = (
+            DataType.to_dtype(DataType.from_dtype(dtype), self.data_loader_backend_module)
+            if dtype is not None
+            else dtype
+        )
+        if self.data_loader_backend_module == "numpy":
+            return np.array(shape, dtype=dtype)
+        elif self.data_loader_backend_module == "torch":
+            return torch.tensor(shape, dtype=dtype)
 
 
 @mod.export()
 class DataLoader:
     """
     Generates synthetic input data.
     """
 
     def __init__(
-        self, seed=None, iterations=None, input_metadata=None, int_range=None, float_range=None, val_range=None
+        self,
+        seed=None,
+        iterations=None,
+        input_metadata=None,
+        int_range=None,
+        float_range=None,
+        val_range=None,
+        data_loader_backend_module=None,
     ):
         """
         Args:
             seed (int):
                     The seed to use when generating random inputs.
                     Defaults to ``util.constants.DEFAULT_SEED``.
             iterations (int):
@@ -57,14 +147,18 @@
                     the data loader should generate.
                     If either value in the tuple is None, the default will be used for that value.
                     If None is provided instead of a tuple, then the default values will be used for both the
                     minimum and maximum.
                     This can be specified on a per-input basis using a dictionary. In that case,
                     use an empty string ("") as the key to specify default range for inputs not explicitly listed.
                     Defaults to (0.0, 1.0).
+            data_loader_backend_module (str):
+                    A string denoting what module to use to construct the input data arrays. Currently supports
+                    "numpy" and "torch".
+                    Defaults to "numpy".
 
             int_range (Tuple[int]):
                     [DEPRECATED - Use val_range instead]
                     A tuple containing exactly 2 integers, indicating the minimum and maximum integer values (inclusive)
                     the data loader should generate. If either value in the tuple is None, the default will be used
                     for that value.
                     If None is provided instead of a tuple, then the default values will be used for both the
@@ -75,33 +169,44 @@
                     the data loader should generate. If either value in the tuple is None, the default will be used
                     for that value.
                     If None is provided instead of a tuple, then the default values will be used for both the
                     minimum and maximum.
         """
 
         def default_tuple(tup, default):
-            if tup is None or (not isinstance(tup, tuple) and not isinstance(tup, list)):
+            if tup is None or (
+                not isinstance(tup, tuple) and not isinstance(tup, list)
+            ):
                 return default
             new_tup = []
             for elem, default_elem in zip(tup, default):
                 new_tup.append(util.default(elem, default_elem))
             return tuple(new_tup)
 
         self.seed = util.default(seed, constants.DEFAULT_SEED)
         self.iterations = util.default(iterations, 1)
         self.user_input_metadata = util.default(input_metadata, {})
+        self.data_loader_backend_module = util.default(
+            data_loader_backend_module, "numpy"
+        )
 
         self._int_range_set = int_range is not None
         if self._int_range_set:
-            mod.warn_deprecated("The int_range parameter in DataLoader", "val_range", remove_in="0.50.0")
+            mod.warn_deprecated(
+                "The int_range parameter in DataLoader", "val_range", remove_in="0.50.0"
+            )
         self._int_range = default_tuple(int_range, (1, 25))
 
         self._float_range_set = float_range is not None
         if self._float_range_set:
-            mod.warn_deprecated("The float_range parameter in DataLoader", "val_range", remove_in="0.50.0")
+            mod.warn_deprecated(
+                "The float_range parameter in DataLoader",
+                "val_range",
+                remove_in="0.50.0",
+            )
         self._float_range = default_tuple(float_range, (-1.0, 1.0))
 
         self.input_metadata = None
         self.default_val_range = default_tuple(val_range, (0.0, 1.0))
         self.val_range = util.default(val_range, self.default_val_range)
 
         if self.user_input_metadata:
@@ -114,14 +219,15 @@
             "DataLoader",
             seed=self.seed,
             iterations=self.iterations,
             input_metadata=self.user_input_metadata or None,
             int_range=self._int_range,
             float_range=self._float_range,
             val_range=self.val_range,
+            data_loader_backend_module=self.data_loader_backend_module,
         )[0]
 
     def _get_range(self, name, cast_type):
         if cast_type == int and self._int_range_set:
             return self._int_range
         elif cast_type == float and self._float_range_set:
             return self._float_range
@@ -137,21 +243,22 @@
 
         Args:
             index (int):
                     Since this class behaves like an iterable, it takes an index parameter.
                     Generated data is guaranteed to be the same for the same index.
 
         Returns:
-            OrderedDict[str, numpy.ndarray]: A mapping of input names to input numpy buffers.
+            OrderedDict[str, Union[numpy.ndarray, torch.Tensor]]: A mapping of input names to input numpy buffers.
         """
         if index >= self.iterations:
             raise IndexError()
 
         G_LOGGER.verbose(f"Generating data using numpy seed: {self.seed + index}")
-        rng = np.random.RandomState(self.seed + index)
+
+        array_sampler = ArraySampler(self.data_loader_backend_module, self.seed + index)
 
         def get_static_shape(name, shape):
             static_shape = shape
             if util.is_shape_dynamic(shape):
                 if shape.min is not None:
                     static_shape = shape.min
                 elif shape.max is not None:
@@ -178,78 +285,91 @@
         # Note that this is a hack needed for older versions of TensorRT. Ideally, we wouldn't care
         # whether the input is a shape tensor or not.
         def is_shape_tensor(name, dtype):
             if name not in self.input_metadata or name not in self.user_input_metadata:
                 return False
 
             _, shape = self.input_metadata[name]
-            if not np.issubdtype(dtype, np.integer) or util.is_shape_dynamic(shape) or len(shape) != 1:
+            if (
+                (dtype is not None and not DataType.from_dtype(dtype).is_integral)
+                or util.is_shape_dynamic(shape)
+                or len(shape) != 1
+            ):
                 return False
 
             user_shape = self.user_input_metadata[name].shape
             # Shape of shape cannot be dynamic.
             return not util.is_shape_dynamic(user_shape) and len(user_shape) == shape[0]
 
         def generate_buffer(name, dtype, shape):
             if is_shape_tensor(name, dtype):
-                buffer = np.array(shape, dtype=dtype)
+                buffer = array_sampler.constant_array(shape, dtype)
                 G_LOGGER.info(
                     f"Assuming {name} is a shape tensor. Setting input values to: {buffer}. "
                     "If these values are not correct, please set it correctly in 'input_metadata' or by providing --input-shapes",
                     mode=LogMode.ONCE,
                 )
-            elif np.issubdtype(dtype, np.integer) or np.issubdtype(dtype, np.bool_):
-                imin, imax = self._get_range(name, cast_type=int if np.issubdtype(dtype, np.integer) else bool)
+            elif dtype is not None and (
+                DataType.from_dtype(dtype).is_integral
+                or DataType.from_dtype(dtype) == DataType.BOOL
+            ):
+                imin, imax = self._get_range(
+                    name,
+                    cast_type=int if DataType.from_dtype(dtype).is_integral else bool,
+                )
                 G_LOGGER.verbose(
                     f"Input tensor: {name} | Generating input data in range: [{imin}, {imax}]",
                     mode=LogMode.ONCE,
                 )
                 # high is 1 greater than the max int drawn.
-                buffer = rng.randint(low=imin, high=imax + 1, size=shape, dtype=dtype)
+                buffer = array_sampler.sample_integer(shape, dtype, imin, imax)
             else:
                 fmin, fmax = self._get_range(name, cast_type=float)
                 G_LOGGER.verbose(
                     f"Input tensor: {name} | Generating input data in range: [{fmin}, {fmax}]",
                     mode=LogMode.ONCE,
                 )
 
-                # Special handling for infinite lower/upper bounds
-                # Without this, two inifinities will collapse into a NaN, resulting in no inifinities
-                # in the final output.
-                scale = fmax - fmin
-                shift = fmin
-                if util.is_inf(fmin):
-                    scale = fmin
-                    shift = 0
-                if util.is_inf(fmax):
-                    scale = fmax
+                buffer = array_sampler.sample_float(shape, dtype, fmin, fmax)
 
-                buffer = (rng.random_sample(size=shape) * scale + shift).astype(dtype)
-
-            buffer = np.array(buffer)  # To handle scalars, since the above functions return a float if shape is ().
             return buffer
 
         if self.input_metadata is None and self.user_input_metadata is not None:
             self.input_metadata = self.user_input_metadata
 
         buffers = OrderedDict()
         for name, (dtype, shape) in self.input_metadata.items():
             try:
-                dtype = DataType.to_dtype(DataType.from_dtype(dtype), "numpy") if dtype is not None else None
+                dtype = (
+                    DataType.to_dtype(
+                        DataType.from_dtype(dtype), self.data_loader_backend_module
+                    )
+                    if dtype is not None
+                    else None
+                )
             except DataTypeConversionException:
                 G_LOGGER.critical(
-                    f"Could not convert data type: {dtype} to NumPy, so the default data loader cannot generate a NumPy array for input: {name}. "
+                    f"Could not convert data type: {dtype} to {self.data_loader_backend_module}, so the default data loader cannot generate a {self.data_loader_backend_module} array for input: {name}. "
                     f"Please use a custom data loader to provide inputs. "
                 )
             if name in self.user_input_metadata:
                 user_dtype, user_shape = self.user_input_metadata[name]
 
                 dtype = util.default(user_dtype, dtype)
-                dtype = DataType.to_dtype(DataType.from_dtype(dtype), "numpy") if dtype is not None else None
-                is_valid_shape_override = user_shape is not None and util.is_valid_shape_override(user_shape, shape)
+                dtype = (
+                    DataType.to_dtype(
+                        DataType.from_dtype(dtype), self.data_loader_backend_module
+                    )
+                    if dtype is not None
+                    else None
+                )
+                is_valid_shape_override = (
+                    user_shape is not None
+                    and util.is_valid_shape_override(user_shape, shape)
+                )
 
                 if util.is_shape_dynamic(user_shape):
                     G_LOGGER.warning(
                         f"Input tensor: {name} [shape={shape}] | Provided input shape: {user_shape} is dynamic.\nDynamic shapes cannot be used to generate inference data. Will use default shape instead.\nTo avoid this, please provide a fixed shape to the data loader. "
                     )
                 elif not is_valid_shape_override and not is_shape_tensor(name, dtype):
                     G_LOGGER.warning(
@@ -262,15 +382,17 @@
             static_shape = get_static_shape(name, shape)
             buffers[name] = generate_buffer(name, dtype, shape=static_shape)
 
         # Warn about unused metadata
         for name in self.user_input_metadata.keys():
             if name not in self.input_metadata:
                 msg = f"Input tensor: {name} | Metadata was provided, but the input does not exist in one or more runners."
-                close_match = util.find_str_in_iterable(name, self.input_metadata.keys())
+                close_match = util.find_str_in_iterable(
+                    name, self.input_metadata.keys()
+                )
                 if close_match:
                     msg += f"\nMaybe you meant to set: {close_match}?"
                 G_LOGGER.warning(msg)
 
         # Warn about unused val_range
         if not isinstance(self.val_range, tuple):
             util.check_sequence_contains(
@@ -301,17 +423,21 @@
         """
         if iteration >= len(self.cache):
             raise IndexError()
 
         # Attempts to match existing input buffers to the requested input_metadata
         def coerce_cached_input(index, name, dtype, shape):
             cached_feed_dict = self.cache[iteration]
-            cached_name = util.find_str_in_iterable(name, cached_feed_dict.keys(), index)
+            cached_name = util.find_str_in_iterable(
+                name, cached_feed_dict.keys(), index
+            )
             if cached_name is None:
-                G_LOGGER.critical(f"Input tensor: {name} | Does not exist in the data loader cache.")
+                G_LOGGER.critical(
+                    f"Input tensor: {name} | Does not exist in the data loader cache."
+                )
 
             if cached_name != name:
                 G_LOGGER.warning(
                     f"Input tensor: {name} | Buffer name ({cached_name}) does not match expected input name ({name})."
                 )
 
             buffer = cached_feed_dict[cached_name]
@@ -328,43 +454,49 @@
                 else:
                     type_info = None
                     if dtype.is_integral:
                         type_info = np.iinfo(np_type)
                     elif dtype.is_floating:
                         type_info = np.finfo(np_type)
 
-                    if type_info is not None and util.array.any((buffer < type_info.min) | (buffer > type_info.max)):
+                    if type_info is not None and util.array.any(
+                        (buffer < type_info.min) | (buffer > type_info.max)
+                    ):
                         G_LOGGER.warning(
                             f"Some values in this input are out of range of {dtype}. Unexpected behavior may ensue!"
                         )
                 buffer = util.array.cast(buffer, dtype)
 
             if not util.is_valid_shape_override(util.array.shape(buffer), shape):
                 G_LOGGER.warning(
                     f"Input tensor: {name} | Buffer shape ({util.array.shape(buffer)}) does not match expected input shape ({shape}). "
                     f"Attempting to transpose/reshape. "
                 )
                 buffer = util.try_match_shape(buffer, shape)
 
-            if util.array.dtype(buffer) != dtype or not util.is_valid_shape_override(util.array.shape(buffer), shape):
+            if util.array.dtype(buffer) != dtype or not util.is_valid_shape_override(
+                util.array.shape(buffer), shape
+            ):
                 G_LOGGER.critical(
                     f"Input tensor: {name} | Cannot reuse input data due to mismatch in shape or data type.\n"
                     f"Note: Cached input: [dtype={util.array.dtype(buffer)}, shape={util.array.shape(buffer)}], "
                     f"Requested input: [dtype={dtype}, shape={shape}]"
                 )
             return buffer
 
         feed_dict = OrderedDict()
 
         # Reload from data loader if needed
         data_loader_feed_dict = None
 
         for index, (name, (dtype, shape)) in enumerate(self.input_metadata.items()):
             try:
-                buffer = coerce_cached_input(index, name, DataType.from_dtype(dtype), shape)
+                buffer = coerce_cached_input(
+                    index, name, DataType.from_dtype(dtype), shape
+                )
             except PolygraphyException:
                 G_LOGGER.warning(
                     f"Could not use buffer previously cached from data loader for input: {name}. Attempting to reload inputs from the data loader.\nNote that this will only work if the data loader supports random access.\nPlease refer to warnings above for details on why the previously generated input buffer didn't work. "
                 )
                 try:
                     if data_loader_feed_dict is None:
                         data_loader_feed_dict = self.data_loader[iteration]
```

## polygraphy/datatype/datatype.py

```diff
@@ -86,25 +86,30 @@
         "FLOAT64": DataTypeEntry("float64", 8, _DataTypeKind.FLOATING_POINT),
         "FLOAT32": DataTypeEntry("float32", 4, _DataTypeKind.FLOATING_POINT),
         "FLOAT16": DataTypeEntry("float16", 2, _DataTypeKind.FLOATING_POINT),
         "INT16": DataTypeEntry("int16", 2, _DataTypeKind.INTEGRAL),
         "INT32": DataTypeEntry("int32", 4, _DataTypeKind.INTEGRAL),
         "INT64": DataTypeEntry("int64", 8, _DataTypeKind.INTEGRAL),
         "INT8": DataTypeEntry("int8", 1, _DataTypeKind.INTEGRAL),
+        "INT4": DataTypeEntry("int4", 0.5, _DataTypeKind.INTEGRAL),
         "UINT16": DataTypeEntry("uint16", 2, _DataTypeKind.INTEGRAL),
         "UINT32": DataTypeEntry("uint32", 4, _DataTypeKind.INTEGRAL),
         "UINT64": DataTypeEntry("uint64", 8, _DataTypeKind.INTEGRAL),
         "UINT8": DataTypeEntry("uint8", 1, _DataTypeKind.INTEGRAL),
         "BOOL": DataTypeEntry("bool", 1, _DataTypeKind._OTHER),
         "STRING": DataTypeEntry("string", 0, _DataTypeKind._OTHER),
         "BFLOAT16": DataTypeEntry("bfloat16", 2, _DataTypeKind.FLOATING_POINT),
         "FLOAT8E4M3FN": DataTypeEntry("float8e4m3fn", 1, _DataTypeKind.FLOATING_POINT),
-        "FLOAT8E4M3FNUZ": DataTypeEntry("float8e4m3fnuz", 1, _DataTypeKind.FLOATING_POINT),
+        "FLOAT8E4M3FNUZ": DataTypeEntry(
+            "float8e4m3fnuz", 1, _DataTypeKind.FLOATING_POINT
+        ),
         "FLOAT8E5M2": DataTypeEntry("float8e5m2", 1, _DataTypeKind.FLOATING_POINT),
-        "FLOAT8E5M2FNUZ": DataTypeEntry("float8e5m2fnuz", 1, _DataTypeKind.FLOATING_POINT),
+        "FLOAT8E5M2FNUZ": DataTypeEntry(
+            "float8e5m2fnuz", 1, _DataTypeKind.FLOATING_POINT
+        ),
     }
 
     @staticmethod
     def from_dtype(dtype, source_module=None):
         """
         Converts a data type from any known external libraries to a corresponding
         Polygraphy data type.
@@ -164,15 +169,17 @@
         Returns:
             Any: The corresponding data type from the target module.
 
         Raises:
             PolygraphyException: If the data type could not be converted.
         """
         if not isinstance(dtype, DataTypeEntry):
-            G_LOGGER.internal_error(f"Received input of type other than DataType: {dtype}")
+            G_LOGGER.internal_error(
+                f"Received input of type other than DataType: {dtype}"
+            )
             return dtype
 
         if target_module not in DataType._EXPORTER_FUNCS:
             G_LOGGER.critical(
                 f"Could not find target module: {target_module} in known exporters. "
                 f"Note: Exporter functions have been registered for the following modules: {list(DataType._EXPORTER_FUNCS.keys())}"
             )
@@ -186,15 +193,16 @@
 
 
 def register_dtype_importer(source_module):
     """
     Registers an importer function with the DataType class.
 
     IMPORTANT: You *must* ensure that the importer function does not attempt to automatically install
-    or import modules which are not already installed. `mod.has_mod` is an easy way to guard the code against this.
+    or import modules which are not already installed.
+    With a lazily imported module, `module.is_installed()/is_importable()` is an easy way to guard the code against this.
     We do not want to automatically install heavy modules like PyTorch or TensorRT just for the sake of DataType.
 
     For example:
     ::
 
         @register_dtype_importer("numpy")
         def func(dtype):
```

## polygraphy/datatype/numpy.py

```diff
@@ -47,15 +47,15 @@
 
     Args:
         numpy_type (np.dtype): The NumPy data type.
 
     Returns:
         DataType: The Polygraphy data type.
     """
-    if not mod.has_mod("numpy"):
+    if not np.is_installed() or not np.is_importable():
         return None
 
     try:
         dtype = np.dtype(numpy_type)
     except TypeError:
         return None
```

## polygraphy/datatype/onnx.py

```diff
@@ -56,15 +56,15 @@
 
     Args:
         onnx_type (onnx.TensorProto.DataType): The ONNX data type.
 
     Returns:
         DataType: The Polygraphy data type.
     """
-    if not mod.has_mod("onnx"):
+    if not onnx.is_installed() or not onnx.is_importable():
         return None
 
     return _get_mapping().get(onnx_type)
 
 
 @register_dtype_exporter("onnx")
 def from_datatype(self):
```

## polygraphy/datatype/tensorrt.py

```diff
@@ -12,15 +12,19 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 from polygraphy import mod, util
-from polygraphy.datatype.datatype import DataType, register_dtype_importer, register_dtype_exporter
+from polygraphy.datatype.datatype import (
+    DataType,
+    register_dtype_importer,
+    register_dtype_exporter,
+)
 
 trt = mod.lazy_import("tensorrt>=8.5")
 
 
 def _get_mapping():
     DATATYPE_FROM_TENSORRT = {
         trt.float32: DataType.FLOAT32,
@@ -28,14 +32,15 @@
         trt.int32: DataType.INT32,
         trt.int8: DataType.INT8,
         util.try_getattr(trt, "int64"): DataType.INT64,
         util.try_getattr(trt, "uint8"): DataType.UINT8,
         util.try_getattr(trt, "bool"): DataType.BOOL,
         util.try_getattr(trt, "bfloat16"): DataType.BFLOAT16,
         util.try_getattr(trt, "fp8"): DataType.FLOAT8E4M3FN,
+        util.try_getattr(trt, "int4"): DataType.INT4,
     }
     if None in DATATYPE_FROM_TENSORRT:
         del DATATYPE_FROM_TENSORRT[None]
 
     return DATATYPE_FROM_TENSORRT
 
 
@@ -46,15 +51,15 @@
 
     Args:
         tensorrt_type (tensorrt.DataType): The TensorRT data type.
 
     Returns:
         DataType: The Polygraphy data type.
     """
-    if not mod.has_mod("tensorrt"):
+    if not trt.is_installed() or not trt.is_importable():
         return None
 
     return _get_mapping().get(tensorrt_type)
 
 
 @register_dtype_exporter("tensorrt")
 def from_datatype(self):
```

## polygraphy/datatype/torch.py

```diff
@@ -43,15 +43,15 @@
 
     Args:
         torch_type (torch.dtype): The PyTorch data type.
 
     Returns:
         DataType: The Polygraphy data type.
     """
-    if not mod.has_mod("torch"):
+    if not torch.is_installed() or not torch.is_importable():
         return None
 
     return _get_mapping().get(torch_type)
 
 
 @register_dtype_exporter("torch")
 def from_datatype(self):
```

## polygraphy/json/serde.py

```diff
@@ -182,15 +182,17 @@
         type_name = dct.get(constants.TYPE_MARKER)
         if type_name is not None:
             if type_name not in self.polygraphy_registered:
                 user_type_name = {
                     "Tensor": "torch.Tensor",
                     "ndarray": "np.ndarray",
                 }.get(type_name, type_name)
-                G_LOGGER.critical(f"Could not decode serialized type: {user_type_name}. This could be because a required module is missing. ")
+                G_LOGGER.critical(
+                    f"Could not decode serialized type: {user_type_name}. This could be because a required module is missing. "
+                )
             return self.polygraphy_registered[type_name](dct)
 
         return dct
 
 
 NUMPY_REGISTRATION_SUCCESS = False
 TORCH_REGISTRATION_SUCCESS = False
@@ -205,15 +207,15 @@
     This needs to be attempted multiple times because dependencies may become available in the
     middle of execution - for example, if using dependency auto-installation.
     """
 
     @functools.wraps(func)
     def wrapped(*args, **kwargs):
         global NUMPY_REGISTRATION_SUCCESS
-        if not NUMPY_REGISTRATION_SUCCESS and mod.has_mod("numpy"):
+        if not NUMPY_REGISTRATION_SUCCESS and np.is_installed() and np.is_importable():
             # We define this alongside load_json/save_json so that it is guaranteed to be
             # imported before we need to encode/decode NumPy arrays.
             @Encoder.register(np.ndarray)
             def encode(array):
                 outfile = io.BytesIO()
                 np.save(outfile, array, allow_pickle=False)
                 outfile.seek(0)
@@ -239,15 +241,15 @@
                 if isinstance(arr, np.ndarray):
                     return arr
                 return list(arr.values())[0]  # For backwards compatibility
 
             NUMPY_REGISTRATION_SUCCESS = True
 
         global TORCH_REGISTRATION_SUCCESS
-        if not TORCH_REGISTRATION_SUCCESS and mod.has_mod("torch"):
+        if not TORCH_REGISTRATION_SUCCESS and torch.is_installed() and torch.is_importable():
 
             @Encoder.register(torch.Tensor)
             def encode(tensor):
                 outfile = io.BytesIO()
                 torch.save(tensor, outfile)
                 outfile.seek(0)
                 data = base64.b64encode(outfile.read()).decode()
```

## polygraphy/mod/importer.py

```diff
@@ -19,14 +19,20 @@
 import importlib
 import importlib.util
 import os
 import subprocess as sp
 import sys
 from typing import List
 
+try:
+    # Available in Python 3.8+
+    import importlib.metadata
+except ModuleNotFoundError:
+    pass
+
 from polygraphy import constants
 from polygraphy.mod import util as mod_util
 
 # Tracks all of Polygraphy's lazy imports, excluding internal ones.
 _all_external_lazy_imports = set()
 
 # Sometimes the Python package name differs from the module name.
@@ -91,21 +97,35 @@
                 Defaults to [].
 
     Returns:
         LazyModule:
                 A lazily loaded module. When an attribute is first accessed,
                 the module will be imported.
     """
+
+    def issue_wrong_version_error(installed_version, version):
+        from polygraphy.logger import G_LOGGER, LogMode
+
+        G_LOGGER.error(
+            f"Module: '{name}' version '{installed_version}' is installed, but version '{version}' is required.\n"
+            f"Please install the required version or set POLYGRAPHY_AUTOINSTALL_DEPS=1 in your environment variables "
+            f"to allow Polygraphy to do so automatically.\n"
+            f"Attempting to continue with the currently installed version of this module, but note that this may cause errors!",
+            mode=LogMode.ONCE,
+        )
+
     VERSION_CHARS = ["=", ">", "<"]
 
     log = True if log is None else log
     requires = [] if requires is None else requires
 
     def split_name_version(inp):
-        version_char_indices = [inp.index(char) for char in VERSION_CHARS if char in inp]
+        version_char_indices = [
+            inp.index(char) for char in VERSION_CHARS if char in inp
+        ]
         if not version_char_indices:
             return inp, None
 
         min_index = min(version_char_indices)
         return inp[:min_index], inp[min_index:]
 
     name, version = split_name_version(name)
@@ -116,40 +136,54 @@
 
     def import_mod():
         from polygraphy import config
         from polygraphy.logger import G_LOGGER, LogMode
 
         def install_mod(install_name, install_version, raise_error=True):
             modname = install_name.split(".")[0]
-            pkg = pkg_name if pkg_name is not None else _PKG_NAME_FROM_MODULE.get(modname, modname)
-            extra_flags = install_flags if install_flags is not None else _EXTRA_FLAGS_FOR_MODULE.get(modname, [])
+            pkg = (
+                pkg_name
+                if pkg_name is not None
+                else _PKG_NAME_FROM_MODULE.get(modname, modname)
+            )
+            extra_flags = (
+                install_flags
+                if install_flags is not None
+                else _EXTRA_FLAGS_FOR_MODULE.get(modname, [])
+            )
 
             def fail():
                 log_func = G_LOGGER.critical if raise_error else G_LOGGER.warning
-                log_func(f"Could not automatically install required module: {pkg}. Please install it manually.")
+                log_func(
+                    f"Could not automatically install required module: {pkg}. Please install it manually."
+                )
 
             if config.ASK_BEFORE_INSTALL:
                 res = None
                 while res not in ["y", "n"]:
-                    res = input(f"Automatically install '{pkg}' (version: {install_version or 'any'}) ([Y]/n)? ")
+                    res = input(
+                        f"Automatically install '{pkg}' (version: {install_version or 'any'}) ([Y]/n)? "
+                    )
                     res = res.strip()[:1].lower() or "y"
 
                 if res == "n":
                     fail()
 
             if install_version == LATEST_VERSION:
                 extra_flags.append("--upgrade")
             elif install_version is not None:
                 pkg += install_version
 
             cmd = config.INSTALL_CMD + [pkg] + extra_flags
             G_LOGGER.info(f"Running installation command: {' '.join(cmd)}")
             status = sp.run(cmd, stdout=sp.PIPE, stderr=sp.PIPE)
             if status.returncode != 0:
-                G_LOGGER.error(f"Error during installation:\n{constants.TAB}{status.stderr.decode()}")
+                G_LOGGER.error(
+                    f"Error during installation:\n{constants.TAB}{status.stderr.decode()}"
+                )
                 fail()
 
             mod = importlib.import_module(install_name)
             return mod
 
         mod = None
         try:
@@ -178,25 +212,23 @@
             ):
                 if config.AUTOINSTALL_DEPS:
                     G_LOGGER.info(
                         f"Note: Module: '{install_name}' version '{installed_mod.__version__}' is installed, but version '{install_version}' is required.\n"
                         f"Attempting to upgrade now."
                     )
                     # We can try to use the other version if install fails, so this is non-fatal.
-                    installed_mod = install_mod(install_name, install_version, raise_error=False)
+                    installed_mod = install_mod(
+                        install_name, install_version, raise_error=False
+                    )
                     if install_name == name:
                         mod = installed_mod
 
                 elif install_version != LATEST_VERSION:
-                    G_LOGGER.error(
-                        f"Module: '{install_name}' version '{installed_mod.__version__}' is installed, but version '{install_version}' is required.\n"
-                        f"Please install the required version or set POLYGRAPHY_AUTOINSTALL_DEPS=1 in your environment variables "
-                        f"to allow Polygraphy to do so automatically.\n"
-                        f"Attempting to continue with the currently installed version of this module, but note that this may cause errors!",
-                        mode=LogMode.ONCE,
+                    issue_wrong_version_error(
+                        installed_mod.__version__, install_version
                     )
 
         if log:
             G_LOGGER.module_info(mod)
 
         return mod
 
@@ -215,27 +247,73 @@
             module = self.__polygraphy_import_mod()
             return getattr(module, name)
 
         def __setattr__(self, name, value):
             module = self.__polygraphy_import_mod()
             return setattr(module, name, value)
 
+        def is_installed(self):
+            """
+            Checks whether any version of this module is installed.
+            The module will not be imported by this method.
+
+            Returns:
+                bool: Whether the module is installed.
+            """
+            global importlib
+
+            try:
+                return name in sys.modules or (
+                    importlib.util.find_spec(name) is not None
+                )
+            except:
+                return False
+
+        def is_importable(self):
+            """
+            Checks whether this module is importable. Note that a module may be installed but not importable.
+
+            Returns:
+                bool: Whether the module is importable.
+            """
+            try:
+                importlib.import_module(name)
+                return True
+            except:
+                return False
+
     return LazyModule()
 
 
 def has_mod(modname):
     """
-    Checks whether a module is installed.
+    Checks whether a module is installed without importing the module.
 
     Args:
         modname (str): The name of the module to check.
 
     Returns:
         bool: Whether the module is installed.
     """
+    import warnings
+
+    import polygraphy
+    from polygraphy.logger import G_LOGGER
+
+    remove_in = "0.50.0"
+    if mod_util.version(polygraphy.__version__) >= mod_util.version(remove_in):
+        G_LOGGER.internal_error(
+            f"has_mod should have been removed in version: {remove_in}"
+        )
+    warnings.warn(
+        f"has_mod is deprecated and will be removed in Polygraphy {remove_in}",
+        DeprecationWarning,
+        stacklevel=3,
+    )
+
     try:
         return modname in sys.modules or (importlib.util.find_spec(modname) is not None)
     except ValueError:
         return False
 
 
 def autoinstall(lazy_mod):
@@ -288,13 +366,11 @@
         try:
             mod = importlib.import_module(modname)
             return getattr(mod, name)
         except Exception as err:
             ext = os.path.splitext(path)[1]
             err_msg = f"Could not import symbol: {name} from script: {path}"
             if ext != ".py":
-                err_msg += (
-                    f"\nThis could be because the extension of the file is not '.py'. Note: The extension is: {ext}"
-                )
+                err_msg += f"\nThis could be because the extension of the file is not '.py'. Note: The extension is: {ext}"
             err_msg += f"\nNote: Error was: {err}"
             err_msg += f"\nNote: sys.path was: {sys.path}"
             G_LOGGER.critical(err_msg)
```

## polygraphy/tools/_main.py

```diff
@@ -13,15 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from polygraphy import mod
 
 
 @mod.export()
-def main():
+def main(run_opts = None):
     """
     The Polygraphy CLI Toolkit
 
     Includes various subtools that can assist with prototyping and
     debugging inference with deep learning models. See the help output
     for details.
     """
@@ -47,15 +47,18 @@
 
     subparsers = parser.add_subparsers(title="Tools", dest="tools")
     subparsers.required = True
 
     for tool in TOOL_REGISTRY:
         tool.setup_parser(subparsers)
 
-    args, unknown = parser.parse_known_args()
+    if run_opts is not None:
+        args, unknown = parser.parse_known_args(run_opts)
+    else:
+        args, unknown = parser.parse_known_args()
 
     if unknown:
         G_LOGGER.error(f"Unrecognized Options: {unknown}")
         return 1
 
     selected_tool = args.subcommand
     show_start_end_logging = False
```

## polygraphy/tools/registry.py

```diff
@@ -52,13 +52,14 @@
 try_register_tool("polygraphy.tools.convert", "Convert")
 try_register_tool("polygraphy.tools.inspect", "Inspect")
 try_register_tool("polygraphy.tools.check", "Check")
 try_register_tool("polygraphy.tools.surgeon", "Surgeon")
 try_register_tool("polygraphy.tools.template", "Template")
 try_register_tool("polygraphy.tools.debug", "Debug")
 try_register_tool("polygraphy.tools.data", "Data")
+try_register_tool("polygraphy.tools.plugin", "Plugin")
 
 # Check that tool names are unique
 tool_names = [tool.name for tool in TOOL_REGISTRY]
 duplicates = {name for name in tool_names if tool_names.count(name) > 1}
 if duplicates:
     G_LOGGER.internal_error(f"Multiple tools have the same name. Duplicate tool names found: {duplicates}")
```

## polygraphy/tools/sparse.py

```diff
@@ -40,14 +40,16 @@
         # map: tensor name -> producer node object
         self.tname2producer = dict()
         for n in g.node:
             for t in n.output:
                 self.tname2producer[t] = n
 
         self.prune_infos = dict()
+        self.sparse_tensors = set()
+        self.weights_skip = set()
 
     # Look back through Q/DQ/Cast nodes
     def __tensor(self, t, axis):
         if t in self.w_name2obj:
             G_LOGGER.super_verbose(f"Tracking weight: ({t})")
             self.prune_infos[t] = PruneInfo(t, axis)
             return
@@ -146,14 +148,19 @@
         prune_infos = list(self.prune_infos.values())
         count = len(prune_infos)
         final_prune_infos = []
         for i in range(count):
             pinfo = prune_infos[i]
             G_LOGGER.super_verbose(f"Processing tensor {i + 1}/{count}: {pinfo}")
             t = self.w_name2obj[pinfo.name]
+            if t.name in self.weights_skip:
+                G_LOGGER.warning(
+                    f"Skipping tensor: {t.name} since it was marked to skip pruning"
+                )
+                continue
             supported_dtypes = [
                 onnx.TensorProto.FLOAT,
                 onnx.TensorProto.FLOAT16,
                 onnx.TensorProto.BFLOAT16,
             ]
             if not t.data_type in supported_dtypes:
                 G_LOGGER.warning(
@@ -183,15 +190,17 @@
 
         if check:
             G_LOGGER.start(f"Checking the sparsity pattern of {count} tensors.")
             for i in range(count):
                 pinfo = prune_infos[i]
                 tensor = self.w_name2obj[pinfo.name]
                 G_LOGGER.extra_verbose(f"Checking tensor {i + 1}/{count}: {pinfo.name}")
-                process_tensor(pinfo, tensor, True)
+                is_sparse = process_tensor(pinfo, tensor, True)
+                if is_sparse:
+                    self.sparse_tensors.add(tensor.name)
             G_LOGGER.finish(f"Finished checking {count} tensors. ")
             return None
         else:
             G_LOGGER.start(f"Pruning {count} tensors.")
             new_w_name2obj = dict()
             for i in range(count):
                 pinfo = prune_infos[i]
@@ -199,15 +208,16 @@
                 G_LOGGER.extra_verbose(f"Pruning tensor {i+ 1}/{count}: {pinfo.name}")
                 new_t = process_tensor(pinfo, tensor, False)
                 new_w_name2obj[new_t.name] = new_t
             G_LOGGER.finish(f"Finished pruning {count} tensors. ")
 
             return build_new_model(self.model, new_w_name2obj)
 
-    def prune(self):
+    def prune(self, weights_skip=set()):
+        self.weights_skip = weights_skip
         return self.process(False)
 
     def check(self):
         self.process(True)
 
 
 def process_bf16_tensor(tensor, outer, pdim, pstride, check):
@@ -240,15 +250,15 @@
                             v0_zero = 1 if bf16_data_0 == 0 else 0
                             v1_zero = 1 if bf16_data_1 == 0 else 0
                             return v0_zero + v1_zero
 
                         zeros = bf16_zeros_in_int32(i32_data_0) + bf16_zeros_in_int32(i32_data_0)
                     if zeros < 2:
                         G_LOGGER.warning(f"Found non-sparse tensor: {tensor.name}")
-                        return tensor
+                        return False
                 else:
                     if is_raw_data:
                         # data is 8bit array, bf16 is 16bit
                         # the index is doubled, and we need twice change for one bf16 value
                         data[short2long(1) * 2] = 0
                         data[short2long(1) * 2 + 1] = 0
                         data[short2long(2) * 2] = 0
@@ -256,15 +266,15 @@
                     else:
                         # data is 32bit array, bf16 is 16bit
                         # We use the index but only need to change one value
                         data[short2long(0)] = 0
 
     if check:
         G_LOGGER.info(f"Found sparse tensor: {tensor.name}")
-        return tensor
+        return True
     else:
         if is_raw_data:
             tensor.raw_data = bytes(data)
         return tensor
 
 
 def process_tensor(pinfo, tensor, check):
@@ -305,24 +315,24 @@
                 vals_abs[min0_idx] = sys.float_info.max
                 min1_vabs = min(vals_abs)
                 min1_idx = vals_abs.index(min1_vabs)
 
                 if check:
                     if min0_vabs != 0 or min1_vabs != 0:
                         G_LOGGER.warning(f"Found non-sparse tensor: {tensor.name}")
-                        return tensor
+                        return False
                 else:
                     min0_idx = short2long(min0_idx)
                     min1_idx = short2long(min1_idx)
                     np.put(data, min0_idx, 0)
                     np.put(data, min1_idx, 0)
 
     if check:
         G_LOGGER.info(f"Found sparse tensor: {tensor.name}")
-        return tensor
+        return True
     else:
         # pack raw data pack and then push to the model
         data = data.reshape(dims)
         return onnx_numpy_helper.from_array(data, name=tensor.name)
 
 
 def build_new_model(m, new_w_name2obj):
```

## polygraphy/tools/args/backend/trt/config.py

```diff
@@ -316,14 +316,20 @@
         self.group.add_argument(
             "--refittable",
             help="Enable the engine to be refitted with new weights after it is built.",
             action="store_true",
             default=None,
         )
         self.group.add_argument(
+            "--strip-plan",
+            help="Builds the engine with the refittable weights stripped.",
+            action="store_true",
+            default=None,
+        )
+        self.group.add_argument(
             "--use-dla",
             help="[EXPERIMENTAL] Use DLA as the default device type",
             action="store_true",
             default=None,
         )
         self.group.add_argument(
             "--allow-gpu-fallback",
@@ -390,14 +396,30 @@
             "in the trt.QuantizationFlag enum, and are case-insensitive. "
             "If no arguments are provided, e.g. '--quantization-flags', then all quantization flags are disabled. "
             "Defaults to TensorRT's default quantization flags.",
             nargs="*",
             default=None,
         )
 
+        self.group.add_argument(
+            "--profiling-verbosity",
+            help="The verbosity of NVTX annotations in the generated engine."
+            "Values come from the names of values in the `trt.ProfilingVerbosity` enum and are case-insensitive. "
+            "For example, `--profiling-verbosity detailed`. "
+            "Defaults to 'verbose'.",
+            default=None,
+        )
+
+        self.group.add_argument(
+            "--weight-streaming",
+            help="Build a weight streamable engine. Must be set with --strongly-typed. The weight streaming amount can be set with --weight-streaming-budget.",
+            action="store_true",
+            default=None
+        )
+
         if self._allow_engine_capability:
             self.group.add_argument(
                 "--engine-capability",
                 help="The desired engine capability. "
                 "Possible values come from the names of the values in the trt.EngineCapability enum and are case-insensitive. ",
                 default=None,
             )
@@ -439,22 +461,25 @@
             use_dla (bool): Whether to enable DLA.
             allow_gpu_fallback (bool): Whether to allow GPU fallback when DLA is enabled.
             memory_pool_limits (Dict[str, int]): Mapping of strings representing memory pool enum values to memory limits in bytes.
             engine_capability (str): The desired engine capability.
             direct_io (bool): Whether to disallow reformatting layers at network input/output tensors which have user-specified formats.
             preview_features (List[str]): Names of preview features to enable.
             refittable (bool): Whether the engine should be refittable.
+            strip_plan (bool): Whether the engine should be built with the refittable weights stripped.
             builder_optimization_level (int): The builder optimization level.
             hardware_compatibility_level (str): A string representing a hardware compatibility level enum value.
+            profiling_verbosity (str): A string representing a profiling verbosity level enum value.
             max_aux_streams (int): The maximum number of auxiliary streams that TensorRT is allowed to use.
             version_compatible (bool): Whether or not to build a TensorRT forward-compatible.
             exclude_lean_runtime (bool): Whether to exclude the lean runtime from a version compatible plan.
             quantization_flags (List[str]): Names of quantization flags to enable.
             error_on_timing_cache_miss (bool): Whether to emit error when a tactic being timed is not present in the timing cache.
             disable_compilation_cache (bool): Whether to disable caching JIT-compiled code.
+            weight_streaming (bool): Whether to enable weight streaming for the TensorRT Engine.
         """
 
         trt_min_shapes = args_util.get(args, "trt_min_shapes", default=[])
         trt_max_shapes = args_util.get(args, "trt_max_shapes", default=[])
         trt_opt_shapes = args_util.get(args, "trt_opt_shapes", default=[])
 
         default_shapes = TensorMetadata()
@@ -473,14 +498,15 @@
         self.precision_constraints = args_util.get(args, "precision_constraints")
 
         if self.precision_constraints == "none":
             self.precision_constraints = None
 
         self.restricted = args_util.get(args, "restricted")
         self.refittable = args_util.get(args, "refittable")
+        self.strip_plan = args_util.get(args, "strip_plan")
 
         self.calibration_cache = args_util.get(args, "calibration_cache")
         calib_base = args_util.get(args, "calibration_base_class")
         self.calibration_base_class = None
         if calib_base is not None:
             self.calibration_base_class = inline(safe("trt.{:}", inline_identifier(calib_base)))
 
@@ -545,14 +571,21 @@
         self.hardware_compatibility_level = None
         hardware_compatibility_level = args_util.get(args, "hardware_compatibility_level")
         if hardware_compatibility_level is not None:
             self.hardware_compatibility_level = make_trt_enum_val(
                 "HardwareCompatibilityLevel", hardware_compatibility_level
             )
 
+        self.profiling_verbosity = None
+        profiling_verbosity = args_util.get(args, "profiling_verbosity")
+        if profiling_verbosity is not None:
+            self.profiling_verbosity = make_trt_enum_val(
+                "ProfilingVerbosity", profiling_verbosity
+            )
+
         self.max_aux_streams = args_util.get(args, "max_aux_streams")
         self.version_compatible = args_util.get(args, "version_compatible")
         self.exclude_lean_runtime = args_util.get(args, "exclude_lean_runtime")
 
         quantization_flags = args_util.get(args, "quantization_flags")
         self.quantization_flags = None
         if quantization_flags is not None:
@@ -561,14 +594,16 @@
         if self.exclude_lean_runtime and not self.version_compatible:
             G_LOGGER.critical(f"`--exclude-lean-runtime` requires `--version-compatible` to be enabled.")
 
         self.error_on_timing_cache_miss = args_util.get(args, "error_on_timing_cache_miss")
 
         self.disable_compilation_cache = args_util.get(args, "disable_compilation_cache")
 
+        self.weight_streaming = args_util.get(args, "weight_streaming")
+
     def add_to_script_impl(self, script):
         profiles = []
         for profile_dict in self.profile_dicts:
             profile_str = "Profile()"
             for name in profile_dict.keys():
                 profile_str += safe(".add({:}, min={:}, opt={:}, max={:})", name, *profile_dict[name]).unwrap()
             profiles.append(profile_str)
@@ -628,18 +663,17 @@
         if any(
             arg is not None
             for arg in [
                 self.tactic_sources,
                 self.memory_pool_limits,
                 self.preview_features,
                 self.engine_capability,
+                self.profiling_verbosity,
                 self.hardware_compatibility_level,
                 self.quantization_flags,
-                self.error_on_timing_cache_miss,
-                self.disable_compilation_cache,
             ]
         ):
             script.add_import(imports="tensorrt", imp_as="trt")
 
         if self.trt_config_script is not None:
             script.add_import(imports=["InvokeFromScript"], frm="polygraphy.backend.common")
             config_loader_str = make_invocable(
@@ -661,25 +695,28 @@
                 algorithm_selector=algo_selector,
                 sparse_weights=self.sparse_weights,
                 tactic_sources=self.tactic_sources,
                 use_dla=self.use_dla,
                 allow_gpu_fallback=self.allow_gpu_fallback,
                 memory_pool_limits=self.memory_pool_limits,
                 refittable=self.refittable,
+                strip_plan=self.strip_plan,
                 preview_features=self.preview_features,
                 engine_capability=self.engine_capability,
                 direct_io=self.direct_io,
                 builder_optimization_level=self.builder_optimization_level,
                 hardware_compatibility_level=self.hardware_compatibility_level,
+                profiling_verbosity=self.profiling_verbosity,
                 max_aux_streams=self.max_aux_streams,
                 version_compatible=self.version_compatible,
                 exclude_lean_runtime=self.exclude_lean_runtime,
                 quantization_flags=self.quantization_flags,
                 error_on_timing_cache_miss=self.error_on_timing_cache_miss,
                 disable_compilation_cache=self.disable_compilation_cache,
+                weight_streaming=self.weight_streaming,
             )
             if config_loader_str is not None:
                 script.add_import(imports="CreateConfig", frm="polygraphy.backend.trt", imp_as="CreateTrtConfig")
 
         if config_loader_str is not None:
             config_loader_name = script.add_loader(config_loader_str, "create_trt_config")
         else:
```

## polygraphy/tools/args/backend/trt/loader.py

```diff
@@ -76,23 +76,32 @@
             "--onnx-flags",
             help="Flag(s) for adjusting the default parsing behavior of the ONNX parser."
             "Flag values come from the `trt.OnnxParserFlag` enum and are case-insensitve."
             "For example: --onnx-flags native_instancenorm ",
             nargs="+",
             default=None,
         )
+        self.group.add_argument(
+            "--plugin-instancenorm",
+            help="Switch to clear the `trt.OnnxParserFlag.NATIVE_INSTANCENORM` flag and"
+            "force the usage of the plugin implementation of ONNX InstanceNorm."
+            "Note that `trt.OnnxParserFlag.NATIVE_INSTANCENORM` is ON by default since TensorRT 10.0.",
+            action="store_true",
+            default=None,
+        )
 
     def parse_impl(self, args):
         """
         Parses command-line arguments and populates the following attributes:
 
         Attributes:
             flags (List[str]): flags for onnxparser
         """
         self._flags = args_util.get(args, "onnx_flags", default=[])
+        self._plugin_instancenorm = args_util.get(args, "plugin_instancenorm", default=None)
 
     def get_flags(self):
         """
         Updates and returns the ONNX parser flags as necessary.
         This must be called only in `add_to_script_impl`.
         Flags should not be accessed directly.
         """
@@ -106,15 +115,15 @@
             and "native_instancenorm" not in [f.lower() for f in flags]
         ):
             G_LOGGER.warning(
                 f"Version or hardware compatibility mode is enabled. Automatically enabling `NATIVE_INSTANCENORM` ONNX parser flag."
             )
             flags.append("native_instancenorm")
 
-        return [make_trt_enum_val("OnnxParserFlag", f) for f in flags] or None
+        return ([make_trt_enum_val("OnnxParserFlag", f) for f in flags] or None, self._plugin_instancenorm)
 
 
 @mod.export()
 class TrtLoadNetworkArgs(BaseArgs):
     """
     TensorRT Network Loading: loading TensorRT networks.
 
@@ -217,28 +226,37 @@
         self.group.add_argument(
             "--strongly-typed",
             help="Mark the network as being strongly typed.",
             action="store_true",
             default=None,
         )
 
+        self.group.add_argument(
+            "--mark-debug",
+            help="Specify list of names of tensors to be marked as debug tensors."
+            "For example, `--mark-debug tensor1 tensor2 tensor3`. ",
+            nargs="+",
+            default=None,
+        )
+
     def parse_impl(self, args):
         """
         Parses command-line arguments and populates the following attributes:
 
         Attributes:
             outputs (List[str]): Names of output tensors.
             exclude_outputs (List[str]): Names of tensors which should be unmarked as outputs.
             trt_network_func_name (str): The name of the function in a custom network script that creates the network.
             layer_precisions (Dict[str, str]): Layer names mapped to their desired compute precision, in string form.
             tensor_datatypes (Dict[str, str]): Tensor names mapped to their desired data types, in string form.
             tensor_formats (Dict[str, List[str]]): Tensor names mapped to their desired formats, in string form.
             postprocess_scripts (List[Tuple[str, str]]):
                     A list of tuples specifying a path to a network postprocessing script and the name of the postprocessing function.
             strongly_typed (bool): Whether to mark the network as being strongly typed.
+            mark_debug (List[str]): Names of tensors which should be marked as debug tensors.
         """
         self.outputs = args_util.get_outputs(args, "trt_outputs")
 
         self.exclude_outputs = args_util.get(args, "trt_exclude_outputs")
 
         self.trt_network_func_name = args_util.get(args, "trt_network_func_name")
 
@@ -277,28 +295,30 @@
             if not func:
                 func = "postprocess"
             if not os.path.isfile(script_path):
                 G_LOGGER.warning(f"Could not find postprocessing script {script_path}")
             self.postprocess_scripts.append((script_path, func))
 
         self.strongly_typed = args_util.get(args, "strongly_typed")
+        
+        self.mark_debug = args_util.get(args, "mark_debug")
 
     def add_to_script_impl(self, script):
         network_func_name = self.arg_groups[ModelArgs].extra_model_info
         if self.trt_network_func_name is not None:
             mod.warn_deprecated("--trt-network-func-name", "the model argument", "0.50.0", always_show_warning=True)
             network_func_name = self.trt_network_func_name
 
         model_file = self.arg_groups[ModelArgs].path
         model_type = self.arg_groups[ModelArgs].model_type
         outputs = args_util.get_outputs_for_script(script, self.outputs)
-        parser_flags = self.arg_groups[TrtOnnxFlagArgs].get_flags()
+        parser_flags, plugin_instancenorm = self.arg_groups[TrtOnnxFlagArgs].get_flags()
 
         if any(
-            arg is not None for arg in [self.layer_precisions, self.tensor_datatypes, self.tensor_formats, parser_flags]
+            arg is not None for arg in [self.layer_precisions, self.tensor_datatypes, self.tensor_formats, parser_flags, plugin_instancenorm]
         ):
             script.add_import(imports="tensorrt", imp_as="trt")
 
         if model_type == "trt-network-script":
             script.add_import(imports=["InvokeFromScript"], frm="polygraphy.backend.common")
             loader_str = make_invocable(
                 "InvokeFromScript",
@@ -314,23 +334,25 @@
                 onnx_loader = self.arg_groups[OnnxLoadArgs].add_to_script(
                     script, disable_custom_outputs=True, serialize_model=True
                 )
                 loader_str = make_invocable(
                     "NetworkFromOnnxBytes",
                     self.arg_groups[TrtLoadPluginsArgs].add_to_script(script, onnx_loader),
                     flags=parser_flags,
+                    plugin_instancenorm=plugin_instancenorm,
                     strongly_typed=self.strongly_typed,
                 )
                 loader_name = script.add_loader(loader_str, "parse_network_from_onnx")
             else:
                 script.add_import(imports=["NetworkFromOnnxPath"], frm="polygraphy.backend.trt")
                 loader_str = make_invocable(
                     "NetworkFromOnnxPath",
                     self.arg_groups[TrtLoadPluginsArgs].add_to_script(script, model_file),
                     flags=parser_flags,
+                    plugin_instancenorm=plugin_instancenorm,
                     strongly_typed=self.strongly_typed,
                 )
                 loader_name = script.add_loader(loader_str, "parse_network_from_onnx")
         else:
             G_LOGGER.internal_error("Loading from ONNX is not enabled and a network script was not provided!")
 
         def add_loader_if_nondefault(loader, result_var_name, **kwargs):
@@ -355,14 +377,17 @@
         )
         loader_name = add_loader_if_nondefault(
             "SetTensorDatatypes", "set_tensor_datatypes", tensor_datatypes=self.tensor_datatypes
         )
         loader_name = add_loader_if_nondefault(
             "SetTensorFormats", "set_tensor_formats", tensor_formats=self.tensor_formats
         )
+        loader_name = add_loader_if_nondefault(
+            "MarkDebug", "mark_debug", mark_debug=self.mark_debug
+        )
 
         return loader_name
 
     def load_network(self):
         """
         Loads a TensorRT Network model according to arguments provided on the command-line.
```

## polygraphy/tools/args/backend/trt/runner.py

```diff
@@ -38,21 +38,58 @@
         self.group.add_argument(
             "--optimization-profile",
             help="The index of optimization profile to use for inference",
             type=int,
             default=None,
             dest="optimization_profile",
         )
+        self.group.add_argument(
+            "--allocation-strategy",
+            help="The way activation memory is allocated. "
+            "static: Pre-allocate based on the max possible size across all profiles. "
+            "profile: Allocate what's needed for the profile to use."
+            "runtime: Allocate what's needed for the current input shapes.",
+            type=str,
+            default=None,
+            dest="allocation_strategy",
+            choices=["static", "profile", "runtime"],
+        )
+        self.group.add_argument(
+            "--weight-streaming-budget",
+            help="The amount of GPU memory in bytes that TensorRT can use for weights at runtime. The engine must be built with weight streaming enabled. It can take on the following values: "
+            "None or 0: Disables weight streaming at runtime. "
+            "-1: TensorRT will decide the streaming budget automatically. "
+            "0 to 100%%: The percentage of weights TRT will stream. 100%% will stream the maximum number of weights. "
+            ">0B: The exact amount of streamable weights that reside on the GPU (unit suffixes are supported).",
+            type=str,
+            default=None
+        )
 
     def parse_impl(self, args):
         """
         Parses command-line arguments and populates the following attributes:
 
         Attributes:
             optimization_profile (int): The index of the optimization profile to initialize the runner with.
+            allocation_strategy (str): The way activation memory is allocated.
+            weight_streaming_budget (int): The weight streaming budget in bytes.
+            weight_streaming_percent (float): The percentage of weights streamed.
         """
         self.optimization_profile = args_util.get(args, "optimization_profile")
+        self.allocation_strategy = args_util.get(args, "allocation_strategy")
+        self.weight_streaming_budget = None
+        self.weight_streaming_percent = None
+        
+        ws_arg = args_util.get(args, "weight_streaming_budget")
+        if ws_arg and ws_arg.endswith("%"):
+            percent = float(ws_arg[:-1])
+            assert 0 <= percent <= 100, "Invalid percentage for --weight-streaming-budget!"
+            self.weight_streaming_percent = percent
+        elif ws_arg:
+            budget = args_util.parse_num_bytes(ws_arg)
+            assert budget == -1 or budget >= 0, "Invalid amount for --weight-streaming-budget!"
+            self.weight_streaming_budget = budget
 
     def add_to_script_impl(self, script):
         script.add_import(imports=["TrtRunner"], frm="polygraphy.backend.trt")
         loader_name = self.arg_groups[TrtLoadEngineArgs].add_to_script(script)
-        script.add_runner(make_invocable("TrtRunner", loader_name, optimization_profile=self.optimization_profile))
+        script.add_runner(make_invocable("TrtRunner", loader_name, optimization_profile=self.optimization_profile, allocation_strategy=self.allocation_strategy, weight_streaming_budget=self.weight_streaming_budget, weight_streaming_percent=self.weight_streaming_percent))
```

## polygraphy/tools/args/comparator/data_loader.py

```diff
@@ -86,14 +86,23 @@
             metavar="NUM",
             help="Number of inference iterations for which the default data loader should supply data",
             type=int,
             default=None,
             dest="iterations",
         )
 
+        self._array_modules = ["numpy", "torch"]
+        self.group.add_argument(
+            "--data-loader-backend-module",
+            type=str,
+            choices=self._array_modules,
+            help=f"The module to use for generating input arrays. Currently supported options: {', '.join(self._array_modules)}",
+            default=None,
+        )
+
         custom_loader_group = self.group.add_mutually_exclusive_group()
         custom_loader_group.add_argument(
             "--load-inputs",
             "--load-input-data",
             help="Path(s) to load inputs. The file(s) should be a JSON-ified "
             "List[Dict[str, numpy.ndarray]], i.e. a list where each element is the feed_dict for a single iteration. "
             "When this option is used, all other data loader arguments are ignored. ",
@@ -124,14 +133,15 @@
         Attributes:
             seed (int): The seed to use for random data generation.
             val_range (Dict[str, Tuple[int]]): Per-input ranges of values to generate.
             iterations (int): The number of iterations for which to generate data.
             load_inputs_paths (List[str]): Path(s) from which to load inputs.
             data_loader_script (str): Path to a custom script to load inputs.
             data_loader_func_name (str): Name of the function in the custom data loader script that loads data.
+            data_loader_backend_module (str): Module to be used that provides arrays.
         """
 
         def omit_none_tuple(tup):
             if all([elem is None for elem in tup]):
                 return None
             return tup
 
@@ -164,14 +174,16 @@
                         f"Note: Option was parsed as: input: {name}, range: {vals}"
                     )
 
         self.iterations = args_util.get(args, "iterations")
 
         self.load_inputs_paths = args_util.get(args, "load_inputs_paths")
 
+        self.data_loader_backend_module = args_util.get(args, "data_loader_backend_module")
+
         self.data_loader_script, self.data_loader_func_name = args_util.parse_script_and_func_name(
             args_util.get(args, "data_loader_script"), default_func_name="load_data"
         )
         func_name = args_util.get(args, "data_loader_func_name")
         if func_name is not None:
             mod.warn_deprecated("--data-loader-func-name", "--data-loader-script", "0.50.0", always_show_warning=True)
             self.data_loader_func_name = func_name
@@ -220,14 +232,15 @@
                 "DataLoader",
                 seed=self.seed,
                 iterations=self.iterations,
                 input_metadata=user_input_metadata_str,
                 int_range=self._int_range,
                 float_range=self._float_range,
                 val_range=self.val_range,
+                data_loader_backend_module=self.data_loader_backend_module,
             )
             if data_loader:
                 script.add_import(imports=["DataLoader"], frm="polygraphy.comparator")
 
         if using_random_data != self.is_using_random_data():
             G_LOGGER.internal_error("is_using_random_data() reported a false positive!")
```

## polygraphy/tools/check/subtool/lint.py

```diff
@@ -12,38 +12,33 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 import contextlib
+import enum
 import functools
 import io
-import enum
-import sys
-import os
 import json
-import tempfile
+import os
 import re
+import sys
+import tempfile
 from collections import OrderedDict
 from typing import Optional, Union
 
-from polygraphy.tools import util as tools_util
 from polygraphy import mod
-from polygraphy.logger import G_LOGGER
-from polygraphy.exception import PolygraphyException
-from polygraphy.tools.args import (
-    OnnxLoadArgs,
-    ModelArgs,
-    DataLoaderArgs,
-    OnnxrtSessionArgs,
-)
 from polygraphy.comparator import IterationResult
-from polygraphy.tools.base import Tool
+from polygraphy.exception import PolygraphyException
 from polygraphy.json import save_json
+from polygraphy.logger import G_LOGGER
+from polygraphy.tools import util as tools_util
+from polygraphy.tools.args import DataLoaderArgs, ModelArgs, OnnxLoadArgs, OnnxrtSessionArgs
+from polygraphy.tools.base import Tool
 
 onnx = mod.lazy_import("onnx")
 gs = mod.lazy_import("onnx_graphsurgeon>=0.3.21")
 onnx_util = mod.lazy_import("polygraphy.backend.onnx.util")
 onnx_backend = mod.lazy_import("polygraphy.backend.onnx")
 onnxrt_backend = mod.lazy_import("polygraphy.backend.onnxrt")
 
@@ -84,15 +79,15 @@
     3. Subgraph nested inside nodes are not recursively linted.
     4. Custom Ops are documented as warnings in the JSON Report, but are treated as exceptions by the internal inference checks. Therefore downstream nodes that depend on the custom op are not checked for error or custom ops.
     5. The subtool verifies data-dependent failures either based on user's input data or generating random data for the input tensors. Therefore, the subtool's coverage of subgraphs are completely dependent on the input data and does not guarantee 100% coverage.
     For example, if a subgraph has a conditional branch, the subtool will only check the branch that is taken based on the input data.
     6. Large models (>2GB) require external data to be in same directory as the model file, custom paths to external data are not supported.
     """
 
-    CUSTOM_OP_EXCEPTION_SUBSTR = "No opset import for domain"
+    CUSTOM_OP_EXCEPTION_SUBSTRS = ["No opset import for domain", "is not a registered function/op"]
     ONNX_CHECKER_IGNORE_SUBSTR = "Bad node spec for node"
     INVALID_ONNX_EXCEPTION_SUBSTR = "Error parsing message with type 'onnx.ModelProto'"
     MAXIMUM_PROTOBUF = 2e9  # 2GB
 
     class ContextManager:
         """
         Keeps track of the linting process, including the current node being linted, cached tensors and their consumers.
@@ -805,19 +800,22 @@
 
             if not exception:
                 # ORT inference check passes, early exit
                 # any recorded warnings from stderr are added to the report.
                 # NOTE: This is only done if early-exiting, as otherwise these warnings tend to be repeats
                 # of node level warnings/exceptions.
                 if warn_str:
-                    self.report.add(
-                        Lint.Level.WARNING,
-                        Lint.Source.ONNXRUNTIME,
-                        warn_str,
-                    )
+                    warnings = warn_str.split('\n')
+                    for warning in warnings:
+                        if len(warning) > 0:
+                            self.report.add(
+                                Lint.Level.WARNING,
+                                Lint.Source.ONNXRUNTIME,
+                                warning,
+                            )
 
                 ### report.add(unused nodes and tensors) ###
                 _report_unused_info(graph)
 
                 self.report.summary["passing"] = {node.name for node in graph.nodes}
                 self.report.export(args.output)
                 G_LOGGER.verbose("ORT inference check passed. Model is valid. Early exiting.")
@@ -839,15 +837,15 @@
                 inference_output = None
 
                 if g:  # has valid ancestors. Can perform inference.
                     model_bytes = onnx_backend.BytesFromOnnx(gs.export_onnx(g, do_type_check=False))
                     inference_output, exception, _, _ = _ort_inference_check(model_bytes, lcm.feed_dict())
                     # NOTE: we ignore stdout and stderr as it contains info from polygraphy not relevant to linting.
                     err_str = str(exception) if exception else ""
-                    if Lint.CUSTOM_OP_EXCEPTION_SUBSTR in err_str:
+                    if any([substr in err_str for substr in Lint.CUSTOM_OP_EXCEPTION_SUBSTRS]):
                         self.report.add(
                             level=Lint.Level.WARNING,
                             source=Lint.Source.ONNXRUNTIME,
                             message=err_str,
                             node_name=g.name,
                             op=g.nodes[0].op,
                         )
```

## polygraphy/tools/inspect/subtool/capability.py

```diff
@@ -12,14 +12,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 import os
 
+from collections import OrderedDict
 from polygraphy import mod
 from polygraphy.common.interface import TypedDict
 from polygraphy.logger import G_LOGGER
 from polygraphy.tools.args import OnnxInferShapesArgs, OnnxLoadArgs, ModelArgs, OnnxSaveArgs
 from polygraphy.tools.base import Tool
 
 common_backend = mod.lazy_import("polygraphy.backend.common")
@@ -84,14 +85,38 @@
     except AttributeError:
         trt_util.fail_unavailable("supports_model in tensorrt.OnnxParser")
 
     supported, nodelists = parser.supports_model(common_backend.bytes_from_path(path), path)
     return supported, nodelists, parser
 
 
+def parse(path):
+    """
+    Invokes the ONNX parser's `parse` on the specified model.
+
+    Args:
+        path (str): The path to the ONNX model.
+
+    Returns:
+        Tuple[bool, parser]:
+            (1) Whether the model was parsed successfully.
+            (2) The TensorRT ONNX parser instance.
+    """
+    _, network = trt_backend.create_network()
+    parser = trt.OnnxParser(network, trt_backend.get_trt_logger())
+
+    try:
+        parser.parse
+    except AttributeError:
+        trt_util.fail_unavailable("parse in tensorrt.OnnxParser")
+
+    supported = parser.parse(common_backend.bytes_from_path(path), path)
+    return supported, parser
+
+
 def save_subgraph(onnx_save_args, graph, start, end, prefix="", use_tmp_file=False):
     """
     Extracts a subgraph from the main graph and saves it to disk.
 
     Args:
         graph (onnx_graphsurgeon.Graph): The parent/main graph.
         start (int): The (inclusive) index of the start node.
@@ -143,31 +168,103 @@
 
     for op, node_index_map in final_unsupported.items():
         for reason, node_indices in node_index_map.items():
             summary += f"{op:{op_width}}| {len(node_indices):7} | {reason:{reason_width}} | {node_indices}\n"
     return summary
 
 
+def gen_results_summary_no_partitioning(stack_trace_to_errors):
+    """
+    Generates a results summary given all the errors with a corresponding stack trace.
+
+    Args:
+        stack_trace_to_errors (``OrderedDict[str, List[Tuple[str, str, str]]]``):
+                Reported errors with a corresponding stack trace.
+
+    Returns:
+        str: A summary of all the unsupported ops in model, along with reasons and stack traces.
+    """
+    stack_trace_width = max(map(len, list(stack_trace_to_errors.keys()) + ["Stack trace "]))
+    op_width = max(max(len(op) for errors_per_stack in stack_trace_to_errors.values() for op, _, _ in errors_per_stack), len("Operator "))
+    node_width = max(max(len(node) for errors_per_stack in stack_trace_to_errors.values() for _, node, _ in errors_per_stack), len("Node "))
+    reason_width = max(len(reason) for errors_per_stack in stack_trace_to_errors.values() for _, _, reason in errors_per_stack)
+
+    summary = "===== Summary =====\n"
+
+    header = f"{'Stack trace':{stack_trace_width}} | {'Operator':{op_width}} | {'Node':{node_width}} | {'Reason':{reason_width}}\n"
+    summary += header + "-" * len(header) + "\n"
+
+    for stack_trace, op_node_reason in stack_trace_to_errors.items():
+        for op, node, reason in op_node_reason:
+            summary += f"{stack_trace:{stack_trace_width}} | {op:{op_width}} | {node:{node_width}} | {reason:{reason_width}}\n"
+    return summary
+
+
 class Capability(Tool):
     """
-    Determine the capability of TensorRT to run an ONNX graph. Graph will be paritioned into supported and unsupported subgraphs.
+    Determine the capability of TensorRT to run an ONNX graph. Graph will be either partitioned into supported and unsupported subgraphs
+    or only analyzed in terms of statically checked errors.
     """
 
     def __init__(self):
         super().__init__("capability")
 
     def get_subscriptions_impl(self):
         return [
             ModelArgs(model_opt_required=True, input_shapes_opt_name=False, required_model_type="onnx"),
             OnnxInferShapesArgs(),
             OnnxLoadArgs(outputs_opt_prefix=False),
             OnnxSaveArgs(output_default_path="polygraphy_capability_dumps", allow_multiple_models=True),
         ]
+    
+    def add_parser_args_impl(self, parser):
+        parser.add_argument(
+            "--with-partitioning",
+            help="Whether to partition the model graph on the nodes with parsing failures",
+            action="store_true",
+        )
 
     def run_impl(self, args):
+        if args.with_partitioning:
+            self.supports_model_variant()
+        else:
+            self.no_partitioning_variant()
+
+    def no_partitioning_variant(self):
+        supported, parser = parse(self.arg_groups[ModelArgs].path)
+        if supported:
+            G_LOGGER.info("Graph is fully supported by TensorRT; Will not report errors.")
+            return
+        
+        stack_trace_to_errors = OrderedDict()
+        for err_idx in range(parser.num_errors):
+            parser_error = parser.get_error(err_idx)
+            stack_trace = ""
+            if parser_error.local_function_stack_size() > 0:
+                for function_idx in range(parser_error.local_function_stack_size()):
+                    stack_trace += parser_error.local_function_stack()[function_idx]
+                    if function_idx != parser_error.local_function_stack_size() - 1:
+                        stack_trace += " -> "
+                
+            if stack_trace not in stack_trace_to_errors:
+                stack_trace_to_errors[stack_trace] = []
+
+            node_operator = parser_error.node_operator()
+            node_name = parser_error.node_name()
+            parser_error_desc = str(parser_error)
+            stack_trace_to_errors[stack_trace].append(tuple((node_operator, node_name, parser_error_desc)))
+        
+        summary = gen_results_summary_no_partitioning(stack_trace_to_errors)
+
+        G_LOGGER.info(summary)
+        util.save_file(
+            summary, os.path.join(self.arg_groups[OnnxSaveArgs].path, "results.txt"), "w", description="results"
+        )
+
+    def supports_model_variant(self):
         supported, nodelists, _ = supports_model(self.arg_groups[ModelArgs].path)
         if supported:
             G_LOGGER.info("Graph is fully supported by TensorRT; Will not generate subgraphs.")
             return
 
         parent_graph = onnx_backend.gs_from_onnx(self.arg_groups[OnnxLoadArgs].load_onnx())
```

## polygraphy/tools/surgeon/surgeon.py

```diff
@@ -11,15 +11,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 from polygraphy.tools.base import Tool
-from polygraphy.tools.surgeon.subtool import Extract, Insert, Sanitize, Prune
+from polygraphy.tools.surgeon.subtool import Extract, Insert, Sanitize, Prune, WeightStripper, WeightReconstructor
 
 ################################# MAIN TOOL #################################
 
 
 class Surgeon(Tool):
     """
     Modify ONNX models.
@@ -30,8 +30,10 @@
 
     def get_subtools_impl(self):
         return "Surgical Instruments", [
             Extract(),
             Sanitize(),
             Insert(),
             Prune(),
+            WeightStripper(),
+            WeightReconstructor(),
         ]
```

## polygraphy/tools/surgeon/subtool/__init__.py

```diff
@@ -1,4 +1,6 @@
 from polygraphy.tools.surgeon.subtool.extract import Extract
 from polygraphy.tools.surgeon.subtool.insert import Insert
 from polygraphy.tools.surgeon.subtool.sanitize import Sanitize
 from polygraphy.tools.surgeon.subtool.prune import Prune
+from polygraphy.tools.surgeon.subtool.weight_strip import WeightStripper
+from polygraphy.tools.surgeon.subtool.weight_reconstruct import WeightReconstructor
```

## polygraphy/util/array.py

```diff
@@ -40,30 +40,36 @@
 
     Args:
         obj (Any): The object to check.
 
     Returns:
         bool: Whether the object is a PyTorch tensor.
     """
-    return mod.has_mod("torch") and isinstance(obj, torch.Tensor)
+    return (
+        torch.is_installed() and torch.is_importable() and isinstance(obj, torch.Tensor)
+    )
 
 
 @mod.export()
 def is_numpy(obj):
     """
     Whether the provided object is a NumPy array or scalar.
     This function does *not* introduce a dependency on the NumPy module.
 
     Args:
         obj (Any): The object to check.
 
     Returns:
         bool: Whether the object is a NumPy array.
     """
-    return mod.has_mod("numpy") and (isinstance(obj, np.ndarray) or isinstance(obj, np.generic))
+    return (
+        np.is_installed()
+        and np.is_importable()
+        and (isinstance(obj, np.ndarray) or isinstance(obj, np.generic))
+    )
 
 
 @mod.export()
 def is_device_view(obj):
     """
     Whether the provided object is a DeviceView array.
 
@@ -140,29 +146,34 @@
             In the case of more than one array, this function will automatically convert the rest to be of the
             same kind as the first.
     """
 
     def dispatch_impl(func):
         def _get_key(obj):
             key = None
-            if is_torch(obj):
-                key = "torch"
+
+            if is_device_view(obj):
+                key = "device_view"
             elif is_numpy(obj):
                 key = "numpy"
-            elif is_device_view(obj):
-                key = "device_view"
+            elif is_torch(obj):
+                key = "torch"
             elif isinstance(obj, numbers.Number):
                 key = "number"
 
             if not key:
-                G_LOGGER.critical(f"Function: {func.__name__} is unsupported for objects of type: {type(obj).__name__}")
+                G_LOGGER.critical(
+                    f"Function: {func.__name__} is unsupported for objects of type: {type(obj).__name__}"
+                )
             return key
 
         if num_arrays < 0:
-            G_LOGGER.critical(f"Function: {func.__name__} is unsupported with {num_arrays} < 0")
+            G_LOGGER.critical(
+                f"Function: {func.__name__} is unsupported with {num_arrays} < 0"
+            )
 
         @functools.wraps(func)
         def wrapped(*args, **kwargs):
             if len(args) < num_arrays:
                 G_LOGGER.critical(
                     f"Function: {func.__name__} is unsupported for less than {num_arrays} positional arguments"
                 )
@@ -184,15 +195,19 @@
                 elif key == "numpy":
                     return to_numpy(obj)
                 else:
                     G_LOGGER.critical(
                         f"Function: {func.__name__} is unsupported for objects of type: {type(obj).__name__}"
                     )
 
-            converted_args = [obj0] + list(map(convert_array, args[1:num_arrays])) + list(args[num_arrays:])
+            converted_args = (
+                [obj0]
+                + list(map(convert_array, args[1:num_arrays]))
+                + list(args[num_arrays:])
+            )
 
             return mapping[key](*converted_args, **kwargs)
 
         return wrapped
 
     return dispatch_impl
 
@@ -327,15 +342,19 @@
 
     Returns:
         bool: Whether the array is in CPU, i.e. host, memory.
 
     Raises:
         PolygraphyException: if the input is of an unrecognized type.
     """
-    return {"torch": lambda obj: obj.device.type == "cpu", "numpy": lambda _: True, "device_view": lambda _: False}
+    return {
+        "torch": lambda obj: obj.device.type == "cpu",
+        "numpy": lambda _: True,
+        "device_view": lambda _: False,
+    }
 
 
 @mod.export()
 @dispatch()
 def is_on_gpu():
     """
     Returns whether the input array is in GPU memory.
@@ -345,15 +364,19 @@
 
     Returns:
         bool: Whether the array is in GPU, i.e. host, memory.
 
     Raises:
         PolygraphyException: if the input is of an unrecognized type.
     """
-    return {"torch": lambda obj: obj.device.type == "cuda", "numpy": lambda _: False, "device_view": lambda _: True}
+    return {
+        "torch": lambda obj: obj.device.type == "cuda",
+        "numpy": lambda _: False,
+        "device_view": lambda _: True,
+    }
 
 
 @mod.export()
 @dispatch()
 def dtype():
     """
     Return the data type the input array.
@@ -409,15 +432,19 @@
     """
     if not is_contiguous(obj):
         G_LOGGER.critical(f"Input array to view() must be contiguous in memory")
 
     if is_device_view(obj):
         return obj.view(shape=shape, dtype=dtype)
 
-    dtype = DataType.to_dtype(dtype, "numpy") if is_numpy(obj) else DataType.to_dtype(dtype, "torch")
+    dtype = (
+        DataType.to_dtype(dtype, "numpy")
+        if is_numpy(obj)
+        else DataType.to_dtype(dtype, "torch")
+    )
     return obj.reshape(-1).view(dtype).reshape(shape)
 
 
 @mod.export()
 @dispatch()
 def is_contiguous():
     """
@@ -461,15 +488,19 @@
     """
 
     def impl_numpy(obj):
         if is_contiguous(obj):
             return obj
         return np.ascontiguousarray(obj)
 
-    return {"torch": lambda obj: obj.contiguous(), "numpy": impl_numpy, "device_view": lambda obj: obj}
+    return {
+        "torch": lambda obj: obj.contiguous(),
+        "numpy": impl_numpy,
+        "device_view": lambda obj: obj,
+    }
 
 
 @mod.export()
 @dispatch()
 def resize_or_reallocate():
     """
     Resizes the provided buffer, possibly reallocating the buffer.
@@ -493,15 +524,17 @@
                 )
                 obj = np.empty(shape, dtype=np.dtype(obj.dtype))
         return obj
 
     return {
         "numpy": numpy_impl,
         "torch": lambda obj, shape: obj.resize_(shape) if shape != obj.shape else obj,
-        "device_view": lambda obj, shape: obj.resize(shape) if shape != obj.shape else obj,
+        "device_view": lambda obj, shape: obj.resize(shape)
+        if shape != obj.shape
+        else obj,
     }
 
 
 ##
 ## Math Helpers
 ##
 
@@ -665,16 +698,20 @@
     Raises:
         PolygraphyException: if the input is of an unrecognized type.
     """
     DEFAULT_RTOL = 1e-5
     DEFAULT_ATOL = 1e-8
 
     return {
-        "torch": lambda lhs, rhs, rtol=DEFAULT_RTOL, atol=DEFAULT_ATOL: torch.allclose(lhs, rhs, rtol=rtol, atol=atol),
-        "numpy": lambda lhs, rhs, rtol=DEFAULT_RTOL, atol=DEFAULT_ATOL: np.allclose(lhs, rhs, rtol=rtol, atol=atol),
+        "torch": lambda lhs, rhs, rtol=DEFAULT_RTOL, atol=DEFAULT_ATOL: torch.allclose(
+            lhs, rhs, rtol=rtol, atol=atol
+        ),
+        "numpy": lambda lhs, rhs, rtol=DEFAULT_RTOL, atol=DEFAULT_ATOL: np.allclose(
+            lhs, rhs, rtol=rtol, atol=atol
+        ),
     }
 
 
 @mod.export()
 def unravel_index(index, shape):
     """
     Unravels a flat index into a N-dimensional index based on the specified shape.
@@ -1015,18 +1052,24 @@
 
         # Top K has no implementation for float16 in torch-cpu, so
         # If gpu is available, run computation there
         # Otherwise, run the calculation on cpu using fp32 precision
         if obj.dtype == torch.float16:
             if torch.cuda.is_available():
                 original_device = obj.device
-                ret = tuple(torch.topk(obj.to('cuda'), builtins.min(k, axis_len), dim=axis))
+                ret = tuple(
+                    torch.topk(obj.to("cuda"), builtins.min(k, axis_len), dim=axis)
+                )
                 return (ret[0].to(original_device), ret[1].to(original_device))
             else:
-                ret = tuple(torch.topk(obj.type(torch.float32), builtins.min(k, axis_len), dim=axis))
+                ret = tuple(
+                    torch.topk(
+                        obj.type(torch.float32), builtins.min(k, axis_len), dim=axis
+                    )
+                )
                 return (ret[0].type(torch.float16), ret[1].type(torch.float16))
         return tuple(torch.topk(obj, builtins.min(k, axis_len), dim=axis))
 
     return {
         "numpy": numpy_impl,
         "torch": torch_impl,
     }
```

## Comparing `polygraphy-0.49.0.dist-info/LICENSE.txt` & `polygraphy-0.49.9.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `polygraphy-0.49.0.dist-info/METADATA` & `polygraphy-0.49.9.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,15 @@
 Metadata-Version: 2.1
 Name: polygraphy
-Version: 0.49.0
+Version: 0.49.9
 Summary: Polygraphy: A Deep Learning Inference Prototyping and Debugging Toolkit
 Home-page: https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy
 Author: NVIDIA
 Author-email: svc_tensorrt@nvidia.com
 License: Apache 2.0
-Platform: UNKNOWN
 Classifier: Intended Audience :: Developers
 Classifier: Programming Language :: Python :: 3
 Requires-Python: >=3.6
 License-File: LICENSE.txt
 
 # Polygraphy: A Deep Learning Inference Prototyping and Debugging Toolkit
 
@@ -176,9 +175,7 @@
 
 For how-to guides, see the [how-to guides directory](./how-to).
 
 
 ## Contributing
 
 For information on how you can contribute to this project, see [CONTRIBUTING.md](./CONTRIBUTING.md)
-
-
```

## Comparing `polygraphy-0.49.0.dist-info/RECORD` & `polygraphy-0.49.9.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,76 +1,76 @@
-polygraphy/__init__.py,sha256=WUPGGV8EsX_WU9flpICQ76wrT_E4JAPSDTsnOzoktoY,49
+polygraphy/__init__.py,sha256=P3as3i_Q7sZpahfLKdr8dzpjYVXhMGF1vwO4Tw8ImwA,49
 polygraphy/config.py,sha256=trC1W-hE8FDlD-TNNht7MgzAt-mMNPZ7t5kvEGngi8U,2438
 polygraphy/constants.py,sha256=3iIPdOA2_b6f4r5RMhELb_BDR_lAZ1W8YLF38jVzcVY,1079
 polygraphy/backend/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 polygraphy/backend/base/__init__.py,sha256=5w_VxcOAzTrKzgdzwXuZAAsZwOIiP_kgSA9EKBm9iRw,90
 polygraphy/backend/base/loader.py,sha256=D9Vt3Rl-cvK1ODlr7Fc_B1aBywPIsNsWSTE-FgJ5kNo,1377
 polygraphy/backend/base/runner.py,sha256=mM7qxvIQ4796wvo01QyYWxfLZgJ5lbPiwcy2Jv5Ab4s,9847
 polygraphy/backend/base/util.py,sha256=4ty5Mpd-gYzgy2xyOM6s2Nowrc-Gk_j0E2EC9GUD90Y,1990
 polygraphy/backend/common/__init__.py,sha256=7_awR1NnUHxfxPv-zBHez_ZBJFynOyxmg3S_HwrC-HY,47
 polygraphy/backend/common/loader.py,sha256=qpL3lGQN41r8eFIfQcFdNROb20SvNDMrMBSDeda3M0Q,2925
 polygraphy/backend/onnx/__init__.py,sha256=WjVLF43HwLa3mbeFGaTRKLYTWL8RPPc9zViDmjQXFLw,45
-polygraphy/backend/onnx/loader.py,sha256=iXCC-u7HKO26tBg2peqsVc1RjrDeCuY_axk3A2NtsP8,38138
+polygraphy/backend/onnx/loader.py,sha256=ttAErm9r5r0aJcmJrEKzlDZLL3Qe-7oCjh0Rvkd1TKw,38293
 polygraphy/backend/onnx/util.py,sha256=jH37nO-3ZT5U6-9wOF3nUwgHKYdp502eW8mihipTvSM,17261
 polygraphy/backend/onnxrt/__init__.py,sha256=wcULQDSo7XMp74bYyxtcEmpdNfVpE-mJAVkWNalutN0,94
 polygraphy/backend/onnxrt/loader.py,sha256=NAh3h5RYcvkLCn-54BWeSgJ-PLEpL6E0xLSZDi2pcEQ,2796
-polygraphy/backend/onnxrt/runner.py,sha256=I_6WyK12rIo962p70u3yjuO17vtSsRo6g7SW7XQDMAI,3368
+polygraphy/backend/onnxrt/runner.py,sha256=nBqE7-IvZ6qKbEa6pB11cRksNgrjq-3w_HWW6JxCZgg,3431
 polygraphy/backend/pluginref/__init__.py,sha256=9XwcYNKJv3fMJ58nTQLrnAsV7TZjIPtgh7FuImVM0e4,664
 polygraphy/backend/pluginref/references.py,sha256=WRH2uOevF46MCsVdbboVokFjy_BXbbEo7bAZSCtxoxU,3471
 polygraphy/backend/pluginref/runner.py,sha256=76-lxUvB5vgbQT1_eOLKF8gafkJOdGI1DEs11EO7xrU,2712
 polygraphy/backend/pyt/__init__.py,sha256=jlV_pLqQhJ4fDLcf7eJTLI73DY8JHNNTLmCJoJeecsg,44
 polygraphy/backend/pyt/runner.py,sha256=0IEFzMjbXzRMqstJI80nXiV_3g39s7cnskIb5eMabvk,2830
-polygraphy/backend/tf/__init__.py,sha256=9-5XbleP4NKNoNxiSCBRmepNZL8W8ru4_Y-Ty6KG5cE,1154
+polygraphy/backend/tf/__init__.py,sha256=45nSvrcR6YjuePWnot6LLcIqIR-mK6Bkv5F2jWrDigw,1172
 polygraphy/backend/tf/loader.py,sha256=hMBnhpn456hJh_fi5lKg_fAr1QCFDz4j8HfQk03OqSs,18232
 polygraphy/backend/tf/runner.py,sha256=YfV8NY7MIYe3JOGd7_w0GLgHLphpQ67n9CjLMD-nHWc,3731
 polygraphy/backend/tf/util.py,sha256=nzjJCxydOC1Ivwj7rEE7BByvGFUzbayVapiCauxvor4,6966
 polygraphy/backend/trt/__init__.py,sha256=73nad0rkUSCmk_RmHgDmeRMrVUerFuo0Z99JHLU8-d0,323
-polygraphy/backend/trt/algorithm_selector.py,sha256=mcWfcWCpPDV0O1ZfNSW5mO-nkWgJay6sbuOX-qNVPNk,14550
+polygraphy/backend/trt/algorithm_selector.py,sha256=LEKoV5bt7KINpDnhyzJBX3pR8vV-0onhp0x-Na-RdOI,14280
 polygraphy/backend/trt/calibrator.py,sha256=ODJ48DN9_bV9ICV99U3L-MKN5X2kprsDHphj7uBK8zw,10266
-polygraphy/backend/trt/config.py,sha256=AAuksfxEXTIX3LqwgdbTWxnksKbnDceQCa481T_se18,23294
-polygraphy/backend/trt/loader.py,sha256=i0FzVr577XnV84YLwttA_acRhiE39DLqYcRTlQjiD_w,31818
+polygraphy/backend/trt/config.py,sha256=n04jdePNtBTkUDQd16DVK86mH7x_IncKEa3DlhS7SpA,25025
+polygraphy/backend/trt/loader.py,sha256=BmN6-NULTR7wxpchYLe0_7dH5qmEkBDO9SdfnG7ecrw,34597
 polygraphy/backend/trt/profile.py,sha256=w_oXpOBYhR-xrzViOGvkMbG1TosTmzjYRP9cjRs_Cek,8187
-polygraphy/backend/trt/runner.py,sha256=TQ-dI2E3IhzvZJRagNE6a-7ky8dWLOtJ3eyJCciW3i4,14820
-polygraphy/backend/trt/util.py,sha256=mX5HmKob2A74-RfNovbugpKoUqF0IdESyJHz6K_DuTs,31210
+polygraphy/backend/trt/runner.py,sha256=acoGfMRA6pIW4EDNmfI0hIpsCdVpDYebGnzdXyMH4Rk,21627
+polygraphy/backend/trt/util.py,sha256=cxVmSAvTZH6b6XTxTBWaln7-o85dl7StK3F_PmmMQ4U,33219
 polygraphy/common/__init__.py,sha256=dW1Fgh-pXCxFEv7mKxKVa33AuoUnb2Xamp64Blcp-HU,74
 polygraphy/common/interface.py,sha256=41dr03BfIof3voiisynHyJnBOdci_mhHLHLnKu4BGhA,6173
-polygraphy/common/struct.py,sha256=ZRh9wFT-jeJC5FQjUjGu8i8PAyl6GyuPA8V8emAVNLQ,6465
+polygraphy/common/struct.py,sha256=CnU0xzNzHqbcOTixUaWFS43TDfQjN93fmYQtRlQarck,6504
 polygraphy/comparator/__init__.py,sha256=QKtBpvd38mVajYVADfKrqlEv4CWjgEnUVKJBMYcSHgI,230
 polygraphy/comparator/comparator.py,sha256=i5H6So57f5WHLYQpa1b8xaGgNn7Y08GmO8mpdekVV7Q,18853
-polygraphy/comparator/compare.py,sha256=o4QjJQMR-msM0qVVvAEPbteAtECZQOYR-CXGWxkp3KA,33310
-polygraphy/comparator/data_loader.py,sha256=g4suL2Oq5c-DOxN0Ob7gvjaIglfTpaH-HkZMfdIrD94,20292
+polygraphy/comparator/compare.py,sha256=Mue6fJYTee6Hg6pPJt7VsCWsVbLgF-9ehQZd8NgghGY,35137
+polygraphy/comparator/data_loader.py,sha256=QvdF2zMbKZzqC93_3z6W0lyFJDKQ_SF0huWW1-eQsZc,24486
 polygraphy/comparator/postprocess.py,sha256=3Jnn2FpySBTfzFQXzAL7LPHec4MFjqzgvTSdJIV3hkg,2531
 polygraphy/comparator/struct.py,sha256=zJCsTICHU_bt070jqPQ_Rmlu0oQFGK3M0WTPFHteovk,13689
 polygraphy/comparator/util.py,sha256=cAL_8fvSCd4QHB_No7dALTh5cehv6mvxjZyKmMMJpjA,13976
 polygraphy/cuda/__init__.py,sha256=jdyfrXN-_49ao_DeZigz2IMufGX4e3A1fP0mksnYqj0,35
 polygraphy/cuda/cuda.py,sha256=gnWzVfFJbIfgQvSTTUoh-RhMhpeGGN1vIWLWADQJUjA,16567
 polygraphy/datatype/__init__.py,sha256=WV9kaVKjzCVar8JNoY153iNkpVtBuOs-tzIEOLYaois,246
-polygraphy/datatype/datatype.py,sha256=ss3JkUxgdJXvCeXacoysK_Wv_XJxNB5lo6GGpgwM1E4,9352
-polygraphy/datatype/numpy.py,sha256=Rvgiy4jkvWUUP8OJ87XKkZUxhmdUE8ZAHfZ3z-RV-uk,2127
-polygraphy/datatype/onnx.py,sha256=PENLKVl7mnStAL_lxX4c9XvHKk8CSp2vE8lPMGreD3k,2442
+polygraphy/datatype/datatype.py,sha256=wCcwUMQ29i5hsstOeW6biw2DiElxEDCVLegmjSZ3EWQ,9555
+polygraphy/datatype/numpy.py,sha256=c8L4hziTAKk6iT0QS7HckNv9pApqX1YYPE4WKKWT0Vc,2150
+polygraphy/datatype/onnx.py,sha256=Zbnm1qYrZ5k4buE_zSFTgshcX2XnjI8TABKTF6e4wqA,2470
 polygraphy/datatype/onnxrt.py,sha256=62iCKCZaBLYv5Yopc4dBR5DmMndEOvV6vrI9Avbnosw,2237
-polygraphy/datatype/tensorrt.py,sha256=8QBD70V3aTW9o_Fr8z9oAvyOcBTve-j5kZV7kCPJZ0Y,2120
-polygraphy/datatype/torch.py,sha256=nPxkCqSY9PjrX7vcjgBBw5Fy-BPMkjiK1UYrKeovU8Q,1901
+polygraphy/datatype/tensorrt.py,sha256=c6v5u2Lx1qVhWWmNkuixORzBHIMfs9awoQV3r0-SBKI,2213
+polygraphy/datatype/torch.py,sha256=PDXKj-PiqYQQp6av86VJDQg3ERIswcwStSTUkU7uSpY,1930
 polygraphy/exception/__init__.py,sha256=z-x8_fzZsfI3G6vAqUCgpSp_Ue7_txqF8jI8SeT-3Kg,45
 polygraphy/exception/exception.py,sha256=NOGuTAIM9Cy_IC_Z5lYwqcZis9BuwNtBuif1iQkduIc,1681
 polygraphy/func/__init__.py,sha256=yRioHr5HwhapEl9GkgwLFMu0LvohbrA0NGc_UCH6oag,35
 polygraphy/func/func.py,sha256=1F_3ultLmhFCzryzCeWfwdMarSE4unFO4nHpLvtIAdA,6479
 polygraphy/json/__init__.py,sha256=4rQLiJjz4LHmCKQhrBrpofKFmvY3YXnbfoj-lL7NfW8,36
-polygraphy/json/serde.py,sha256=ZWTJADRxXw5E-zFoZScvxWk9_6jtDqHQp8Qc1GH0Ysk,13356
+polygraphy/json/serde.py,sha256=WztM7FvsOFvertV9jB3ch8uAjrjoPf-Ga1P4cf4rDQE,13440
 polygraphy/logger/__init__.py,sha256=AdcCdmePOuclyrJXzXzlLGIgizSEhKgBlCKTG1-4Hqw,55
 polygraphy/logger/logger.py,sha256=pOWjzwG2Q9wlJg4Yu8V9NoUFR0V7-QVLO_0PTgEOSbs,24130
 polygraphy/mod/__init__.py,sha256=WoWko0oacps7_aoRoHt6Iu02Kwupu_sr7KhvokyuUNU,116
 polygraphy/mod/exporter.py,sha256=sA3lQUauPcruwyo1wjVcjDtnNSozV1iX-u4HVrC_Gz8,12827
-polygraphy/mod/importer.py,sha256=i2V4FzzuCG5bP6Lv9qkHYeIqtRqal_3fRNji6jPyhMU,11004
+polygraphy/mod/importer.py,sha256=w6u14Lhkr7KBM8-lHGdA7FjD9sgZciRKxwg49VJShxI,12917
 polygraphy/mod/util.py,sha256=5lRgnth-D64RqZ-uECFW6wt3RZ5l5VqkmclsB1HHczM,1418
 polygraphy/tools/__init__.py,sha256=yaWWN2YPmBMp1nyKhg_Nt4NSUHjvBO1GkfTxZR9kKKc,125
-polygraphy/tools/_main.py,sha256=qMNALPuhNGeGAt25yd51hlcGs0yzlVMIwUH6cyKz6QI,2534
-polygraphy/tools/registry.py,sha256=RjOCKomUtIcbeeJWGpS8vQNjY0jJnoCxZE_e7nRzMEA,2504
+polygraphy/tools/_main.py,sha256=ztRst3_pwd3drppUI0uYXqHxNH9VC3oTZn4U4Oly0u0,2650
+polygraphy/tools/registry.py,sha256=6HjU_cohIiGvmioiCKK5hJZ2eWmbAEf4Ye9fAbpvX1I,2559
 polygraphy/tools/script.py,sha256=xc2MQw9PH_sEDI0QcoS9k_FEHwdDJ-Zzmy3ZENzUUsA,15742
-polygraphy/tools/sparse.py,sha256=cB0-7ab4KzU0XnthmVWFSDupfGvpnviXpIFX-WCBIq8,13359
+polygraphy/tools/sparse.py,sha256=VoPfDgGibAnWlRiGwGg1i8qvVg2VcxKAuEy86ftqRJ8,13789
 polygraphy/tools/util.py,sha256=av9EujtsHXCBm6VHK8sN-GEOqLPWIK-ObCOcDy6qf-w,1903
 polygraphy/tools/args/__init__.py,sha256=-MEyjoL4KkoEA6EfRFbNGq4rxpfTjihjamqESqUMe_0,789
 polygraphy/tools/args/base.py,sha256=OW9vCj0W3uTpJSS6H5wvjsjlQ6mWpmaX-b4Hyka8jgk,7412
 polygraphy/tools/args/model.py,sha256=ap7ba0QRfJd3MVXueg-GLncFjJV36FiFhS1z0wwvYgQ,9278
 polygraphy/tools/args/backend/__init__.py,sha256=Jt2kKc4GHqydiJAmQmGVQcJ0A1pb2R6m6OOq4jdfkRY,920
 polygraphy/tools/args/backend/runner_select.py,sha256=Ju6e8Ybkn29Uy4C_sDCkST7bZhvRh9qLigCJMpEiKQA,3516
 polygraphy/tools/args/backend/onnx/__init__.py,sha256=4ERFLztpbj6utTTUEa_FArEPXJ7B_fWpaUpWXbqcoEA,669
@@ -81,33 +81,33 @@
 polygraphy/tools/args/backend/pluginref/__init__.py,sha256=Fvkt1mrvGKvOSvmI3-EGa44pfF5GlFLYdlNq7k5qcw8,674
 polygraphy/tools/args/backend/pluginref/runner.py,sha256=TwoVzH7n_P2HgxVSgg9bYOVQCN3QwoYooi60Pfx2NT4,1629
 polygraphy/tools/args/backend/tf/__init__.py,sha256=PBHaTxYTdNZAUSjCLrMRpAogkT6_FImSG9QM6MH0l4U,775
 polygraphy/tools/args/backend/tf/config.py,sha256=plOcYiPF4WYPQfL0RyQWqw-Ho-6B7r6apUvPFUFJ3qI,2771
 polygraphy/tools/args/backend/tf/loader.py,sha256=Z1OApHIKEekQ0oI9YxvXSaRtbWKQQBxaCSxLOu-aYZI,10069
 polygraphy/tools/args/backend/tf/runner.py,sha256=JqByuk7tT7pHtfIDeMKfw8A6bRdadhCf2hO-r0m4ATo,2432
 polygraphy/tools/args/backend/trt/__init__.py,sha256=wUCAeBkhzDs6zDY3Xy8wfEIX9wh5suDP71w6rTjQdGU,778
-polygraphy/tools/args/backend/trt/config.py,sha256=B2OWWsTFX5B0t64VXNQ81pm1UaCfOmusgky0IaDxb1E,35966
+polygraphy/tools/args/backend/trt/config.py,sha256=vaUsQ9xqZXFueQmufc5UqAPeSyZ7viijtbSZqYFqUOY,37698
 polygraphy/tools/args/backend/trt/helper.py,sha256=H4vjShXvAWf5XDSs_yTaoW6nomJPHbqrJiTxaiNwnG4,1181
-polygraphy/tools/args/backend/trt/loader.py,sha256=QlD8dLlZy0JeeBeV7cLdZ2njX1rzZvolK0vwT3ho_mo,27106
-polygraphy/tools/args/backend/trt/runner.py,sha256=m0_jjMZiE9bFjlP9i-wuYuGgbz8OeC7qGlcRuM-6va0,2126
+polygraphy/tools/args/backend/trt/loader.py,sha256=xilC0C9l5rM3__UnQYFU4XlmDGe1NmbIH7Bv3iwf72c,28365
+polygraphy/tools/args/backend/trt/runner.py,sha256=sAt49ZnGyxe_HjgiKes5ygRQOip9C8M4UP7qkoMW50k,4428
 polygraphy/tools/args/comparator/__init__.py,sha256=KcS1CzThiWZo4awWt7PU4jBl7xr_w6DV87HR4oupLVw,844
 polygraphy/tools/args/comparator/comparator.py,sha256=zE1spvm5hDC-v_6-y7SejE6jXzDk5a2Ga-x3xJECzUQ,11098
 polygraphy/tools/args/comparator/compare.py,sha256=LBC-7yJ8Hd3Eaed08kmJJFamxFxsGr5DpWFYfMZB9K4,10757
-polygraphy/tools/args/comparator/data_loader.py,sha256=PaGmum3gD0zRpvlBfHatXS7M39Z9_DTPvAASLnuhyNY,12332
+polygraphy/tools/args/comparator/data_loader.py,sha256=h8GhqNtrWpTmUw_m02c_cE6GERkBZ9cPNuRjYQIwoNI,12943
 polygraphy/tools/args/comparator/postprocess.py,sha256=FBNZAGcXB5a2NRbLttPoc0VSKVAs8OgTk9rBokpyx84,4070
 polygraphy/tools/args/logger/__init__.py,sha256=YXr5Jw361m9yDu6wxLOdylQDOxGaUz0r-2YiGYTCv1U,663
 polygraphy/tools/args/logger/logger.py,sha256=QQwkeh39tYiOXiljbnJQLR2TE3VUS-gzet24Oq-5nqI,6675
 polygraphy/tools/args/util/__init__.py,sha256=2-V_JbelADBBXVbaFBHyVOXAkN9Ondy4Oi-mDFo0yNQ,46
 polygraphy/tools/args/util/util.py,sha256=wR73fHyxr8rBfu6Wud1goxwC5GjcKpWevvmwNfr8L3E,12719
 polygraphy/tools/base/__init__.py,sha256=JpIympVeBMmHCtZ0QJB4uqAXGf6JiZcfDcxzTMaFYos,41
 polygraphy/tools/base/tool.py,sha256=7WPYgprFVM_S9eEgaD2vZx8pNJOD5o64Z_4KO71p7MI,7257
 polygraphy/tools/check/__init__.py,sha256=t2TdjiG2Qa_YZmwBpFiXLFJ2xQewgV1QHBgt-6Z9QZo,47
 polygraphy/tools/check/check.py,sha256=kvPKWmE2UOhxk1O2iMb1FwsV98RKqw2SBNVXMTynTDg,1020
 polygraphy/tools/check/subtool/__init__.py,sha256=0t8FTxC1qOjDaXI6w9IpIAq-2f_Psg5At36ISI6pEgA,53
-polygraphy/tools/check/subtool/lint.py,sha256=mV2408xrFb5kK77lkgXh3vFJmHY5eBGEW0CUoylx9UE,41086
+polygraphy/tools/check/subtool/lint.py,sha256=g5Z60qPENvlt8CPqzUuM9zIcumE-dtPwj7j252PjwF8,41313
 polygraphy/tools/convert/__init__.py,sha256=ucdWjwKwdXo_Z6HfUidPvgstuZeRXn1zn5QZNwWRRXU,53
 polygraphy/tools/convert/convert.py,sha256=yJ6nmPC2OyUcMmGtQBmMFqO0hKNAEioRbZW7u1bZxnI,4062
 polygraphy/tools/data/__init__.py,sha256=LWuHCubTQo_FZfD1Hw2RnF1a-zmqJMmJTTMU_cf1G1E,44
 polygraphy/tools/data/data.py,sha256=vJ9ADUTbotn8YwCPfy_mck21Hxskf4-GV0inbLkpVmE,1062
 polygraphy/tools/data/subtool/__init__.py,sha256=JnzdDW83DQPQTTFOGmYI8VePl9jGvC1TNJ57-j-8f5c,59
 polygraphy/tools/data/subtool/to_input.py,sha256=lwuVCzteyhxHduqRt8mX3KRzBglG4tcqHWY8SJPJNcA,3025
 polygraphy/tools/debug/__init__.py,sha256=XQLaJCw7R93figVnlL1ek7lp-XZVgzSB21ldsJrVMNs,47
@@ -118,40 +118,49 @@
 polygraphy/tools/debug/subtool/iterative_debug_args.py,sha256=9qVpwlNZ_rly6yuIyIlFig0OellPOnaub1Wr3xJeiC0,31725
 polygraphy/tools/debug/subtool/precision.py,sha256=HXtCvwSkv4eZr3bM-r7hB5cCL2Z27Cgg6SJpRDBdzpQ,9283
 polygraphy/tools/debug/subtool/reduce.py,sha256=k2R00C7T1vn6f1ajZDMSo12RB1aNFokUYM7mw9XsGNM,24308
 polygraphy/tools/debug/subtool/repeat.py,sha256=MrJdh9cTCnEXg14RJ2Y57CfRbJHhZqZxCoY5C38Ghoo,1378
 polygraphy/tools/inspect/__init__.py,sha256=_42OKjlqVNqbOqLVeghf_VFnzJk5srhqoBwEOy2YRcE,53
 polygraphy/tools/inspect/inspect.py,sha256=oOFx-JhpBc38FluD6KFNsQZ8aLusDN8mMg3B3NVZigM,1204
 polygraphy/tools/inspect/subtool/__init__.py,sha256=xB2AVaLO4wJtTr6fYu0G3kjEOofiXQuHGd86JHok8qE,373
-polygraphy/tools/inspect/subtool/capability.py,sha256=u-slVj-c7Vfi8pLe7d-CGCTjYmXWaQxO80ozfQpaXo4,10325
+polygraphy/tools/inspect/subtool/capability.py,sha256=MyF75_DnZqjeX-eEgcgeoQNQCAwL1fmhW_mG6SJSwO0,14372
 polygraphy/tools/inspect/subtool/data.py,sha256=__DDM_R6c5WRTUUjYAguwMT4yai9ngEysqB8hc8puFU,5413
 polygraphy/tools/inspect/subtool/diff_tactics.py,sha256=6iOPz0MGDkDC_6AWT-PHX3kQYzZH9_tkbi-d4mMmES4,4814
 polygraphy/tools/inspect/subtool/model.py,sha256=ffdrdcITvuezwNgDZmQztrkQbDdGDSUDR-PYreb1jR4,5954
 polygraphy/tools/inspect/subtool/sparsity.py,sha256=DMwQ0hhYrRB8HUR5AjygOp4pIil9g8j8FMv3hIwvmQc,1564
 polygraphy/tools/inspect/subtool/tactics.py,sha256=RldPbXADzlpB1ZGglMGbw4bz_Yz7eakdUQPWd_4giTA,1377
+polygraphy/tools/plugin/__init__.py,sha256=UPnafidkJZHOOQKgZhU7vcAcH7VD5jo-kuQrMCkUMJ8,50
+polygraphy/tools/plugin/plugin.py,sha256=YDJxPx6m-Gd21YmSr8rWAV5frAVQf3qhaIidR0MDqWw,1096
+polygraphy/tools/plugin/subtool/__init__.py,sha256=ihwmBISvoccNZIUsFJE64NnpfZMRbRTgFl_nf_dLKkg,252
+polygraphy/tools/plugin/subtool/list_plugins.py,sha256=DZegHBiEUaHzrNK1fDMvJc1S6Rp8Iiv_o_So79tdWy0,1177
+polygraphy/tools/plugin/subtool/match.py,sha256=ll0BDlTMQnHO36BtysOS7f5XfQA3tw1lZLFqqqg84Fk,1404
+polygraphy/tools/plugin/subtool/plugin_base.py,sha256=5nRA6_2zT5xmDjeWqgYOdOPNOiRVnsini_159JGwsi4,5125
+polygraphy/tools/plugin/subtool/replace.py,sha256=Gjtg_JUjJlrPlGEJS3O1OnfJx-Y8S24vrJNtlbNmS8g,3810
 polygraphy/tools/run/__init__.py,sha256=JYkqxbwlGlZkteMoFIUgY8x_wZmxfPMLXcTBiyZP3GQ,41
 polygraphy/tools/run/run.py,sha256=Y-UnaHYBWacioIBNYnJimdAWVgrzyHBQ4g8fCzuin4c,7909
 polygraphy/tools/surgeon/__init__.py,sha256=CJh1BCk6MxuiGSlDfeFd93x8T6BNJVKubYLOei33FWk,53
-polygraphy/tools/surgeon/surgeon.py,sha256=fui8QaPu_aDIFoGmC_WdrlibUseAyK4ggKXIoAqa6JU,1183
-polygraphy/tools/surgeon/subtool/__init__.py,sha256=woDL3jgwhB8jJbXTic4s8jfDIMeRWjAJsI0I9M4shPM,240
+polygraphy/tools/surgeon/surgeon.py,sha256=Zs35YD9IcTuhH7SfgvY3-WURhz4hF62_Dfw3CxjvLkM,1285
+polygraphy/tools/surgeon/subtool/__init__.py,sha256=xyrBJH0GMLQv3b-vivut5wV8_fdELBBCEw9Pn-PulXA,397
 polygraphy/tools/surgeon/subtool/base.py,sha256=10R-86I5-jmRLzvdTM02fP_u5Z6iuU2v1iX0W-GXURA,3370
 polygraphy/tools/surgeon/subtool/extract.py,sha256=2-XOz6PWOpUfqMia4Wx0DpF6cjL1txDyfdg28HDpXbo,8488
 polygraphy/tools/surgeon/subtool/insert.py,sha256=VjSrHip7fsZDvqP-dfGYf6lx3P1EenMcGWQZ9CFTfzE,6427
 polygraphy/tools/surgeon/subtool/prune.py,sha256=WDw9Vit0-b4iDYRnXAMUBc11L0qJ9PxNsL8cl5d5VsE,1979
 polygraphy/tools/surgeon/subtool/sanitize.py,sha256=x79mMX7KuLSRS2uQRoAuGXWFiN39tcCoSmDFljqXNw4,10051
+polygraphy/tools/surgeon/subtool/weight_reconstruct.py,sha256=PYt8zhX8TmiUxgZn3kgboJ_ip_49ULGU6wcLcvMBZ7U,4063
+polygraphy/tools/surgeon/subtool/weight_strip.py,sha256=PsfXeCJ6VuE6pgU9I-QZWk3IWxeQyhCQX7Rjj5UfjRs,16064
 polygraphy/tools/template/__init__.py,sha256=gK3ums1mZVaYP3tis6D79rdpH3joxE2tLqHrQuI8qhY,56
 polygraphy/tools/template/template.py,sha256=b-26R4gRPP8BZjghbZWafcrP1MZG58XCFbgUy-paugw,1104
 polygraphy/tools/template/subtool/__init__.py,sha256=53V75xK_C_3G6pXaA5PKULE-pjf-umBQz9mfxPhemew,197
 polygraphy/tools/template/subtool/base.py,sha256=ZYm6Hm-GZ_5ublWinJvijZgYxYOxuo6RWZ_CTNvhfD8,1033
 polygraphy/tools/template/subtool/onnx_gs.py,sha256=T7LDt-igSICwtEkhWAta03mEHQsO4UnW5bjwpq41l-c,3058
 polygraphy/tools/template/subtool/trt_config.py,sha256=7SSLKSKCTGbbYrDgnxFJfCXBEZlospbm1PdRuhIdZeQ,2298
 polygraphy/tools/template/subtool/trt_network.py,sha256=8D0k_MLoHA8gSSqQ0DIg2LJJy4yX1mIeTrwRruupVGE,2680
 polygraphy/util/__init__.py,sha256=dbQEYq9OR0nAOuoFFUs8jbzM0l2_q8dAg6Kff8aI_IU,64
-polygraphy/util/array.py,sha256=KJIUCXQgpJXtvErPZYwgEwdrEZtRDoEKYC2urvb5t8Y,34780
+polygraphy/util/array.py,sha256=sZAbW4Rx6He-VO5G7P307OCyraUJwxW4DoF1hO9gSO8,35314
 polygraphy/util/util.py,sha256=AT8GQAdfMhPAtr88iGEe_dJeOQYhkfue4G7bdqWr5CU,31070
-polygraphy-0.49.0.dist-info/LICENSE.txt,sha256=Mu9m8UiBeV87YWYdfsoIBiNvtvZcFPAFO1ebLbohV2Y,10764
-polygraphy-0.49.0.dist-info/METADATA,sha256=Xr4X7EcHwEIjJesKu5NGutR7XpaKVB8wgjEhGKMeaoQ,5823
-polygraphy-0.49.0.dist-info/WHEEL,sha256=z9j0xAa_JmUKMpmz72K0ZGALSM_n-wQVmGbleXx2VHg,110
-polygraphy-0.49.0.dist-info/entry_points.txt,sha256=lNgs0y7uzFcUt4CkaQxUy6slJ8OXk3UAdAY3rlUCOFU,54
-polygraphy-0.49.0.dist-info/top_level.txt,sha256=3tcghWlov9chtrSXn8qZlbW38eRjAWGkS_f8e6oyWaI,11
-polygraphy-0.49.0.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-polygraphy-0.49.0.dist-info/RECORD,,
+polygraphy-0.49.9.dist-info/LICENSE.txt,sha256=Mu9m8UiBeV87YWYdfsoIBiNvtvZcFPAFO1ebLbohV2Y,10764
+polygraphy-0.49.9.dist-info/METADATA,sha256=lh8ObydU6CK8KvYB0c9QQQFvsMDg1Zs0jYplXRAWEms,5803
+polygraphy-0.49.9.dist-info/WHEEL,sha256=z9j0xAa_JmUKMpmz72K0ZGALSM_n-wQVmGbleXx2VHg,110
+polygraphy-0.49.9.dist-info/entry_points.txt,sha256=6PVINcDo7tE7g8prTTwIC4gnQ4qRR5MAr2ghoNIYSdI,53
+polygraphy-0.49.9.dist-info/top_level.txt,sha256=3tcghWlov9chtrSXn8qZlbW38eRjAWGkS_f8e6oyWaI,11
+polygraphy-0.49.9.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+polygraphy-0.49.9.dist-info/RECORD,,
```

