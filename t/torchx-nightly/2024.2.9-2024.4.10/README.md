# Comparing `tmp/torchx_nightly-2024.2.9-py3-none-any.whl.zip` & `tmp/torchx_nightly-2024.4.10-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,121 +1,122 @@
-Zip file size: 246212 bytes, number of entries: 119
--rw-r--r--  2.0 unx      340 b- defN 24-Feb-09 11:18 torchx/__init__.py
--rw-r--r--  2.0 unx      993 b- defN 24-Feb-09 11:18 torchx/notebook.py
--rw-r--r--  2.0 unx      936 b- defN 24-Feb-09 11:18 torchx/version.py
--rw-r--r--  2.0 unx      231 b- defN 24-Feb-09 11:18 torchx/apps/__init__.py
--rw-r--r--  2.0 unx      208 b- defN 24-Feb-09 11:18 torchx/apps/serve/__init__.py
--rw-r--r--  2.0 unx     4371 b- defN 24-Feb-09 11:18 torchx/apps/serve/serve.py
--rw-r--r--  2.0 unx      208 b- defN 24-Feb-09 11:18 torchx/apps/utils/__init__.py
--rw-r--r--  2.0 unx     1412 b- defN 24-Feb-09 11:18 torchx/apps/utils/booth_main.py
--rw-r--r--  2.0 unx     1823 b- defN 24-Feb-09 11:18 torchx/apps/utils/copy_main.py
--rw-r--r--  2.0 unx     3437 b- defN 24-Feb-09 11:18 torchx/apps/utils/process_monitor.py
--rw-r--r--  2.0 unx    10336 b- defN 24-Feb-09 11:18 torchx/cli/__init__.py
--rw-r--r--  2.0 unx     2836 b- defN 24-Feb-09 11:18 torchx/cli/argparse_util.py
--rw-r--r--  2.0 unx      775 b- defN 24-Feb-09 11:18 torchx/cli/cmd_base.py
--rw-r--r--  2.0 unx      820 b- defN 24-Feb-09 11:18 torchx/cli/cmd_cancel.py
--rw-r--r--  2.0 unx     1707 b- defN 24-Feb-09 11:18 torchx/cli/cmd_configure.py
--rw-r--r--  2.0 unx     1268 b- defN 24-Feb-09 11:18 torchx/cli/cmd_describe.py
--rw-r--r--  2.0 unx     1414 b- defN 24-Feb-09 11:18 torchx/cli/cmd_list.py
--rw-r--r--  2.0 unx     6089 b- defN 24-Feb-09 11:18 torchx/cli/cmd_log.py
--rw-r--r--  2.0 unx    10429 b- defN 24-Feb-09 11:18 torchx/cli/cmd_run.py
--rw-r--r--  2.0 unx     1287 b- defN 24-Feb-09 11:18 torchx/cli/cmd_runopts.py
--rw-r--r--  2.0 unx     1821 b- defN 24-Feb-09 11:18 torchx/cli/cmd_status.py
--rw-r--r--  2.0 unx     5203 b- defN 24-Feb-09 11:18 torchx/cli/cmd_tracker.py
--rw-r--r--  2.0 unx      553 b- defN 24-Feb-09 11:18 torchx/cli/colors.py
--rw-r--r--  2.0 unx     3469 b- defN 24-Feb-09 11:18 torchx/cli/main.py
--rw-r--r--  2.0 unx    12106 b- defN 24-Feb-09 11:18 torchx/components/__init__.py
--rw-r--r--  2.0 unx     4135 b- defN 24-Feb-09 11:18 torchx/components/component_test_base.py
--rw-r--r--  2.0 unx    14555 b- defN 24-Feb-09 11:18 torchx/components/dist.py
--rw-r--r--  2.0 unx      697 b- defN 24-Feb-09 11:18 torchx/components/interpret.py
--rw-r--r--  2.0 unx     2814 b- defN 24-Feb-09 11:18 torchx/components/metrics.py
--rw-r--r--  2.0 unx     2141 b- defN 24-Feb-09 11:18 torchx/components/serve.py
--rw-r--r--  2.0 unx     9554 b- defN 24-Feb-09 11:18 torchx/components/structured_arg.py
--rw-r--r--  2.0 unx     1259 b- defN 24-Feb-09 11:18 torchx/components/train.py
--rw-r--r--  2.0 unx     9025 b- defN 24-Feb-09 11:18 torchx/components/utils.py
--rw-r--r--  2.0 unx      208 b- defN 24-Feb-09 11:18 torchx/components/integration_tests/__init__.py
--rw-r--r--  2.0 unx     3980 b- defN 24-Feb-09 11:18 torchx/components/integration_tests/component_provider.py
--rw-r--r--  2.0 unx     5150 b- defN 24-Feb-09 11:18 torchx/components/integration_tests/integ_tests.py
--rw-r--r--  2.0 unx    10280 b- defN 24-Feb-09 11:18 torchx/distributed/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-09 11:18 torchx/examples/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-09 11:18 torchx/examples/apps/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-09 11:18 torchx/examples/apps/datapreproc/__init__.py
--rw-r--r--  2.0 unx     4302 b- defN 24-Feb-09 11:18 torchx/examples/apps/datapreproc/datapreproc.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-09 11:18 torchx/examples/apps/lightning/__init__.py
--rw-r--r--  2.0 unx     6583 b- defN 24-Feb-09 11:18 torchx/examples/apps/lightning/data.py
--rw-r--r--  2.0 unx     5256 b- defN 24-Feb-09 11:18 torchx/examples/apps/lightning/interpret.py
--rw-r--r--  2.0 unx     3932 b- defN 24-Feb-09 11:18 torchx/examples/apps/lightning/model.py
--rw-r--r--  2.0 unx     1926 b- defN 24-Feb-09 11:18 torchx/examples/apps/lightning/profiler.py
--rw-r--r--  2.0 unx     6084 b- defN 24-Feb-09 11:18 torchx/examples/apps/lightning/train.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-09 11:18 torchx/examples/pipelines/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-09 11:18 torchx/examples/pipelines/kfp/__init__.py
--rw-r--r--  2.0 unx     8416 b- defN 24-Feb-09 11:18 torchx/examples/pipelines/kfp/advanced_pipeline.py
--rw-r--r--  2.0 unx     2178 b- defN 24-Feb-09 11:18 torchx/examples/pipelines/kfp/dist_pipeline.py
--rw-r--r--  2.0 unx     2751 b- defN 24-Feb-09 11:18 torchx/examples/pipelines/kfp/intro_pipeline.py
--rw-r--r--  2.0 unx      606 b- defN 24-Feb-09 11:18 torchx/pipelines/__init__.py
--rw-r--r--  2.0 unx      721 b- defN 24-Feb-09 11:18 torchx/pipelines/kfp/__init__.py
--rw-r--r--  2.0 unx     8952 b- defN 24-Feb-09 11:18 torchx/pipelines/kfp/adapter.py
--rw-r--r--  2.0 unx      524 b- defN 24-Feb-09 11:18 torchx/pipelines/kfp/version.py
--rw-r--r--  2.0 unx      300 b- defN 24-Feb-09 11:18 torchx/runner/__init__.py
--rw-r--r--  2.0 unx    27090 b- defN 24-Feb-09 11:18 torchx/runner/api.py
--rw-r--r--  2.0 unx    17171 b- defN 24-Feb-09 11:18 torchx/runner/config.py
--rw-r--r--  2.0 unx     4043 b- defN 24-Feb-09 11:18 torchx/runner/events/__init__.py
--rw-r--r--  2.0 unx     2132 b- defN 24-Feb-09 11:18 torchx/runner/events/api.py
--rw-r--r--  2.0 unx      507 b- defN 24-Feb-09 11:18 torchx/runner/events/handlers.py
--rw-r--r--  2.0 unx      593 b- defN 24-Feb-09 11:18 torchx/runtime/__init__.py
--rw-r--r--  2.0 unx     3040 b- defN 24-Feb-09 11:18 torchx/runtime/tracking/__init__.py
--rw-r--r--  2.0 unx     5457 b- defN 24-Feb-09 11:18 torchx/runtime/tracking/api.py
--rw-r--r--  2.0 unx     2157 b- defN 24-Feb-09 11:18 torchx/schedulers/__init__.py
--rw-r--r--  2.0 unx    14140 b- defN 24-Feb-09 11:18 torchx/schedulers/api.py
--rw-r--r--  2.0 unx    27960 b- defN 24-Feb-09 11:18 torchx/schedulers/aws_batch_scheduler.py
--rw-r--r--  2.0 unx     1352 b- defN 24-Feb-09 11:18 torchx/schedulers/devices.py
--rw-r--r--  2.0 unx    15456 b- defN 24-Feb-09 11:18 torchx/schedulers/docker_scheduler.py
--rw-r--r--  2.0 unx    16549 b- defN 24-Feb-09 11:18 torchx/schedulers/gcp_batch_scheduler.py
--rw-r--r--  2.0 unx     1783 b- defN 24-Feb-09 11:18 torchx/schedulers/ids.py
--rw-r--r--  2.0 unx    42885 b- defN 24-Feb-09 11:18 torchx/schedulers/kubernetes_mcad_scheduler.py
--rw-r--r--  2.0 unx    27068 b- defN 24-Feb-09 11:18 torchx/schedulers/kubernetes_scheduler.py
--rw-r--r--  2.0 unx    40928 b- defN 24-Feb-09 11:18 torchx/schedulers/local_scheduler.py
--rw-r--r--  2.0 unx    17638 b- defN 24-Feb-09 11:18 torchx/schedulers/lsf_scheduler.py
--rw-r--r--  2.0 unx    17448 b- defN 24-Feb-09 11:18 torchx/schedulers/ray_scheduler.py
--rw-r--r--  2.0 unx    19355 b- defN 24-Feb-09 11:18 torchx/schedulers/slurm_scheduler.py
--rw-r--r--  2.0 unx     1992 b- defN 24-Feb-09 11:18 torchx/schedulers/streams.py
--rw-r--r--  2.0 unx      231 b- defN 24-Feb-09 11:18 torchx/schedulers/ray/__init__.py
--rw-r--r--  2.0 unx      610 b- defN 24-Feb-09 11:18 torchx/schedulers/ray/ray_common.py
--rw-r--r--  2.0 unx    12282 b- defN 24-Feb-09 11:18 torchx/schedulers/ray/ray_driver.py
--rw-r--r--  2.0 unx     5462 b- defN 24-Feb-09 11:18 torchx/specs/__init__.py
--rw-r--r--  2.0 unx    34256 b- defN 24-Feb-09 11:18 torchx/specs/api.py
--rw-r--r--  2.0 unx     8526 b- defN 24-Feb-09 11:18 torchx/specs/builders.py
--rw-r--r--  2.0 unx    11714 b- defN 24-Feb-09 11:18 torchx/specs/file_linter.py
--rw-r--r--  2.0 unx    16234 b- defN 24-Feb-09 11:18 torchx/specs/finder.py
--rw-r--r--  2.0 unx     7728 b- defN 24-Feb-09 11:18 torchx/specs/named_resources_aws.py
--rw-r--r--  2.0 unx     2631 b- defN 24-Feb-09 11:18 torchx/specs/named_resources_generic.py
--rw-r--r--  2.0 unx      208 b- defN 24-Feb-09 11:18 torchx/specs/test/components/__init__.py
--rw-r--r--  2.0 unx      546 b- defN 24-Feb-09 11:18 torchx/specs/test/components/a/__init__.py
--rw-r--r--  2.0 unx      208 b- defN 24-Feb-09 11:18 torchx/specs/test/components/a/b/__init__.py
--rw-r--r--  2.0 unx      539 b- defN 24-Feb-09 11:18 torchx/specs/test/components/a/b/c.py
--rw-r--r--  2.0 unx      231 b- defN 24-Feb-09 11:18 torchx/specs/test/components/c/__init__.py
--rw-r--r--  2.0 unx      531 b- defN 24-Feb-09 11:18 torchx/specs/test/components/c/d.py
--rw-r--r--  2.0 unx     4350 b- defN 24-Feb-09 11:18 torchx/tracker/__init__.py
--rw-r--r--  2.0 unx    11254 b- defN 24-Feb-09 11:18 torchx/tracker/api.py
--rw-r--r--  2.0 unx    14487 b- defN 24-Feb-09 11:18 torchx/tracker/mlflow.py
--rw-r--r--  2.0 unx      231 b- defN 24-Feb-09 11:18 torchx/tracker/backend/__init__.py
--rw-r--r--  2.0 unx    10425 b- defN 24-Feb-09 11:18 torchx/tracker/backend/fsspec.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-09 11:18 torchx/util/__init__.py
--rw-r--r--  2.0 unx     1084 b- defN 24-Feb-09 11:18 torchx/util/cuda.py
--rw-r--r--  2.0 unx      390 b- defN 24-Feb-09 11:18 torchx/util/datetime.py
--rw-r--r--  2.0 unx     2710 b- defN 24-Feb-09 11:18 torchx/util/entrypoints.py
--rw-r--r--  2.0 unx     1792 b- defN 24-Feb-09 11:18 torchx/util/io.py
--rw-r--r--  2.0 unx     1116 b- defN 24-Feb-09 11:18 torchx/util/modules.py
--rw-r--r--  2.0 unx      448 b- defN 24-Feb-09 11:18 torchx/util/shlex.py
--rw-r--r--  2.0 unx      663 b- defN 24-Feb-09 11:18 torchx/util/strings.py
--rw-r--r--  2.0 unx     7016 b- defN 24-Feb-09 11:18 torchx/util/types.py
--rw-r--r--  2.0 unx      783 b- defN 24-Feb-09 11:18 torchx/workspace/__init__.py
--rw-r--r--  2.0 unx     5464 b- defN 24-Feb-09 11:18 torchx/workspace/api.py
--rw-r--r--  2.0 unx     2253 b- defN 24-Feb-09 11:18 torchx/workspace/dir_workspace.py
--rw-r--r--  2.0 unx     9410 b- defN 24-Feb-09 11:18 torchx/workspace/docker_workspace.py
--rw-r--r--  2.0 unx     1721 b- defN 24-Feb-09 11:26 torchx_nightly-2024.2.9.dist-info/LICENSE
--rw-r--r--  2.0 unx     5610 b- defN 24-Feb-09 11:26 torchx_nightly-2024.2.9.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Feb-09 11:26 torchx_nightly-2024.2.9.dist-info/WHEEL
--rw-r--r--  2.0 unx      170 b- defN 24-Feb-09 11:26 torchx_nightly-2024.2.9.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        7 b- defN 24-Feb-09 11:26 torchx_nightly-2024.2.9.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    10430 b- defN 24-Feb-09 11:26 torchx_nightly-2024.2.9.dist-info/RECORD
-119 files, 690988 bytes uncompressed, 229678 bytes compressed:  66.8%
+Zip file size: 254146 bytes, number of entries: 120
+-rw-r--r--  2.0 unx      355 b- defN 24-Apr-10 18:16 torchx/__init__.py
+-rw-r--r--  2.0 unx     1008 b- defN 24-Apr-10 18:16 torchx/notebook.py
+-rw-r--r--  2.0 unx      951 b- defN 24-Apr-10 18:16 torchx/version.py
+-rw-r--r--  2.0 unx      231 b- defN 24-Apr-10 18:16 torchx/apps/__init__.py
+-rw-r--r--  2.0 unx      208 b- defN 24-Apr-10 18:16 torchx/apps/serve/__init__.py
+-rw-r--r--  2.0 unx     4386 b- defN 24-Apr-10 18:16 torchx/apps/serve/serve.py
+-rw-r--r--  2.0 unx      208 b- defN 24-Apr-10 18:16 torchx/apps/utils/__init__.py
+-rw-r--r--  2.0 unx     1427 b- defN 24-Apr-10 18:16 torchx/apps/utils/booth_main.py
+-rw-r--r--  2.0 unx     1838 b- defN 24-Apr-10 18:16 torchx/apps/utils/copy_main.py
+-rw-r--r--  2.0 unx     3452 b- defN 24-Apr-10 18:16 torchx/apps/utils/process_monitor.py
+-rw-r--r--  2.0 unx    10351 b- defN 24-Apr-10 18:16 torchx/cli/__init__.py
+-rw-r--r--  2.0 unx     3836 b- defN 24-Apr-10 18:16 torchx/cli/argparse_util.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-10 18:16 torchx/cli/cmd_base.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 18:16 torchx/cli/cmd_cancel.py
+-rw-r--r--  2.0 unx     1722 b- defN 24-Apr-10 18:16 torchx/cli/cmd_configure.py
+-rw-r--r--  2.0 unx     1283 b- defN 24-Apr-10 18:16 torchx/cli/cmd_describe.py
+-rw-r--r--  2.0 unx     1429 b- defN 24-Apr-10 18:16 torchx/cli/cmd_list.py
+-rw-r--r--  2.0 unx     6104 b- defN 24-Apr-10 18:16 torchx/cli/cmd_log.py
+-rw-r--r--  2.0 unx    10527 b- defN 24-Apr-10 18:16 torchx/cli/cmd_run.py
+-rw-r--r--  2.0 unx     1302 b- defN 24-Apr-10 18:16 torchx/cli/cmd_runopts.py
+-rw-r--r--  2.0 unx     1836 b- defN 24-Apr-10 18:16 torchx/cli/cmd_status.py
+-rw-r--r--  2.0 unx     5218 b- defN 24-Apr-10 18:16 torchx/cli/cmd_tracker.py
+-rw-r--r--  2.0 unx      568 b- defN 24-Apr-10 18:16 torchx/cli/colors.py
+-rw-r--r--  2.0 unx     3484 b- defN 24-Apr-10 18:16 torchx/cli/main.py
+-rw-r--r--  2.0 unx    12121 b- defN 24-Apr-10 18:16 torchx/components/__init__.py
+-rw-r--r--  2.0 unx     4150 b- defN 24-Apr-10 18:16 torchx/components/component_test_base.py
+-rw-r--r--  2.0 unx    14570 b- defN 24-Apr-10 18:16 torchx/components/dist.py
+-rw-r--r--  2.0 unx      697 b- defN 24-Apr-10 18:16 torchx/components/interpret.py
+-rw-r--r--  2.0 unx     2814 b- defN 24-Apr-10 18:16 torchx/components/metrics.py
+-rw-r--r--  2.0 unx     2156 b- defN 24-Apr-10 18:16 torchx/components/serve.py
+-rw-r--r--  2.0 unx     9569 b- defN 24-Apr-10 18:16 torchx/components/structured_arg.py
+-rw-r--r--  2.0 unx     1259 b- defN 24-Apr-10 18:16 torchx/components/train.py
+-rw-r--r--  2.0 unx     9040 b- defN 24-Apr-10 18:16 torchx/components/utils.py
+-rw-r--r--  2.0 unx      208 b- defN 24-Apr-10 18:16 torchx/components/integration_tests/__init__.py
+-rw-r--r--  2.0 unx     3995 b- defN 24-Apr-10 18:16 torchx/components/integration_tests/component_provider.py
+-rw-r--r--  2.0 unx     5165 b- defN 24-Apr-10 18:16 torchx/components/integration_tests/integ_tests.py
+-rw-r--r--  2.0 unx    10280 b- defN 24-Apr-10 18:16 torchx/distributed/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-10 18:16 torchx/examples/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-10 18:16 torchx/examples/apps/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-10 18:16 torchx/examples/apps/datapreproc/__init__.py
+-rw-r--r--  2.0 unx     4317 b- defN 24-Apr-10 18:16 torchx/examples/apps/datapreproc/datapreproc.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-10 18:16 torchx/examples/apps/lightning/__init__.py
+-rw-r--r--  2.0 unx     6598 b- defN 24-Apr-10 18:16 torchx/examples/apps/lightning/data.py
+-rw-r--r--  2.0 unx     5256 b- defN 24-Apr-10 18:16 torchx/examples/apps/lightning/interpret.py
+-rw-r--r--  2.0 unx     3947 b- defN 24-Apr-10 18:16 torchx/examples/apps/lightning/model.py
+-rw-r--r--  2.0 unx     1941 b- defN 24-Apr-10 18:16 torchx/examples/apps/lightning/profiler.py
+-rw-r--r--  2.0 unx     6099 b- defN 24-Apr-10 18:16 torchx/examples/apps/lightning/train.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-10 18:16 torchx/examples/pipelines/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-10 18:16 torchx/examples/pipelines/kfp/__init__.py
+-rw-r--r--  2.0 unx     8431 b- defN 24-Apr-10 18:16 torchx/examples/pipelines/kfp/advanced_pipeline.py
+-rw-r--r--  2.0 unx     2193 b- defN 24-Apr-10 18:16 torchx/examples/pipelines/kfp/dist_pipeline.py
+-rw-r--r--  2.0 unx     2766 b- defN 24-Apr-10 18:16 torchx/examples/pipelines/kfp/intro_pipeline.py
+-rw-r--r--  2.0 unx      606 b- defN 24-Apr-10 18:16 torchx/pipelines/__init__.py
+-rw-r--r--  2.0 unx      736 b- defN 24-Apr-10 18:16 torchx/pipelines/kfp/__init__.py
+-rw-r--r--  2.0 unx     8959 b- defN 24-Apr-10 18:16 torchx/pipelines/kfp/adapter.py
+-rw-r--r--  2.0 unx      539 b- defN 24-Apr-10 18:16 torchx/pipelines/kfp/version.py
+-rw-r--r--  2.0 unx      315 b- defN 24-Apr-10 18:16 torchx/runner/__init__.py
+-rw-r--r--  2.0 unx    27101 b- defN 24-Apr-10 18:16 torchx/runner/api.py
+-rw-r--r--  2.0 unx    17838 b- defN 24-Apr-10 18:16 torchx/runner/config.py
+-rw-r--r--  2.0 unx     4058 b- defN 24-Apr-10 18:16 torchx/runner/events/__init__.py
+-rw-r--r--  2.0 unx     2147 b- defN 24-Apr-10 18:16 torchx/runner/events/api.py
+-rw-r--r--  2.0 unx      522 b- defN 24-Apr-10 18:16 torchx/runner/events/handlers.py
+-rw-r--r--  2.0 unx      593 b- defN 24-Apr-10 18:16 torchx/runtime/__init__.py
+-rw-r--r--  2.0 unx     3055 b- defN 24-Apr-10 18:16 torchx/runtime/tracking/__init__.py
+-rw-r--r--  2.0 unx     5472 b- defN 24-Apr-10 18:16 torchx/runtime/tracking/api.py
+-rw-r--r--  2.0 unx     2230 b- defN 24-Apr-10 18:16 torchx/schedulers/__init__.py
+-rw-r--r--  2.0 unx    14155 b- defN 24-Apr-10 18:16 torchx/schedulers/api.py
+-rw-r--r--  2.0 unx    27975 b- defN 24-Apr-10 18:16 torchx/schedulers/aws_batch_scheduler.py
+-rw-r--r--  2.0 unx    20699 b- defN 24-Apr-10 18:16 torchx/schedulers/aws_sagemaker_scheduler.py
+-rw-r--r--  2.0 unx     1367 b- defN 24-Apr-10 18:16 torchx/schedulers/devices.py
+-rw-r--r--  2.0 unx    15973 b- defN 24-Apr-10 18:16 torchx/schedulers/docker_scheduler.py
+-rw-r--r--  2.0 unx    16226 b- defN 24-Apr-10 18:16 torchx/schedulers/gcp_batch_scheduler.py
+-rw-r--r--  2.0 unx     1798 b- defN 24-Apr-10 18:16 torchx/schedulers/ids.py
+-rw-r--r--  2.0 unx    42885 b- defN 24-Apr-10 18:16 torchx/schedulers/kubernetes_mcad_scheduler.py
+-rw-r--r--  2.0 unx    27065 b- defN 24-Apr-10 18:16 torchx/schedulers/kubernetes_scheduler.py
+-rw-r--r--  2.0 unx    41341 b- defN 24-Apr-10 18:16 torchx/schedulers/local_scheduler.py
+-rw-r--r--  2.0 unx    17653 b- defN 24-Apr-10 18:16 torchx/schedulers/lsf_scheduler.py
+-rw-r--r--  2.0 unx    17448 b- defN 24-Apr-10 18:16 torchx/schedulers/ray_scheduler.py
+-rw-r--r--  2.0 unx    19370 b- defN 24-Apr-10 18:16 torchx/schedulers/slurm_scheduler.py
+-rw-r--r--  2.0 unx     2007 b- defN 24-Apr-10 18:16 torchx/schedulers/streams.py
+-rw-r--r--  2.0 unx      231 b- defN 24-Apr-10 18:16 torchx/schedulers/ray/__init__.py
+-rw-r--r--  2.0 unx      610 b- defN 24-Apr-10 18:16 torchx/schedulers/ray/ray_common.py
+-rw-r--r--  2.0 unx    12286 b- defN 24-Apr-10 18:16 torchx/schedulers/ray/ray_driver.py
+-rw-r--r--  2.0 unx     5477 b- defN 24-Apr-10 18:16 torchx/specs/__init__.py
+-rw-r--r--  2.0 unx    34736 b- defN 24-Apr-10 18:16 torchx/specs/api.py
+-rw-r--r--  2.0 unx    10178 b- defN 24-Apr-10 18:16 torchx/specs/builders.py
+-rw-r--r--  2.0 unx    11860 b- defN 24-Apr-10 18:16 torchx/specs/file_linter.py
+-rw-r--r--  2.0 unx    16249 b- defN 24-Apr-10 18:16 torchx/specs/finder.py
+-rw-r--r--  2.0 unx     7743 b- defN 24-Apr-10 18:16 torchx/specs/named_resources_aws.py
+-rw-r--r--  2.0 unx     2646 b- defN 24-Apr-10 18:16 torchx/specs/named_resources_generic.py
+-rw-r--r--  2.0 unx      223 b- defN 24-Apr-10 18:16 torchx/specs/test/components/__init__.py
+-rw-r--r--  2.0 unx      561 b- defN 24-Apr-10 18:16 torchx/specs/test/components/a/__init__.py
+-rw-r--r--  2.0 unx      223 b- defN 24-Apr-10 18:16 torchx/specs/test/components/a/b/__init__.py
+-rw-r--r--  2.0 unx      554 b- defN 24-Apr-10 18:16 torchx/specs/test/components/a/b/c.py
+-rw-r--r--  2.0 unx      246 b- defN 24-Apr-10 18:16 torchx/specs/test/components/c/__init__.py
+-rw-r--r--  2.0 unx      546 b- defN 24-Apr-10 18:16 torchx/specs/test/components/c/d.py
+-rw-r--r--  2.0 unx     4365 b- defN 24-Apr-10 18:16 torchx/tracker/__init__.py
+-rw-r--r--  2.0 unx    11257 b- defN 24-Apr-10 18:16 torchx/tracker/api.py
+-rw-r--r--  2.0 unx    14487 b- defN 24-Apr-10 18:16 torchx/tracker/mlflow.py
+-rw-r--r--  2.0 unx      231 b- defN 24-Apr-10 18:16 torchx/tracker/backend/__init__.py
+-rw-r--r--  2.0 unx    10440 b- defN 24-Apr-10 18:16 torchx/tracker/backend/fsspec.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-10 18:16 torchx/util/__init__.py
+-rw-r--r--  2.0 unx     1099 b- defN 24-Apr-10 18:16 torchx/util/cuda.py
+-rw-r--r--  2.0 unx      405 b- defN 24-Apr-10 18:16 torchx/util/datetime.py
+-rw-r--r--  2.0 unx     2725 b- defN 24-Apr-10 18:16 torchx/util/entrypoints.py
+-rw-r--r--  2.0 unx     1807 b- defN 24-Apr-10 18:16 torchx/util/io.py
+-rw-r--r--  2.0 unx     1131 b- defN 24-Apr-10 18:16 torchx/util/modules.py
+-rw-r--r--  2.0 unx      463 b- defN 24-Apr-10 18:16 torchx/util/shlex.py
+-rw-r--r--  2.0 unx      678 b- defN 24-Apr-10 18:16 torchx/util/strings.py
+-rw-r--r--  2.0 unx     7379 b- defN 24-Apr-10 18:16 torchx/util/types.py
+-rw-r--r--  2.0 unx      798 b- defN 24-Apr-10 18:16 torchx/workspace/__init__.py
+-rw-r--r--  2.0 unx     5479 b- defN 24-Apr-10 18:16 torchx/workspace/api.py
+-rw-r--r--  2.0 unx     2268 b- defN 24-Apr-10 18:16 torchx/workspace/dir_workspace.py
+-rw-r--r--  2.0 unx     9425 b- defN 24-Apr-10 18:16 torchx/workspace/docker_workspace.py
+-rw-r--r--  2.0 unx     1721 b- defN 24-Apr-10 18:24 torchx_nightly-2024.4.10.dist-info/LICENSE
+-rw-r--r--  2.0 unx     6053 b- defN 24-Apr-10 18:24 torchx_nightly-2024.4.10.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-10 18:24 torchx_nightly-2024.4.10.dist-info/WHEEL
+-rw-r--r--  2.0 unx      170 b- defN 24-Apr-10 18:24 torchx_nightly-2024.4.10.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        7 b- defN 24-Apr-10 18:24 torchx_nightly-2024.4.10.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    10540 b- defN 24-Apr-10 18:24 torchx_nightly-2024.4.10.dist-info/RECORD
+120 files, 718443 bytes uncompressed, 237436 bytes compressed:  67.0%
```

## zipnote {}

```diff
@@ -198,14 +198,17 @@
 
 Filename: torchx/schedulers/api.py
 Comment: 
 
 Filename: torchx/schedulers/aws_batch_scheduler.py
 Comment: 
 
+Filename: torchx/schedulers/aws_sagemaker_scheduler.py
+Comment: 
+
 Filename: torchx/schedulers/devices.py
 Comment: 
 
 Filename: torchx/schedulers/docker_scheduler.py
 Comment: 
 
 Filename: torchx/schedulers/gcp_batch_scheduler.py
@@ -333,26 +336,26 @@
 
 Filename: torchx/workspace/dir_workspace.py
 Comment: 
 
 Filename: torchx/workspace/docker_workspace.py
 Comment: 
 
-Filename: torchx_nightly-2024.2.9.dist-info/LICENSE
+Filename: torchx_nightly-2024.4.10.dist-info/LICENSE
 Comment: 
 
-Filename: torchx_nightly-2024.2.9.dist-info/METADATA
+Filename: torchx_nightly-2024.4.10.dist-info/METADATA
 Comment: 
 
-Filename: torchx_nightly-2024.2.9.dist-info/WHEEL
+Filename: torchx_nightly-2024.4.10.dist-info/WHEEL
 Comment: 
 
-Filename: torchx_nightly-2024.2.9.dist-info/entry_points.txt
+Filename: torchx_nightly-2024.4.10.dist-info/entry_points.txt
 Comment: 
 
-Filename: torchx_nightly-2024.2.9.dist-info/top_level.txt
+Filename: torchx_nightly-2024.4.10.dist-info/top_level.txt
 Comment: 
 
-Filename: torchx_nightly-2024.2.9.dist-info/RECORD
+Filename: torchx_nightly-2024.4.10.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## torchx/__init__.py

```diff
@@ -1,11 +1,13 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from .version import (  # noqa F401; noqa F401
     __version__ as __version__,
     TORCHX_IMAGE as IMAGE,
 )
```

## torchx/notebook.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 This contains TorchX utilities for creating and running components and apps from
 an Jupyter/IPython Notebook.
 """
 
 import posixpath
```

## torchx/version.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from torchx.util.entrypoints import load
 
 # Follows PEP-0440 version scheme guidelines
 # https://www.python.org/dev/peps/pep-0440/#version-scheme
 #
 # Examples:
 # 0.1.0.devN # Developmental release
```

## torchx/apps/serve/serve.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import binascii
 import os
 import socket
 import sys
 import tempfile
 import threading
```

## torchx/apps/utils/booth_main.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import sys
 from typing import List
 
 from torchx.runtime.tracking import FsspecResultTracker
```

## torchx/apps/utils/copy_main.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import os
 import shutil
 import sys
 from typing import List
 
 import fsspec
```

## torchx/apps/utils/process_monitor.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import subprocess
 import sys
 import time
 from typing import List
 
 import fsspec
```

## torchx/cli/__init__.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 The ``torchx`` CLI is a commandline tool around :py:class:`torchx.runner.Runner`.
 It allows users to launch :py:class:`torchx.specs.AppDef` directly onto
 one of the supported schedulers without authoring a pipeline (aka workflow).
 This is convenient for quickly iterating on the application logic without
 incurring both the technical and cognitive overhead of learning, writing, and
 dealing with pipelines.
```

## torchx/cli/argparse_util.py

```diff
@@ -1,26 +1,34 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+import logging
+import sys
 from argparse import Action, ArgumentParser, Namespace
-from typing import Any, Dict, Optional, Sequence, Text
+from typing import Any, Dict, List, Optional, Sequence, Set, Text
 
 from torchx.runner import config
 
+logger: logging.Logger = logging.getLogger(__name__)
+
 
-class _torchxconfig(Action):
+class torchxconfig(Action):
     """
     Custom argparse action that loads default torchx CLI options
     from .torchxconfig file.
 
     """
 
+    called_args: Set[str] = set()
+
     # since this action is used for each argparse argument
     # load the config section for the subcmd once
     _subcmd_configs: Dict[str, Dict[str, str]] = {}
 
     def __init__(
         self,
         subcmd: str,
@@ -60,21 +68,26 @@
     def __call__(
         self,
         parser: ArgumentParser,
         namespace: Namespace,
         values: Any,  # pyre-ignore[2] declared as Any in superclass Action
         option_string: Optional[str] = None,
     ) -> None:
+        if option_string is not None:
+            if option_string in self.called_args:
+                logger.error(f"{option_string} is specified more than once")
+                sys.exit(1)
+            self.called_args.add(option_string)
         setattr(namespace, self.dest, values)
 
 
 # argparse takes the action as a Type[Action] so we can't have custom constructors
 # hence for each subcommand we need to subclass the base _torchxconfig Action
 # this is also how store_true and store_false builtin actions are implemented in argparse
-class torchxconfig_run(_torchxconfig):
+class torchxconfig_run(torchxconfig):
     """
     Custom action that gets the default argument from .torchxconfig.
     """
 
     def __init__(
         self,
         dest: str,
@@ -88,7 +101,29 @@
             "run",
             dest=dest,
             default=default,
             required=required,
             option_strings=option_strings,
             **kwargs,
         )
+
+
+class ArgOnceAction(Action):
+    """
+    Custom argparse action only allows argument to be specified once
+    """
+
+    called_args: Set[str] = set()
+
+    def __call__(
+        self,
+        parser: ArgumentParser,
+        namespace: Namespace,
+        values: List[str],
+        option_string: Optional[str] = None,
+    ) -> None:
+        if option_string is not None:
+            if option_string in self.called_args:
+                logger.error(f"{option_string} is specified more than once")
+                sys.exit(1)
+            self.called_args.add(option_string)
+        setattr(namespace, self.dest, values)
```

## torchx/cli/cmd_base.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import abc
 import argparse
 
 
 class SubCommand(abc.ABC):
     """
     Base sub command class, all subcommands should implement this base class
```

## torchx/cli/cmd_cancel.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import logging
 
 from torchx.cli.cmd_base import SubCommand
 from torchx.runner import get_runner
 
 logger: logging.Logger = logging.getLogger(__name__)
```

## torchx/cli/cmd_configure.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import logging
 import sys
 
 from torchx.cli.cmd_base import SubCommand
 from torchx.runner.config import dump
 from torchx.schedulers import get_scheduler_factories
```

## torchx/cli/cmd_describe.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import dataclasses
 import logging
 import pprint
 import sys
 
 from torchx.cli.cmd_base import SubCommand
```

## torchx/cli/cmd_list.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import logging
 
 from tabulate import tabulate
 
 from torchx.cli.cmd_base import SubCommand
 from torchx.runner import get_runner
```

## torchx/cli/cmd_log.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import logging
 import re
 import sys
 import threading
 import time
 from queue import Queue
```

## torchx/cli/cmd_run.py

```diff
@@ -1,25 +1,27 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import logging
 import os
 import sys
 import threading
 from dataclasses import asdict
 from pathlib import Path
 from pprint import pformat
 from typing import Dict, List, Optional, Tuple
 
 import torchx.specs as specs
-from torchx.cli.argparse_util import torchxconfig_run
+from torchx.cli.argparse_util import ArgOnceAction, torchxconfig_run
 from torchx.cli.cmd_base import SubCommand
 from torchx.cli.cmd_log import get_logs
 from torchx.runner import config, get_runner, Runner
 from torchx.runner.config import load_sections
 from torchx.schedulers import get_default_scheduler_name, get_scheduler_factories
 from torchx.specs.finder import (
     _Component,
@@ -127,14 +129,15 @@
             action=torchxconfig_run,
             help="Name of the scheduler to use.",
         )
         subparser.add_argument(
             "-cfg",
             "--scheduler_args",
             type=str,
+            action=ArgOnceAction,
             help="Arguments to pass to the scheduler (Ex:`cluster=foo,user=bar`)."
             " For a list of scheduler run options run: `torchx runopts`",
         )
         subparser.add_argument(
             "--dryrun",
             action="store_true",
             default=False,
@@ -159,14 +162,15 @@
             default=f"file://{Path.cwd()}",
             action=torchxconfig_run,
             help="local workspace to build/patch (buck-target of main binary if using buck)",
         )
         subparser.add_argument(
             "--parent_run_id",
             type=str,
+            action=ArgOnceAction,
             help="optional parent run ID that this run belongs to."
             " It can be used to group runs for experiment tracking purposes",
         )
         subparser.add_argument(
             "component_name_and_args",
             nargs=argparse.REMAINDER,
         )
```

## torchx/cli/cmd_runopts.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import logging
 
 from torchx.cli.cmd_base import SubCommand
 from torchx.cli.colors import ENDC, GREEN
 from torchx.runner.api import get_runner
```

## torchx/cli/cmd_status.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import logging
 import sys
 from typing import List, Optional
 
 from torchx.cli.cmd_base import SubCommand
 from torchx.runner import get_runner
```

## torchx/cli/cmd_tracker.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import logging
 
 from tabulate import tabulate
 
 from torchx.cli.cmd_base import SubCommand
 from torchx.runner.api import get_configured_trackers
```

## torchx/cli/colors.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import sys
 
 # only print colors if outputting directly to a terminal
 if not sys.stdout.closed and sys.stdout.isatty():
     GREEN = "\033[32m"
     BLUE = "\033[34m"
     ORANGE = "\033[38:2:238:76:44m"
```

## torchx/cli/main.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 import os
 import sys
 from argparse import ArgumentParser
 from typing import Dict, List
 
 import torchx
```

## torchx/components/__init__.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 This module contains a collection of builtin TorchX components. The directory
 structure is organized by component category. Components are simply
 templetized app specs. Think of them as a factory methods for different types
 of job definitions. The functions that return ``specs.AppDef`` in this
 module are what we refer to as components.
```

## torchx/components/component_test_base.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 You can unit test the component definitions as you would normal Python code
 since they are valid Python definitions.
 
 We do recommend using :py:class:`ComponentTestCase` to ensure that your
 component can be parsed by the TorchX CLI. The CLI requires stricter formatting
 on the doc string than pure Python as the doc string is used for parsing CLI
```

## torchx/components/dist.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 For distributed training, TorchX relies on the scheduler's gang scheduling
 capabilities to schedule ``n`` copies of nodes. Once launched, the application
 is expected to be written in a way that leverages this topology, for instance,
 with PyTorch's
 `DDP <https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`_.
 You can express a variety of node topologies with TorchX by specifying multiple
```

## torchx/components/serve.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 These components aim to make it easier to interact with inference and serving
 tools such as `torchserve <https://pytorch.org/serve/>`_.
 """
 
 from typing import Dict, Optional
```

## torchx/components/structured_arg.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Defines methods for structured (higher order) component argument parsing.
 Use the functionalities defined in this module to author components
 in such a way that the structured component arguments are consistent across the board.
 
 A structured component argument is a function argument to a component (a function that returns an ``AppDef``)
 that is human-friendly (less typing in the CLI or better readability) but technically embeds multiple
```

## torchx/components/utils.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 This contains TorchX utility components that are `ready-to-use` out of the box. These are
 components that simply execute well known binaries (e.g. ``cp``)
 and are meant to be used as tutorial materials or glue operations between
 meaningful stages in a workflow.
 """
```

## torchx/components/integration_tests/component_provider.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import os
 import tempfile
 from abc import ABC, abstractmethod
 
 import torchx.components.dist as dist_components
 import torchx.components.serve as serve_components
 import torchx.components.utils as utils_components
```

## torchx/components/integration_tests/integ_tests.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import inspect
 import logging
 import sys
 from dataclasses import asdict
 from json import dumps
 from types import ModuleType
 from typing import Callable, cast, Dict, List, Optional, Type
```

## torchx/examples/apps/datapreproc/datapreproc.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Data Preprocessing App Example
 ====================================
 
 This is a simple TorchX app that downloads some data via HTTP, normalizes the
 images via torchvision and then reuploads it via fsspec.
```

## torchx/examples/apps/lightning/data.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Trainer Datasets Example
 ========================
 
 This is the datasets used for the training example. It's using PyTorch Lightning
 libraries.
 """
```

## torchx/examples/apps/lightning/model.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Tiny ImageNet Model
 ====================
 
 This is a toy model for doing regression on the tiny imagenet dataset. It's used
 by the apps in the same folder.
 """
```

## torchx/examples/apps/lightning/profiler.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Simple Logging Profiler
 ===========================
 
 This is a simple profiler that's used as part of the trainer app example. This
 logs the Lightning training stage durations a logger such as Tensorboard. This
 output is used for HPO optimization with Ax.
```

## torchx/examples/apps/lightning/train.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Trainer Example
 =============================================
 
 This is an example TorchX app that uses PyTorch Lightning to train a model.
 
 This app only uses standard OSS libraries and has no runtime torchx
```

## torchx/examples/pipelines/kfp/advanced_pipeline.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Advanced KubeFlow Pipelines Example
 ===================================
 
 This is an example pipeline using KubeFlow Pipelines built with only TorchX
 components.
```

## torchx/examples/pipelines/kfp/dist_pipeline.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Distributed KubeFlow Pipelines Example
 ======================================
 
 This is an example KFP pipeline that uses resource_from_app to launch a
 distributed operator using the kubernetes/volcano job scheduler. This only works
 in Kubernetes KFP clusters with https://volcano.sh/en/docs/ installed on them.
```

## torchx/examples/pipelines/kfp/intro_pipeline.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Intro KubeFlow Pipelines Example
 ================================
 
 This an introductory pipeline using KubeFlow Pipelines built with only TorchX
 components.
```

## torchx/pipelines/kfp/__init__.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 This module contains adapters for converting TorchX components into KubeFlow
 Pipeline components.
 
 The current KFP adapters only support single node (1 role and 1 replica)
 components.
 """
```

## torchx/pipelines/kfp/adapter.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 import os
 import os.path
 import shlex
 from typing import Mapping, Optional, Tuple
 
 import yaml
@@ -70,16 +72,15 @@
 
 class ContainerFactory(Protocol):
     """
     ContainerFactory is a protocol that represents a function that when called produces a
     kfp.dsl.ContainerOp.
     """
 
-    def __call__(self, *args: object, **kwargs: object) -> dsl.ContainerOp:
-        ...
+    def __call__(self, *args: object, **kwargs: object) -> dsl.ContainerOp: ...
 
 
 class KFPContainerFactory(ContainerFactory, Protocol):
     """
     KFPContainerFactory is a ContainerFactory that also has some KFP metadata
     attached to it.
     """
```

## torchx/pipelines/kfp/version.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 # Follows PEP-0440 version scheme guidelines
 # https://www.python.org/dev/peps/pep-0440/#version-scheme
 #
 # Examples:
 # 0.1.0.devN # Developmental release
 # 0.1.0aN  # Alpha release
 # 0.1.0bN  # Beta release
```

## torchx/runner/__init__.py

```diff
@@ -1,8 +1,10 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from torchx.runner.api import get_runner, Runner  # noqa: F401 F403
```

## torchx/runner/api.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 import logging
 import os
 import time
 import warnings
 from datetime import datetime
 from types import TracebackType
@@ -127,15 +129,15 @@
         Transitively calls the ``close()`` method on all the schedulers.
         Once this method is called on the runner, the runner object is deemed
         invalid and any methods called on the runner object as well as
         the schedulers associated with this runner have undefined behavior.
         It is ok to call this method multiple times on the same runner object.
         """
 
-        for name, scheduler in self._scheduler_instances.items():
+        for scheduler in self._scheduler_instances.values():
             scheduler.close()
 
     def run_component(
         self,
         component: str,
         component_args: List[str],
         scheduler: str,
@@ -650,15 +652,15 @@
                 f"Undefined scheduler backend: {scheduler}. Use one of: {self._scheduler_factories.keys()}"
             )
         return sched
 
     def _scheduler_app_id(
         self,
         app_handle: AppHandle,
-        check_session: bool = True
+        check_session: bool = True,
         # pyre-fixme[24]: SchedulerOpts is a generic, and we don't have access to the corresponding type
     ) -> Tuple[Scheduler, str, str]:
         """
         Returns the scheduler and app_id from the app_handle.
         Set ``check_session`` to validate that the session name in the app handle
         is the same as this session.
```

## torchx/runner/config.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Status: Beta
 
 You can store the scheduler run cfg (run configs) for your project
 by storing them in the ``.torchxconfig`` file. Currently this file is only read
 and honored when running the component from the CLI.
 
@@ -262,14 +264,21 @@
                 if opt.opt_type == List[str]:
                     # deal with empty or None default lists
                     if opt.default:
                         # pyre-ignore[6] opt.default type checked already as List[str]
                         val = ";".join(opt.default)
                     else:
                         val = _NONE
+                elif opt.opt_type == Dict[str, str]:
+                    # deal with empty or None default lists
+                    if opt.default:
+                        # pyre-ignore[16] opt.default type checked already as Dict[str, str]
+                        val = ";".join([f"{k}:{v}" for k, v in opt.default.items()])
+                    else:
+                        val = _NONE
                 else:
                     val = f"{opt.default}"
 
             config.set(section, opt_name, val)
     config.write(f, space_around_delimiters=True)
 
 
@@ -521,10 +530,15 @@
                 else:
                     if runopt.opt_type is bool:
                         # need to handle bool specially since str -> bool is based on
                         # str emptiness not value (e.g. bool("False") == True)
                         cfg[name] = config.getboolean(section, name)
                     elif runopt.opt_type is List[str]:
                         cfg[name] = value.split(";")
+                    elif runopt.opt_type is Dict[str, str]:
+                        cfg[name] = {
+                            s.split(":", 1)[0]: s.split(":", 1)[1]
+                            for s in value.replace(",", ";").split(";")
+                        }
                     else:
                         # pyre-ignore[29]
                         cfg[name] = runopt.opt_type(value)
```

## torchx/runner/events/__init__.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env/python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Module contains events processing mechanisms that are integrated with the standard python logging.
 
 Example of usage:
 
 ::
```

## torchx/runner/events/api.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 from dataclasses import asdict, dataclass
 from enum import Enum
 from typing import Optional, Union
 
 
 class SourceType(str, Enum):
```

## torchx/runner/events/handlers.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 from typing import Dict
 
 
 _log_handlers: Dict[str, logging.Handler] = {
     "console": logging.StreamHandler(),
     "null": logging.NullHandler(),
```

## torchx/runtime/tracking/__init__.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 .. note:: EXPERIMENTAL, USE AT YOUR OWN RISK, APIs SUBJECT TO CHANGE
 
 In TorchX applications are binaries (executables),
 hence there is no built-in way to "return" results from applications.
 The ``torchx.runtime.tracking`` module allows applications
 to return simple results (note the keyword "simple"). The return types
```

## torchx/runtime/tracking/api.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import abc
 import json
 from typing import Dict, Union
 
 import fsspec
```

## torchx/schedulers/__init__.py

```diff
@@ -1,38 +1,40 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import importlib
 from typing import Dict, Mapping
 
 from torchx.schedulers.api import Scheduler
 from torchx.util.entrypoints import load_group
 from typing_extensions import Protocol
 
 DEFAULT_SCHEDULER_MODULES: Mapping[str, str] = {
     "local_docker": "torchx.schedulers.docker_scheduler",
     "local_cwd": "torchx.schedulers.local_scheduler",
     "slurm": "torchx.schedulers.slurm_scheduler",
     "kubernetes": "torchx.schedulers.kubernetes_scheduler",
     "kubernetes_mcad": "torchx.schedulers.kubernetes_mcad_scheduler",
     "aws_batch": "torchx.schedulers.aws_batch_scheduler",
+    "aws_sagemaker": "torchx.schedulers.aws_sagemaker_scheduler",
     "gcp_batch": "torchx.schedulers.gcp_batch_scheduler",
     "ray": "torchx.schedulers.ray_scheduler",
     "lsf": "torchx.schedulers.lsf_scheduler",
 }
 
 
 class SchedulerFactory(Protocol):
     # pyre-fixme: Scheduler opts
-    def __call__(self, session_name: str, **kwargs: object) -> Scheduler:
-        ...
+    def __call__(self, session_name: str, **kwargs: object) -> Scheduler: ...
 
 
 def _defer_load_scheduler(path: str) -> SchedulerFactory:
     # pyre-ignore[24]: Scheduler opts
     def run(*args: object, **kwargs: object) -> Scheduler:
         module = importlib.import_module(path)
         return module.create_scheduler(*args, **kwargs)
```

## torchx/schedulers/api.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import abc
 import re
 from dataclasses import dataclass, field
 from datetime import datetime
 from enum import Enum
 from typing import Generic, Iterable, List, Optional, TypeVar
```

## torchx/schedulers/aws_batch_scheduler.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 
 This contains the TorchX AWS Batch scheduler which can be used to run TorchX
 components directly on AWS Batch.
 
 This scheduler is in prototype stage and may change without notice.
```

## torchx/schedulers/devices.py

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
+
+# pyre-strict
 import warnings
 from typing import Callable, Dict, List, Mapping
 
 from torchx.specs.api import DeviceMount
 
 
 def efa_to_devicemounts(num_devices: int) -> List[DeviceMount]:
```

## torchx/schedulers/docker_scheduler.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import fnmatch
 import logging
 import os.path
 import tempfile
 from dataclasses import dataclass
 from datetime import datetime
 from typing import Any, Dict, Iterable, List, Optional, TYPE_CHECKING, Union
@@ -117,14 +119,15 @@
         except APIError as e:
             if "already exists" not in str(e):
                 raise
 
 
 class DockerOpts(TypedDict, total=False):
     copy_env: Optional[List[str]]
+    env: Optional[Dict[str, str]]
 
 
 class DockerScheduler(DockerWorkspaceMixin, Scheduler[DockerOpts]):
     """
     DockerScheduler is a TorchX scheduling interface to Docker.
 
     This is exposed via the scheduler `local_docker`.
@@ -211,14 +214,18 @@
             ), f"copy_env must be a list, got {copy_env}"
             keys = set()
             for pattern in copy_env:
                 keys |= set(fnmatch.filter(os.environ.keys(), pattern))
             for k in keys:
                 default_env[k] = os.environ[k]
 
+        env = cfg.get("env")
+        if env:
+            default_env.update(env)
+
         app_id = make_unique(app.name)
         req = DockerJob(app_id=app_id, containers=[])
         rank0_name = f"{app_id}-{app.roles[0].name}-0"
         for role in app.roles:
             mounts = []
             devices = []
             role.mounts += get_device_mounts(role.resource.devices)
@@ -288,17 +295,17 @@
                         "Name": "on-failure",
                         "MaximumRetryCount": replica_role.max_retries,
                     }
                 resource = replica_role.resource
                 if resource.memMB >= 0:
                     # To support PyTorch dataloaders we need to set /dev/shm to
                     # larger than the 64M default.
-                    c.kwargs["mem_limit"] = c.kwargs[
-                        "shm_size"
-                    ] = f"{int(resource.memMB)}m"
+                    c.kwargs["mem_limit"] = c.kwargs["shm_size"] = (
+                        f"{int(resource.memMB)}m"
+                    )
                 if resource.cpu >= 0:
                     c.kwargs["nano_cpus"] = int(resource.cpu * 1e9)
                 if resource.gpu > 0:
                     # `compute` means a CUDA or OpenCL capable device.
                     # For more info:
                     # * https://github.com/docker/docker-py/blob/master/docker/types/containers.py
                     # * https://github.com/NVIDIA/nvidia-container-runtime
@@ -353,14 +360,22 @@
         opts = runopts()
         opts.add(
             "copy_env",
             type_=List[str],
             default=None,
             help="list of glob patterns of environment variables to copy if not set in AppDef. Ex: FOO_*",
         )
+        opts.add(
+            "env",
+            type_=Dict[str, str],
+            default=None,
+            help="""environment variables to be passed to the run. The separator sign can be eiher comma or semicolon
+            (e.g. ENV1:v1,ENV2:v2,ENV3:v3 or ENV1:V1;ENV2:V2). Environment variables from env will be applied on top
+            of the ones from copy_env""",
+        )
         return opts
 
     def _get_app_state(self, container: "Container") -> AppState:
         if container.status == "exited":
             # docker doesn't have success/failed states -- we have to call
             # `wait()` to get the exit code to determine that
             status = container.wait(timeout=10)
```

## torchx/schedulers/gcp_batch_scheduler.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 
 This contains the TorchX GCP Batch scheduler which can be used to run TorchX
 components directly on GCP Batch.
 
 This scheduler is in prototype stage and may change without notice.
 
@@ -201,22 +203,20 @@
 
             resource = role_dict.resource
             res = batch_v1.ComputeResource()
             cpu = resource.cpu
             if cpu <= 0:
                 cpu = 1
             MILLI = 1000
-            # pyre-fixme[8]: Attribute has type `Field`; used as `int`.
             res.cpu_milli = cpu * MILLI
             memMB = resource.memMB
             if memMB < 0:
                 raise ValueError(
                     f"memMB should to be set to a positive value, got {memMB}"
                 )
-            # pyre-fixme[8]: Attribute has type `Field`; used as `int`.
             res.memory_mib = memMB
 
             # TODO support named resources
             # Using v100 as default GPU type as a100 does not allow changing count for now
             # TODO See if there is a better default GPU type
             if resource.gpu > 0:
                 if resource.gpu not in GPU_COUNT_TO_TYPE:
@@ -356,21 +356,19 @@
     def describe(self, app_id: str) -> Optional[DescribeAppResponse]:
         job = self._get_job(app_id)
         if job is None:
             print(f"app not found: {app_id}")
             return None
 
         gpu = 0
-        # pyre-fixme[16]: `Field` has no attribute `instances`.
         if len(job.allocation_policy.instances) != 0:
             gpu_type = job.allocation_policy.instances[0].policy.machine_type
             gpu = GPU_TYPE_TO_COUNT[gpu_type]
 
         roles = {}
-        # pyre-fixme[16]: `RepeatedField` has no attribute `__iter__`.
         for tg in job.task_groups:
             env = tg.task_spec.environment.variables
             role = env["TORCHX_ROLE_NAME"]
             container = tg.task_spec.runnables[1].container
             roles[role] = Role(
                 name=role,
                 num_replicas=tg.task_count,
@@ -386,15 +384,14 @@
                 max_retries=tg.task_spec.max_retry_count,
             )
 
         # Map job -> DescribeAppResponse
         # TODO map role/replica status
         desc = DescribeAppResponse(
             app_id=app_id,
-            # pyre-fixme[16]: `Field` has no attribute `state`.
             state=JOB_STATE[job.status.state.name],
             roles=list(roles.values()),
         )
         return desc
 
     def log_iter(
         self,
@@ -411,16 +408,18 @@
             raise ValueError("GCPBatchScheduler only supports COMBINED log stream")
 
         job = self._get_job(app_id)
         if not job:
             raise ValueError(f"app not found: {app_id}")
 
         job_uid = job.uid
-        filters = [f"labels.job_uid={job_uid}"]
-        filters.append(f"resource.labels.task_id:task/{job_uid}-group0-{k}")
+        filters = [
+            f"labels.job_uid={job_uid}",
+            f"labels.task_id:{job_uid}-group0-{k}",
+        ]
 
         if since is not None:
             filters.append(f'timestamp>="{str(since.isoformat())}"')
         else:
             # gcloud logger.list by default only returns logs in the last 24 hours
             # Since many ML jobs can run longer add timestamp filter to get all logs
             filters.append(f'timestamp>="{str(datetime.fromtimestamp(0).isoformat())}"')
@@ -433,15 +432,15 @@
         return self._batch_log_iter(filter)
 
     def _batch_log_iter(self, filter: str) -> Iterable[str]:
         from google.cloud import logging
 
         logger = logging.Client().logger(BATCH_LOGGER_NAME)
         for entry in logger.list_entries(filter_=filter):
-            yield entry.payload
+            yield entry.payload + "\n"
 
     def _job_full_name_to_app_id(self, job_full_name: str) -> str:
         """
         job_full_name format: f"projects/{project}/locations/{location}/jobs/{name}"
         app_id format: f"{project}:{location}:{name}"
         where 'name' was created uniquely for the job from the app name
         """
```

## torchx/schedulers/ids.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import os
 import random
 import struct
 
 START_CANDIDATES: str = "bcdfghjklmnpqrstvwxz"
 END_CANDIDATES: str = START_CANDIDATES + "012345679"
```

## torchx/schedulers/kubernetes_scheduler.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 
 This contains the TorchX Kubernetes scheduler which can be used to run TorchX
 components on a Kubernetes cluster.
 
 Prerequisites
 ==============
@@ -19,15 +21,15 @@
 Install Volcano:
 
 .. code:: bash
 
     kubectl apply -f https://raw.githubusercontent.com/volcano-sh/volcano/v1.6.0/installer/volcano-development.yaml
 
 See the
-`Volcano Quickstart <https://github.com/volcano-sh/volcano#quick-start-guide>`_
+`Volcano Quickstart <https://github.com/volcano-sh/volcano>`_
 for more information.
 """
 
 import json
 import logging
 import warnings
 from dataclasses import dataclass
```

## torchx/schedulers/local_scheduler.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 This contains the TorchX local scheduler which can be used to run TorchX
 components locally via subprocesses.
 """
 
 import abc
 import io
@@ -31,14 +33,15 @@
     Any,
     BinaryIO,
     Callable,
     Dict,
     Iterable,
     List,
     Optional,
+    Protocol,
     TextIO,
     Tuple,
 )
 
 from torchx.schedulers.api import (
     AppDryRunInfo,
     DescribeAppResponse,
@@ -258,24 +261,43 @@
 
 # aliases to make clear what the mappings are
 AppId = str
 AppName = str
 RoleName = str
 
 
+class PopenProtocol(Protocol):
+    """
+    Protocol wrapper around python's ``subprocess.Popen``. Keeps track of
+    the a list of interface methods that the process scheduled by the `LocalScheduler`
+    must implement.
+    """
+
+    @property
+    def pid(self) -> int: ...
+
+    @property
+    def returncode(self) -> int: ...
+
+    def wait(self, timeout: Optional[float] = None) -> int: ...
+
+    def poll(self) -> Optional[int]: ...
+
+    def kill(self) -> None: ...
+
+
 @dataclass
 class _LocalReplica:
     """
     Contains information about a locally running role replica.
     """
 
     role_name: RoleName
     replica_id: int
-    # pyre-fixme[24]: Generic type `subprocess.Popen` expects 1 type parameter.
-    proc: subprocess.Popen
+    proc: PopenProtocol
 
     # IO streams:
     # None means no log_dir (out to console)
     stdout: Optional[BinaryIO]
     stderr: Optional[BinaryIO]
     combined: Optional[Tee]
```

## torchx/schedulers/lsf_scheduler.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 This contains the TorchX LSF scheduler which can be used to run TorchX
 components on a LSF cluster.
 
 This scheduler is in prototype stage and may change without notice. If you run
 into any issues or have feedback please submit an issue.
```

## torchx/schedulers/slurm_scheduler.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 This contains the TorchX Slurm scheduler which can be used to run TorchX
 components on a Slurm cluster.
 """
 import csv
 import json
 import logging
```

## torchx/schedulers/streams.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import io
 import os
 import threading
 import time
 from typing import List
```

## torchx/schedulers/ray/ray_driver.py

```diff
@@ -144,20 +144,20 @@
         self.max_replicas: int = len(replicas)
         self.min_replicas: int
         if replicas[0].min_replicas is None:
             self.min_replicas = self.max_replicas
         else:
             self.min_replicas = replicas[0].min_replicas  # pyre-ignore[8]
 
-        self.placement_groups: List[
-            PlacementGroup
-        ] = []  # all the placement groups, shall never change
-        self.actor_info_of_id: Dict[
-            str, ActorInfo
-        ] = {}  # store the info used to recover an actor
+        self.placement_groups: List[PlacementGroup] = (
+            []
+        )  # all the placement groups, shall never change
+        self.actor_info_of_id: Dict[str, ActorInfo] = (
+            {}
+        )  # store the info used to recover an actor
         self.active_tasks: List["ray.ObjectRef"] = []  # list of active tasks
 
         self.terminating: bool = False  # if the job has finished and being terminated
         self.command_actors_count: int = 0  # number of created command actors
 
     def init_placement_groups(self) -> None:
         """Initialize all placement groups needed for this job"""
```

## torchx/specs/__init__.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 This contains the TorchX AppDef and related component definitions. These are
 used by components to define the apps which can then be launched via a TorchX
 scheduler or pipeline adapter.
 """
 import difflib
 from typing import Callable, Dict, Optional
```

## torchx/specs/api.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import copy
 import json
 import re
 from dataclasses import asdict, dataclass, field
 from datetime import datetime
 from enum import Enum
 from string import Template
@@ -633,19 +635,19 @@
 
     def __init__(self, status: AppStatus, *args: object) -> None:
         super().__init__(*args)
 
         self.status = status
 
 
-# valid run cfg values; only support primitives (str, int, float, bool, List[str])
+# valid run cfg values; only support primitives (str, int, float, bool, List[str], Dict[str, str])
 # TODO(wilsonhong): python 3.9+ supports list[T] in typing, which can be used directly
 # in isinstance(). Should replace with that.
 # see: https://docs.python.org/3/library/stdtypes.html#generic-alias-type
-CfgVal = Union[str, int, float, bool, List[str], None]
+CfgVal = Union[str, int, float, bool, List[str], Dict[str, str], None]
 
 
 T = TypeVar("T")
 
 
 class AppDryRunInfo(Generic[T]):
     """
@@ -751,14 +753,18 @@
         tp = List[str], thus can be used to validate ConfigValue.
         """
         try:
             return isinstance(obj, tp)
         except TypeError:
             if isinstance(obj, list):
                 return all(isinstance(e, str) for e in obj)
+            elif isinstance(obj, dict):
+                return all(
+                    isinstance(k, str) and isinstance(v, str) for k, v in obj.items()
+                )
             else:
                 return False
 
     def get(self, name: str) -> Optional[runopt]:
         """
         Returns option if any was registered, or None otherwise
         """
@@ -859,16 +865,21 @@
         def _cast_to_type(value: str, opt_type: Type[CfgVal]) -> CfgVal:
             if opt_type == bool:
                 return value.lower() == "true"
             elif opt_type == List[str]:
                 # lists may be ; or , delimited
                 # also deal with trailing "," by removing empty strings
                 return [v for v in value.replace(";", ",").split(",") if v]
+            elif opt_type == Dict[str, str]:
+                return {
+                    s.split(":", 1)[0]: s.split(":", 1)[1]
+                    for s in value.replace(";", ",").split(",")
+                }
             else:
-                # pyre-ignore[19]
+                # pyre-ignore[19, 6] type won't be dict here as we handled it above
                 return opt_type(value)
 
         cfg: Dict[str, CfgVal] = {}
         for key, val in to_dict(cfg_str).items():
             runopt_ = self.get(key)
             if runopt_:
                 cfg[key] = _cast_to_type(val, runopt_.opt_type)
```

## torchx/specs/builders.py

```diff
@@ -1,32 +1,34 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import argparse
 import inspect
+import os
+from argparse import Namespace
 from typing import Any, Callable, Dict, List, Mapping, Optional, Union
 
 from torchx.specs.api import BindMount, MountType, VolumeMount
 from torchx.specs.file_linter import get_fn_docstring, TorchXArgumentHelpFormatter
-from torchx.util.types import (
-    decode_from_string,
-    decode_optional,
-    get_argparse_param_type,
-    is_bool,
-    is_primitive,
-)
+from torchx.util.types import decode, decode_optional, get_argparse_param_type, is_bool
 
 from .api import AppDef, DeviceMount
 
+ENV_TORCHX_COMPONENT_ARGS = "TORCHX_COMPONENT_ARGS"
+
 
 def _create_args_parser(
-    cmpnt_fn: Callable[..., AppDef], cmpnt_defaults: Optional[Dict[str, str]] = None
+    cmpnt_fn: Callable[..., AppDef],
+    cmpnt_defaults: Optional[Dict[str, str]] = None,
+    config: Optional[Dict[str, Any]] = None,
 ) -> argparse.ArgumentParser:
     parameters = inspect.signature(cmpnt_fn).parameters
     function_desc, args_desc = get_fn_docstring(cmpnt_fn)
     script_parser = argparse.ArgumentParser(
         prog=f"torchx run <run args...> {cmpnt_fn.__name__} ",
         description=function_desc,
         formatter_class=TorchXArgumentHelpFormatter,
@@ -81,23 +83,64 @@
             args["action"] = _reminder_action
             script_parser.add_argument(param_name, **args)
         else:
             arg_names = [f"--{param_name}"]
             if len(param_name) == 1:
                 arg_names = [f"-{param_name}"] + arg_names
             if "default" not in args:
-                args["required"] = True
+                if (config and param_name not in config) or not config:
+                    args["required"] = True
+
             script_parser.add_argument(*arg_names, **args)
     return script_parser
 
 
+def _merge_config_values_with_args(
+    parsed_args: argparse.Namespace, config: Dict[str, Any]
+) -> None:
+    for key, val in config.items():
+        if key in parsed_args:
+            setattr(parsed_args, key, val)
+
+
+def parse_args(
+    cmpnt_fn: Callable[..., AppDef],
+    cmpnt_args: List[str],
+    cmpnt_defaults: Optional[Dict[str, Any]] = None,
+    config: Optional[Dict[str, Any]] = None,
+) -> Namespace:
+    """
+    Parse passed arguments, defaults, and config values into a namespace for
+    a component function.
+
+    Args:
+    cmpnt_fn: Component function
+    cmpnt_args: Function args
+    cmpnt_defaults: Additional default values for parameters of ``app_fn``
+                        (overrides the defaults set on the fn declaration)
+    config: Optional dict containing additional configuration for the component from a passed config file
+
+    Returns:
+    A Namespace object with the args, defaults, and config values incorporated.
+    """
+
+    script_parser = _create_args_parser(cmpnt_fn, cmpnt_defaults, config)
+    parsed_args = script_parser.parse_args(cmpnt_args)
+    if config:
+        _merge_config_values_with_args(parsed_args, config)
+
+    return parsed_args
+
+
 def materialize_appdef(
     cmpnt_fn: Callable[..., AppDef],
     cmpnt_args: List[str],
-    cmpnt_defaults: Optional[Dict[str, str]] = None,
+    cmpnt_defaults: Optional[Dict[str, Any]] = None,
+    config: Optional[Dict[str, Any]] = None,
+    component_args_string: Optional[str] = None,
 ) -> AppDef:
     """
     Creates an application by running user defined ``app_fn``.
 
     ``app_fn`` has the following restrictions:
         * Name must be ``app_fn``
         * All arguments should be annotated
@@ -114,46 +157,49 @@
         * The return object must be ``AppDef``
 
     Args:
         cmpnt_fn: Component function
         cmpnt_args: Function args
         cmpnt_defaults: Additional default values for parameters of ``app_fn``
                           (overrides the defaults set on the fn declaration)
+        config: Optional dict containing additional configuration for the component from a passed config file
     Returns:
         An application spec
     """
 
-    script_parser = _create_args_parser(cmpnt_fn, cmpnt_defaults)
-    parsed_args = script_parser.parse_args(cmpnt_args)
-
     function_args = []
     var_arg = []
     kwargs = {}
 
+    parsed_args = parse_args(cmpnt_fn, cmpnt_args, cmpnt_defaults, config)
+
     parameters = inspect.signature(cmpnt_fn).parameters
     for param_name, parameter in parameters.items():
         arg_value = getattr(parsed_args, param_name)
         parameter_type = parameter.annotation
         parameter_type = decode_optional(parameter_type)
-        if is_bool(parameter_type):
-            arg_value = arg_value and arg_value.lower() == "true"
-        elif not is_primitive(parameter_type):
-            arg_value = decode_from_string(arg_value, parameter_type)
+        arg_value = decode(arg_value, parameter_type)
         if parameter.kind == inspect.Parameter.VAR_POSITIONAL:
             var_arg = arg_value
         elif parameter.kind == inspect.Parameter.KEYWORD_ONLY:
             kwargs[param_name] = arg_value
         elif parameter.kind == inspect.Parameter.VAR_KEYWORD:
             raise TypeError("**kwargs are not supported for component definitions")
         else:
             function_args.append(arg_value)
     if len(var_arg) > 0 and var_arg[0] == "--":
         var_arg = var_arg[1:]
 
-    return cmpnt_fn(*function_args, *var_arg, **kwargs)
+    appdef = cmpnt_fn(*function_args, *var_arg, **kwargs)
+
+    if component_args_string:
+        for role in appdef.roles:
+            role.env[ENV_TORCHX_COMPONENT_ARGS] = component_args_string
+
+    return appdef
 
 
 def make_app_handle(scheduler_backend: str, session_name: str, app_id: str) -> str:
     return f"{scheduler_backend}://{session_name}/{app_id}"
 
 
 _MOUNT_OPT_MAP: Mapping[str, str] = {
@@ -201,17 +247,20 @@
             raise KeyError("type must be specified first")
         cur[key] = val
 
     mounts = []
     for opts in mount_opts:
         typ = opts.get("type")
         if typ == MountType.BIND:
+            src_path = opts["src"]
+            if src_path.startswith("~"):
+                src_path = os.path.expanduser(src_path)
             mounts.append(
                 BindMount(
-                    src_path=opts["src"],
+                    src_path=src_path,
                     dst_path=opts["dst"],
                     read_only="readonly" in opts,
                 )
             )
         elif typ == MountType.VOLUME:
             mounts.append(
                 VolumeMount(
```

## torchx/specs/file_linter.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import abc
 import argparse
 import ast
 import inspect
 from dataclasses import dataclass
 from typing import Callable, cast, Dict, List, Optional, Tuple
 
@@ -25,15 +27,19 @@
     args_decs = {}
     for parameter_name in parameters.keys():
         # The None or Empty string values getting ignored during help command by argparse
         args_decs[parameter_name] = " "
     return args_decs
 
 
-class TorchXArgumentHelpFormatter(argparse.HelpFormatter):
+class TorchXArgumentHelpFormatter(
+    argparse.RawDescriptionHelpFormatter,
+    argparse.ArgumentDefaultsHelpFormatter,
+    argparse.MetavarTypeHelpFormatter,
+):
     """Help message formatter which adds default values and required to argument help.
 
     If the argument is required, the class appends `(required)` at the end of the help message.
     If the argument has default value, the class appends `(default: $DEFAULT)` at the end.
     The formatter is designed to be used only for the torchx components functions.
     These functions do not have both required and default arguments.
     """
@@ -75,15 +81,15 @@
         return default_fn_desc, args_description
     docstring = parse(func_description)
     for param in docstring.params:
         if param.description is not None:
             args_description[param.arg_name] = param.description
     short_func_description = docstring.short_description or default_fn_desc
     if docstring.long_description:
-        short_func_description += " ..."
+        short_func_description += "\n" + docstring.long_description
     return (short_func_description or default_fn_desc, args_description)
 
 
 @dataclass
 class LinterMessage:
     name: str
     description: str
```

## torchx/specs/finder.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import abc
 import importlib
 import inspect
 import logging
 import os
 import pkgutil
 from dataclasses import dataclass
```

## torchx/specs/named_resources_aws.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 r"""
 `torchx.specs.named_resources_aws` contains resource definitions that represent corresponding AWS instance types
 taken from https://aws.amazon.com/ec2/instance-types/. The resources are exposed
 via entrypoints after installing torchx lib. The mapping is stored in the `setup.py` file.
 
 The named resources currently do not specify AWS instance type capabilities but merely represent
 the equvalent resource in mem, cpu and gpu numbers.
```

## torchx/specs/named_resources_generic.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Defines generic named resources that are not specific to any cloud provider's
 instance types. These generic named resources are meant to be used as
 default values for components and examples and are NOT meant to be used
 long term as the specific capabilities (e.g. number of cpu, gpu, memMB)
 are subject to change.
```

## torchx/specs/test/components/__init__.py

```diff
@@ -1,5 +1,7 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
+
+# pyre-strict
```

## torchx/specs/test/components/a/__init__.py

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
+
+# pyre-strict
 import torchx
 from torchx import specs
 
 
 def comp_a() -> specs.AppDef:
     return specs.AppDef(
         name="foo.comp_a",
```

## torchx/specs/test/components/a/b/__init__.py

```diff
@@ -1,5 +1,7 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
+
+# pyre-strict
```

## torchx/specs/test/components/a/b/c.py

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
+
+# pyre-strict
 import torchx
 from torchx import specs
 
 
 def d() -> specs.AppDef:
     return specs.AppDef(
         name="foo.b.c.d",
```

## torchx/specs/test/components/c/__init__.py

```diff
@@ -1,6 +1,8 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
+
+# pyre-strict
```

## torchx/specs/test/components/c/d.py

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
+
+# pyre-strict
 import torchx
 from torchx import specs
 
 
 def e() -> specs.AppDef:
     return specs.AppDef(
         name="bar.e",
```

## torchx/tracker/__init__.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 .. note:: PROTOTYPE, USE AT YOUR OWN RISK, APIs SUBJECT TO CHANGE
 
 
 Practitioners running ML jobs often need to track information such as:
 
 * Job inputs:
```

## torchx/tracker/api.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import logging
 import os
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
 from functools import lru_cache
@@ -63,16 +65,15 @@
         artifact_name(Optional[str]): type of artifact represening parent AppRun.
     """
 
     parent: AppRun
     artifact_name: Optional[str]
 
 
-class Lineage:
-    ...
+class Lineage: ...
 
 
 class TrackerBase(ABC):
     """
     Abstraction of tracking solution implementations/services.
 
     This API is stil experimental and may change in the future to a large extend.
@@ -328,9 +329,8 @@
             for source in sources:
                 parent = AppRun(source.source_run_id, backends=self.backends)
                 model_run_source = AppRunTrackableSource(parent, source.artifact_name)
                 model_run_sources.append(model_run_source)
 
         return model_run_sources
 
-    def children(self) -> Iterable[AppRun]:
-        ...
+    def children(self) -> Iterable[AppRun]: ...
```

## torchx/tracker/backend/fsspec.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import json
 import os
 import time
 from base64 import b32decode, b32encode
 from dataclasses import dataclass
```

## torchx/util/cuda.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import torch
 
 
 def has_cuda_devices() -> bool:
     """
     Checks if the host that is running this function has CUDA (GPU) devices
     and that the installed version of PyTorch is CUDA-capable (PyTorch can be installed
```

## torchx/util/datetime.py

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from datetime import datetime, timedelta
 
 
 def get_past_epoch_time(days_past: int) -> int:
     past_day = datetime.now() - timedelta(days_past)
     return int(past_day.timestamp())
```

## torchx/util/entrypoints.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, Optional
 
 import importlib_metadata as metadata
 from importlib_metadata import EntryPoint
 
 
 # pyre-ignore-all-errors[3, 2]
```

## torchx/util/io.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from os import path
 from pathlib import Path
 from typing import Optional
 
 from torchx.util import entrypoints
```

## torchx/util/modules.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import importlib
 from types import ModuleType
 from typing import Callable, Optional, Union
 
 
 def load_module(path: str) -> Union[ModuleType, Optional[Callable[..., object]]]:
     """
```

## torchx/util/shlex.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import shlex
 from typing import Iterable
 
 
 def join(args: Iterable[str]) -> str:
     """
     This is equivalent to Python 3.8+'s shlex.join method but also works on Python 3.7.
```

## torchx/util/strings.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import re
 
 
 def normalize_str(data: str) -> str:
     """
     Invokes ``lower`` on thes string and removes all
     characters that do not satisfy ``[a-z0-9\\-]`` pattern.
```

## torchx/util/types.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import inspect
 from typing import Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar, Union
 
 import typing_inspect
 
 
 def to_list(arg: str) -> List[str]:
@@ -116,14 +118,24 @@
         raise ValueError("List types support only primitives: int, str, float")
     arg_values = []
     for value in to_list(encoded_value):
         arg_values.append(value_type(value))
     return arg_values
 
 
+def decode(encoded_value: Any, annotation: Any):
+    if encoded_value is None:
+        return None
+    if is_bool(annotation):
+        return encoded_value and encoded_value.lower() == "true"
+    if not is_primitive(annotation) and type(encoded_value) == str:
+        return decode_from_string(encoded_value, annotation)
+    return encoded_value
+
+
 def decode_from_string(
     encoded_value: str, annotation: Any
 ) -> Union[Dict[Any, Any], List[Any], None]:
     """Decodes string representation to the underlying type(Dict or List)
 
     Given a string representation of the value, the method decodes it according
     to the ``annotation`` type:
```

## torchx/workspace/__init__.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Status: Beta
 
 Workspaces are used to apply local changes on top of existing images so you can
 execute your code on a remote cluster. This module contains the interfaces used
 by workspace implementations.
```

## torchx/workspace/api.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import abc
 import fnmatch
 import posixpath
 from typing import Generic, Iterable, Mapping, Tuple, TYPE_CHECKING, TypeVar
 
 from torchx.specs import AppDef, CfgVal, Role, runopts
```

## torchx/workspace/dir_workspace.py

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import os
 import posixpath
 import shutil
 from tempfile import mkdtemp
 from typing import Mapping
 
 import fsspec
```

## torchx/workspace/docker_workspace.py

```diff
@@ -1,13 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import io
 import logging
 import posixpath
 import stat
 import sys
 import tarfile
 import tempfile
```

## Comparing `torchx_nightly-2024.2.9.dist-info/LICENSE` & `torchx_nightly-2024.4.10.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `torchx_nightly-2024.2.9.dist-info/METADATA` & `torchx_nightly-2024.4.10.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: torchx-nightly
-Version: 2024.2.9
+Version: 2024.4.10
 Summary: TorchX SDK and Components
 Home-page: https://github.com/pytorch/torchx
 Author: TorchX Devs
 Author-email: torchx@fb.com
 License: BSD-3
 Keywords: pytorch,machine learning
 Platform: UNKNOWN
@@ -20,51 +20,59 @@
 License-File: LICENSE
 Requires-Dist: pyre-extensions
 Requires-Dist: docstring-parser >=0.8.1
 Requires-Dist: importlib-metadata
 Requires-Dist: pyyaml
 Requires-Dist: docker
 Requires-Dist: filelock
-Requires-Dist: fsspec ==2023.10.0
+Requires-Dist: fsspec >=2023.10.0
 Requires-Dist: urllib3 <1.27,>=1.21.1
 Requires-Dist: tabulate
 Provides-Extra: aws_batch
 Requires-Dist: boto3 ; extra == 'aws_batch'
 Provides-Extra: dev
-Requires-Dist: aiobotocore ; extra == 'dev'
+Requires-Dist: aiobotocore ==2.12.1 ; extra == 'dev'
 Requires-Dist: ax-platform[mysql] ==0.2.3 ; extra == 'dev'
-Requires-Dist: black ==23.3.0 ; extra == 'dev'
-Requires-Dist: boto3 ; extra == 'dev'
+Requires-Dist: boto3 ==1.34.51 ; extra == 'dev'
 Requires-Dist: captum >=0.4.0 ; extra == 'dev'
 Requires-Dist: docker ; extra == 'dev'
+Requires-Dist: kubernetes ==25.3.0 ; extra == 'dev'
 Requires-Dist: flake8 ==3.9.0 ; extra == 'dev'
-Requires-Dist: fsspec[s3] ==2023.10.0 ; extra == 'dev'
+Requires-Dist: fsspec ==2024.3.1 ; extra == 'dev'
+Requires-Dist: s3fs ==2024.3.1 ; extra == 'dev'
 Requires-Dist: google-api-core ; extra == 'dev'
-Requires-Dist: google-cloud-batch >=0.5.0 ; extra == 'dev'
-Requires-Dist: google-cloud-logging >=3.0.0 ; extra == 'dev'
-Requires-Dist: google-cloud-runtimeconfig >=0.33.2 ; extra == 'dev'
+Requires-Dist: google-cloud-batch ==0.17.14 ; extra == 'dev'
+Requires-Dist: google-cloud-logging ==3.10.0 ; extra == 'dev'
+Requires-Dist: google-cloud-runtimeconfig ==0.34.0 ; extra == 'dev'
 Requires-Dist: hydra-core ; extra == 'dev'
 Requires-Dist: ipython ; extra == 'dev'
 Requires-Dist: kfp ==1.8.22 ; extra == 'dev'
 Requires-Dist: mlflow-skinny ; extra == 'dev'
-Requires-Dist: moto ==4.1.6 ; extra == 'dev'
-Requires-Dist: protobuf ==3.20.3 ; extra == 'dev'
+Requires-Dist: moto ==4.2.14 ; extra == 'dev'
 Requires-Dist: pyre-extensions ; extra == 'dev'
 Requires-Dist: pyre-check ; extra == 'dev'
 Requires-Dist: pytest ; extra == 'dev'
+Requires-Dist: pytest-cov ; extra == 'dev'
 Requires-Dist: pytorch-lightning ==1.5.10 ; extra == 'dev'
+Requires-Dist: sagemaker >=2.149.0 ; extra == 'dev'
 Requires-Dist: torch-model-archiver >=0.4.2 ; extra == 'dev'
-Requires-Dist: torch >=1.10.0 ; extra == 'dev'
-Requires-Dist: torchmetrics <0.11.0 ; extra == 'dev'
-Requires-Dist: torchserve >=0.4.2 ; extra == 'dev'
-Requires-Dist: torchtext >=0.11.0 ; extra == 'dev'
-Requires-Dist: torchvision >=0.11.1 ; extra == 'dev'
+Requires-Dist: torch ==2.2.1 ; extra == 'dev'
+Requires-Dist: torchmetrics ==0.10.3 ; extra == 'dev'
+Requires-Dist: torchserve >=0.10.0 ; extra == 'dev'
+Requires-Dist: torchtext ==0.17.1 ; extra == 'dev'
+Requires-Dist: torchvision ==0.17.1 ; extra == 'dev'
 Requires-Dist: ts ==0.5.1 ; extra == 'dev'
-Requires-Dist: usort ==1.0.2 ; extra == 'dev'
 Requires-Dist: ray[default] ; extra == 'dev'
+Requires-Dist: lintrunner ; extra == 'dev'
+Requires-Dist: lintrunner-adapters ; extra == 'dev'
+Requires-Dist: grpcio ==1.62.1 ; extra == 'dev'
+Requires-Dist: grpcio-status ==1.48.1 ; extra == 'dev'
+Requires-Dist: googleapis-common-protos ==1.63.0 ; extra == 'dev'
+Requires-Dist: google-api-core ==2.18.0 ; extra == 'dev'
+Requires-Dist: protobuf ==3.20.3 ; extra == 'dev'
 Provides-Extra: gcp_batch
 Requires-Dist: google-cloud-batch >=0.5.0 ; extra == 'gcp_batch'
 Requires-Dist: google-cloud-logging >=3.0.0 ; extra == 'gcp_batch'
 Requires-Dist: google-cloud-runtimeconfig >=0.33.2 ; extra == 'gcp_batch'
 Provides-Extra: kfp
 Requires-Dist: kfp ==1.6.2 ; extra == 'kfp'
 Provides-Extra: kubernetes
```

## Comparing `torchx_nightly-2024.2.9.dist-info/RECORD` & `torchx_nightly-2024.4.10.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,119 +1,120 @@
-torchx/__init__.py,sha256=ff7M2dxFLin47E152U1gSdfLog0UFD-dIlB1lX033es,340
-torchx/notebook.py,sha256=6x9VlBjiOpwB9aCnRiGz40Nmd_Kxj2wxQp9t0HW-Zg4,993
-torchx/version.py,sha256=WO5RhvE1bK2IFm0GNA_bk6jrfdTRotCt1WZmOKqNkIY,936
+torchx/__init__.py,sha256=QFDTdJacncWYWHL-2QyWdY5MUck3jVfSPRRGdvedcKc,355
+torchx/notebook.py,sha256=Rc6XUMzSq7NXtsYdtVluE6T89LpEhcba-3ANxuaLCCU,1008
+torchx/version.py,sha256=XdtrgRZHzaJ4l4lkzeV6TKMxQJCoKe_G5TSBJl2smpg,951
 torchx/apps/__init__.py,sha256=fE0IHi1JJpxsNVBNzWNee2thrNXFFRhY94c80RxNSIE,231
 torchx/apps/serve/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchx/apps/serve/serve.py,sha256=2LxjeE_d1qCM2bxltDwYXjKbgcizm2YaIc_qPC2akvo,4371
+torchx/apps/serve/serve.py,sha256=u_h8agld1TwIPq5GRosHL3uxhkljNfS65McLB77O0OE,4386
 torchx/apps/utils/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchx/apps/utils/booth_main.py,sha256=qZn-ThyHfWXz2z901pX5qbJCT75APzplYOb_GawIj18,1412
-torchx/apps/utils/copy_main.py,sha256=EoRHatzahLLsJmQXJL4HJgDycH8y_U5EZrkns1Fbyao,1823
-torchx/apps/utils/process_monitor.py,sha256=2-RcFX6Rn-9LSQ8dUWiS13F4b5aM8dsqyT0nnyRjslM,3437
-torchx/cli/__init__.py,sha256=lah8hAdRGyQcz_CQ8SRE3kQG7BLuNVeNLi448sHJpSo,10336
-torchx/cli/argparse_util.py,sha256=WhOu5OGI5SMyKCDIxxPKMCfS5RwztiWbVIzH7KXpAWg,2836
-torchx/cli/cmd_base.py,sha256=v6sjnk2yrPRKaJ2sjKILCY8gLNMxQrfHK4xVpjkfCoc,775
-torchx/cli/cmd_cancel.py,sha256=MDIrJ26Id5KCd10uIM-nHh77hIa8GbM_m0bVWkogjVU,820
-torchx/cli/cmd_configure.py,sha256=PSDjkG-EuRalG0t2CZ9qB68wNHYPsc66VT3PNRC9LJU,1707
-torchx/cli/cmd_describe.py,sha256=s6Qw6RDSibJeP0Flfw02QFqbhwvc57l9DYja4tmklkw,1268
-torchx/cli/cmd_list.py,sha256=zwhGQro9UgNjBMF6demc3nMp_pqPxNnH5cdtoGDZSZU,1414
-torchx/cli/cmd_log.py,sha256=nYGwQHPmzH1m-jwdR1ndpscF3XcT1-brS4wsLXZOFUA,6089
-torchx/cli/cmd_run.py,sha256=fh4Av9kfGkQOvQvFKuQEaC5zcge7I8r4qUFyMiS8TFg,10429
-torchx/cli/cmd_runopts.py,sha256=smQEJChk678o9n1y-crsVFiTMICp1C0PXcwoC3nRCtU,1287
-torchx/cli/cmd_status.py,sha256=BG-ScFn1wpLnf_VWphFvv-OI24JRQIbpCQ3_vPVVtG4,1821
-torchx/cli/cmd_tracker.py,sha256=S4GOTtBWoR3sbAJxRzjrF_I5uPvUjjoX-S4YEgGBLI0,5203
-torchx/cli/colors.py,sha256=bVN_jEDwLgvypnDMeCHKn0q0ZDDhQjBJnyVfZHAE6nc,553
-torchx/cli/main.py,sha256=DJYikTWacADa4VoscqZGjZmMKWWK29tBl6-pGtnzsRE,3469
-torchx/components/__init__.py,sha256=6-TQ4SY-Tn56os_1lOs_HMabOoE7gkkud_8e1BgvfJw,12106
-torchx/components/component_test_base.py,sha256=eKOwBp5cRgiA4FgZd_FCvyJ-ppv2v3JN9AGXnaSK_Cw,4135
-torchx/components/dist.py,sha256=tBOL_DjUeBjNDUcF2wnjlUEzZck9fo1ojRkbQA5ERT8,14555
+torchx/apps/utils/booth_main.py,sha256=rG-WWqXK8rqqx4bg1ay28CXlhpnc0AtnKZEjQpBD_dA,1427
+torchx/apps/utils/copy_main.py,sha256=_O7eElApHUSpunEglh81BMiF2PBKBxOyhb8qPMSuXMs,1838
+torchx/apps/utils/process_monitor.py,sha256=9gH2Cn4191Y9dWEcNGPPWyIt_23q03LlGc3H1PG_ipk,3452
+torchx/cli/__init__.py,sha256=3lloxeC_V5KFrTL2X0-tUs7KQJ-XuIH5MuGLA-q3R10,10351
+torchx/cli/argparse_util.py,sha256=kZb1ubEHDrBsmrxpySFRQCW7wmHuRHD8eAInuEZjlsI,3836
+torchx/cli/cmd_base.py,sha256=SdqMtqi04CEqnzcgcS35DbDbsBeMxSgEhfynfpIkMGk,790
+torchx/cli/cmd_cancel.py,sha256=NKfOCu_44Lch9vliGSQ0Uv6BVqpUqj7Tob652TI-ua4,835
+torchx/cli/cmd_configure.py,sha256=1kTv0qbsbV44So74plAySwWu56pQrqjhfW_kbfdC3Rw,1722
+torchx/cli/cmd_describe.py,sha256=E5disbHoKTsqYKp2s3DaFW9GDLCCOgdOc3pQoHKoyCs,1283
+torchx/cli/cmd_list.py,sha256=BVqHEW2oTEJ3GqcFK7c1K-i2R-DUjaXQ-WBr0meeIGM,1429
+torchx/cli/cmd_log.py,sha256=Xh5vrsbwyV_ppwurrENGBNKxc1XLVbFC6YH1b8jlHAM,6104
+torchx/cli/cmd_run.py,sha256=zw-rqKI7QxYUs99vQdhvZm0L11-QXTIlvDAc_7TS2Dg,10527
+torchx/cli/cmd_runopts.py,sha256=NWZiP8XpQjfTDJgays2c6MgL_8wxFoeDge6NstaZdKk,1302
+torchx/cli/cmd_status.py,sha256=ubtmCp4PylrIh_kC3ZJ5QJm7lzXRt_aRPmY7j-sZu_0,1836
+torchx/cli/cmd_tracker.py,sha256=RfLxE4Cq1wfk7k051RtZ8RPJp0pEKSCa3KmTeRs3LF8,5218
+torchx/cli/colors.py,sha256=yLMes7e_UoLAfhxE0W6edhc58t83UHAlnCN2ANPeuXw,568
+torchx/cli/main.py,sha256=1Jf2cnO6Y2W69Adt88avmNPVrL6ZR4Hkff6GVB4293k,3484
+torchx/components/__init__.py,sha256=6Sb8RWRGObajkH7eFSKv5bHaN5bzTqJiSEmrIIo3OIc,12121
+torchx/components/component_test_base.py,sha256=22iNSdVa_qTW3SMM30Pw5UEWlK4DZVw0C03EqYiaLOI,4150
+torchx/components/dist.py,sha256=t5LKU_gm0R7N7A-_vfW5uOhwON89EjMrfEahqd2FaTY,14570
 torchx/components/interpret.py,sha256=g8gkKdDJvsBfX1ZrpVT7n2bMEtmwRV_1AqDyAnnQ_aA,697
 torchx/components/metrics.py,sha256=1gbp8BfzZWGa7PD1db5vRADlONzmae4qSBUUdCWayr0,2814
-torchx/components/serve.py,sha256=9RlpwlU2KOC7sMOZBeYwUpJIKDCXrU8xNo1SH-AT3fc,2141
-torchx/components/structured_arg.py,sha256=I8g1M-4pmR2TH9cUXTCUROjWPZ1ecd9-UA7xIA6o0qc,9554
+torchx/components/serve.py,sha256=uxIC5gU2ecg0EJIPX_oEPzNNOXRAre4j2eXusrgwGAI,2156
+torchx/components/structured_arg.py,sha256=etjZ1XRttrolvtr1bFuTi7bVsmvBAgqO0q-TeM3Etxk,9569
 torchx/components/train.py,sha256=vtrQXRcD7bIcbb3lSeyD9BBlIe1mv1WNW6rnLK9R0Mw,1259
-torchx/components/utils.py,sha256=m7mFe6du2AMHpViwcW9dF8rr_twQB6KHQuEzJyHwBXw,9025
+torchx/components/utils.py,sha256=fwqAO4lVbh1zyKSjjcE5X_kkpUVBQ0uRp_-ItZ3UyQQ,9040
 torchx/components/integration_tests/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchx/components/integration_tests/component_provider.py,sha256=FTwzCrr3p4SWYbxcjN4E6i7qjklyrWgDWL3UGkhoDaA,3980
-torchx/components/integration_tests/integ_tests.py,sha256=OVgRvGrLWhDUNlqbbYj90ukGmkAwka2KubCWUR8pC7Y,5150
+torchx/components/integration_tests/component_provider.py,sha256=cFNGqmclcZTJlOW_YGf5XEuGeWloTmcJEAh02Aob_PQ,3995
+torchx/components/integration_tests/integ_tests.py,sha256=O8jd8Jq5O0mns7xzIFsHexBDHkIIAIfELQkWCzNPzRw,5165
 torchx/distributed/__init__.py,sha256=pkB_V4eX0DnupRsmOGnINL2XGDM1oRLG0-fH7-feeNI,10280
 torchx/examples/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchx/examples/apps/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchx/examples/apps/datapreproc/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-torchx/examples/apps/datapreproc/datapreproc.py,sha256=7GV37WS3JLueID8vGFvqO93ua0PSSx__hz2MYNto53Q,4302
+torchx/examples/apps/datapreproc/datapreproc.py,sha256=cu88O_WZgqZ6g7jVIG2kagAVbJ4oPMzTH03_H65w8RU,4317
 torchx/examples/apps/lightning/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-torchx/examples/apps/lightning/data.py,sha256=uTZM_G8Wd-UjzlmeDIsl645qZl9qD7d-cesRFt0nnm4,6583
+torchx/examples/apps/lightning/data.py,sha256=Sp9mv29FlCtMdMeSKOrHan_QHK-h_crCIkCXopb20kQ,6598
 torchx/examples/apps/lightning/interpret.py,sha256=Hd3kE5a6FyhxCmJBfTzb4Tlj518zhX8V0XvZfzu4nqE,5256
-torchx/examples/apps/lightning/model.py,sha256=GQpn4SDdwYxsgD4oZLynZCqPB8emd_veB5feYFqEVZ8,3932
-torchx/examples/apps/lightning/profiler.py,sha256=SWl4deBqZVf-K8Eojoelo7P04AnUIE4ig03Tjoo3qCE,1926
-torchx/examples/apps/lightning/train.py,sha256=_MTFX0llRbgNMNoy6v7ryWjPxHZ9AM2Xz270sicMx0g,6084
+torchx/examples/apps/lightning/model.py,sha256=ppj8pYkJ1Zj4kZX6JXULGgx3sYVfNIQ24OuDYXc5lYo,3947
+torchx/examples/apps/lightning/profiler.py,sha256=z6rvcTK6oTr1m6kr1Ymmh1HpTyTyVX7naz64FtqOYDQ,1941
+torchx/examples/apps/lightning/train.py,sha256=MGSapykGv-m4nl0WyRC-yzhPWEI4bHQ5WKrniR6czQk,6099
 torchx/examples/pipelines/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchx/examples/pipelines/kfp/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-torchx/examples/pipelines/kfp/advanced_pipeline.py,sha256=bMBk3bZMl7TPGyE8I3GJHgucsUjxZqshc9W57PzyrzY,8416
-torchx/examples/pipelines/kfp/dist_pipeline.py,sha256=hMAVKuwmCTZN0c2lHkkix7gSFS5wHePH-elaWijaJa4,2178
-torchx/examples/pipelines/kfp/intro_pipeline.py,sha256=wAkk0DoGSbJRNlti7gLdaudIzWiqXC4SlqVerYcphFY,2751
+torchx/examples/pipelines/kfp/advanced_pipeline.py,sha256=U5N_XmpxbuEIh-hDayjJ5Lnk2lYvmgr7oznFnsKUk5g,8431
+torchx/examples/pipelines/kfp/dist_pipeline.py,sha256=-XOE2gFE0vcmPCdMZeKZ7MIYj15PPtL6kYGjjIoYMkY,2193
+torchx/examples/pipelines/kfp/intro_pipeline.py,sha256=P6E8TJEhuxcBZXYHo6OmiFLMNeQq-Z35uwiMa9yb0yo,2766
 torchx/pipelines/__init__.py,sha256=2MbRVk5xwRjg-d2qPemeXpEhDsocMQumPQ53lsesZAI,606
-torchx/pipelines/kfp/__init__.py,sha256=uDP4O5xrV23ncb79cNw7ifWUWJl6Qy2-d_BQyfQnTRw,721
-torchx/pipelines/kfp/adapter.py,sha256=USspiYah8BGTkPLorZi8RTFYDGZHoF67TkzheHMh2jQ,8952
-torchx/pipelines/kfp/version.py,sha256=4422bXkqH2qFTTv-Vh16Cu4EO9DrKYJTl2GpMmC9kDg,524
-torchx/runner/__init__.py,sha256=YYJP4xys6f_hKCBLQkqrCAmjhgXGlXuAVNf4ozSLu70,300
-torchx/runner/api.py,sha256=RrI89QGeBHsT52qaawYHWqe2oiE-R0OtXrzsDImO_fQ,27090
-torchx/runner/config.py,sha256=rt2rpKa6E1pv7IqtU57O7AmSI65HkQCKXAeIOit4XEw,17171
-torchx/runner/events/__init__.py,sha256=T3kz5OT0j-SlC5rzq8HO2wGrhE3yGHbIZgLhsTWzw20,4043
-torchx/runner/events/api.py,sha256=10B9Z9da0-1XHyO6J6hq91M7gos-cIVkDnuK6mC8Xkg,2132
-torchx/runner/events/handlers.py,sha256=esLkzce9fvwgjqu1IrO_furmhECUqwUeL7wy5zbqtUo,507
+torchx/pipelines/kfp/__init__.py,sha256=8iJ8lql_fxwuk9VCYSxXnX6tPL228fB5mDZpOs-kpn4,736
+torchx/pipelines/kfp/adapter.py,sha256=q9fGRS3-xOYVl-tAjTNlUFLbw22r5IiCz9IP34t1rWU,8959
+torchx/pipelines/kfp/version.py,sha256=mYBxd6bm4MeR34D--xo-JLQ9wHeAl_ZQLwbItCf9tr0,539
+torchx/runner/__init__.py,sha256=x8Sz7s_tLxPgJgvWIhK4ju9BNZU61uBFywGwDY6CqJs,315
+torchx/runner/api.py,sha256=YpANr5AyFiufB5BcexGIIeExoN5_1ANAzyVWIwz30xo,27101
+torchx/runner/config.py,sha256=pBNbe51EvsfyZvOdhAKvobq0V9lKtpnDkWUzEgnxiP4,17838
+torchx/runner/events/__init__.py,sha256=b1POchGocV9dTMnZdFVigDlLLopeID6ixSp3rBJlxVw,4058
+torchx/runner/events/api.py,sha256=vYXkkUGxiQQcViJDrMpxN9tPy-57VRt7M9q_VcGnFjM,2147
+torchx/runner/events/handlers.py,sha256=ThHCIJW21BfBgB7b6ftyjASJmD1KdizpjuTtsyqnvJs,522
 torchx/runtime/__init__.py,sha256=Wxje2BryzeQneFu5r6P9JJiEKG-_C9W1CcZ_JNrKT6g,593
-torchx/runtime/tracking/__init__.py,sha256=uHbJ1NqsxFWGYz2aV0_p4OCMhW467zDJu_86B0C1Mn8,3040
-torchx/runtime/tracking/api.py,sha256=9mlsCnnKP8hfvypNcEX2_57OYMW4AuTMM-nvsIgOzK4,5457
-torchx/schedulers/__init__.py,sha256=cCansxGU45SV_lxhgzyw2on7AJyIvhprAFo6Di1x9xQ,2157
-torchx/schedulers/api.py,sha256=XlYrD6ZjV71HotJxdVZxA_Zc8DuxhM4KKCnkibqZflU,14140
-torchx/schedulers/aws_batch_scheduler.py,sha256=zTb4uOg-U7-1ih643Nym6KynE7n2_yXaf4wnBrSd4N4,27960
-torchx/schedulers/devices.py,sha256=PNbcpf8fEM18Ag1RgK9Q30zPBalEcPdsFWctdbLxuv8,1352
-torchx/schedulers/docker_scheduler.py,sha256=I-kZN-dXoQyokLPe9ZKjfhkVX5lHx_C5jvLLc2JmXQQ,15456
-torchx/schedulers/gcp_batch_scheduler.py,sha256=mBxJbrNTUbIuYmudzyhOOcf8KAuUpxhiQMTDgJPtL8M,16549
-torchx/schedulers/ids.py,sha256=IGsJEbCYTdfKdU3MhKLQU6b7sWCJy5dlRV6JIL_9BlE,1783
+torchx/runtime/tracking/__init__.py,sha256=dYnAPnrXYREfPXkpHhdOFkcYIODWEbA13PdD-wLQYBo,3055
+torchx/runtime/tracking/api.py,sha256=SmUQyUKZqG3KlAhT7CJOGqRz1O274E4m63wQeOVq3CU,5472
+torchx/schedulers/__init__.py,sha256=M9SBZiNdE3KI_yc1-BiRtAetfTgtX07uKkuvGUeZQLU,2230
+torchx/schedulers/api.py,sha256=s2hI87uAWtU2SHMNBKjAqelzQU_GKp_BjcxdtjVVDDk,14155
+torchx/schedulers/aws_batch_scheduler.py,sha256=7qxy3UFRq0F731-kTjEi6VABWKD60o0req6CBMsTohU,27975
+torchx/schedulers/aws_sagemaker_scheduler.py,sha256=dPah3yaKFUVm-ZZrzFbyM_abP-LCTd-AcAjZ6t2iycU,20699
+torchx/schedulers/devices.py,sha256=BnjZnbXGTWiZKLmMKE3zPDOkb4Vao3jgPVa01aV6vyY,1367
+torchx/schedulers/docker_scheduler.py,sha256=eXw2WtPPueY-eaquw_KkEb_CFGrTXaUHuV116UYg7T4,15973
+torchx/schedulers/gcp_batch_scheduler.py,sha256=dlUfvjfMuQiRcSXQAdwxqdadwPhOf82L5u-ejRWtFgE,16226
+torchx/schedulers/ids.py,sha256=3E-_vwVYC-8Tv8kjuY9-W7TbOe_-Laqd8a65uIN3hQY,1798
 torchx/schedulers/kubernetes_mcad_scheduler.py,sha256=xAt-on3K8HwS2kzWasn0zXd2q4IDQzo2N5A5Ehh9NII,42885
-torchx/schedulers/kubernetes_scheduler.py,sha256=6NXYJwiYCXNeB3ubr8t4q_SuAa-vlYdiCAPXTB3f-zg,27068
-torchx/schedulers/local_scheduler.py,sha256=_fnInhqIdh0xiSpfgT0172zM0-wmqJSMRQo9baT54Uo,40928
-torchx/schedulers/lsf_scheduler.py,sha256=KM4-LSBiTYdtP1Js8F9dSjAdNwimaTaLraZmgnZiRuI,17638
+torchx/schedulers/kubernetes_scheduler.py,sha256=qubhzfBAOFag6Yi_oAgdq1Jxr56LG68cioTre3qggEM,27065
+torchx/schedulers/local_scheduler.py,sha256=uMrRQKjhgMaZr2rnxaEvxBxE1GjpUTgxL-kNfaxnGsI,41341
+torchx/schedulers/lsf_scheduler.py,sha256=FhpI8KgYKySz6xMTLmy0IcesJk2SpJ_no-iRzpqV0Wk,17653
 torchx/schedulers/ray_scheduler.py,sha256=unnDtDu1rPpCLJLDcm4NYRo9ZCCtQgG5BtlHwVfly-U,17448
-torchx/schedulers/slurm_scheduler.py,sha256=rMlknOk0YW4kB4c4VCoTOy30fOYe0dOoW1CrpXxJv_I,19355
-torchx/schedulers/streams.py,sha256=ObaKwEEcnsjrPyc6VZOp8cgZ_f2RFextAxeISxZUWeQ,1992
+torchx/schedulers/slurm_scheduler.py,sha256=_57XhN9ggFaD6ZG9cPXkAlcHoF-R5mPmAvqPwhNJ4qw,19370
+torchx/schedulers/streams.py,sha256=8_SLezgnWgfv_zXUsJCUM34-h2dtv25NmZuxEwkzmxw,2007
 torchx/schedulers/ray/__init__.py,sha256=fE0IHi1JJpxsNVBNzWNee2thrNXFFRhY94c80RxNSIE,231
 torchx/schedulers/ray/ray_common.py,sha256=pyNYFvTKVwdjDAeCBNbPwAWwVNmlLOJWExfn90XY8u8,610
-torchx/schedulers/ray/ray_driver.py,sha256=0DL8Ad_hire-WgH8ZEYx1Q-mI2SUfZDk-6_6PICk8OQ,12282
-torchx/specs/__init__.py,sha256=fSA89Y0ZpdZLJmhIfEKNbjNNi6fbDR9k1bpIM7Xm7xo,5462
-torchx/specs/api.py,sha256=6Y9QkpuC8rFW7qiwVwrprd9s-z710aZzXqxYn2S2VEc,34256
-torchx/specs/builders.py,sha256=MJKci3zUKEC3WHh13MqZpmhdib39Bj6BK3aUvWQ-a2w,8526
-torchx/specs/file_linter.py,sha256=LREWELpHJyE7YN3rc5ixf2ZydWFU9dlcSy5gGqdB5rA,11714
-torchx/specs/finder.py,sha256=RJI0PkG69esuVzhCp4w6Lotu2tSzIRh6PhWemCSQR7I,16234
-torchx/specs/named_resources_aws.py,sha256=lHIOAdETkQEvOIB0QV2nMbLT99I-K5d-yiXGPJzhDJ0,7728
-torchx/specs/named_resources_generic.py,sha256=_xz0cRjy3fz-CVtX9G_MY7f3NX6n3AkP3xzAkuDevwk,2631
-torchx/specs/test/components/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchx/specs/test/components/a/__init__.py,sha256=T7exlQ47Fak5ajCEGPg6_yOfChJCWpIMhWBmSVUnlrQ,546
-torchx/specs/test/components/a/b/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchx/specs/test/components/a/b/c.py,sha256=QyTZfsCaSZscmk3DeNOkAyMoz6GCcayrWtOKbNFIZ1M,539
-torchx/specs/test/components/c/__init__.py,sha256=fE0IHi1JJpxsNVBNzWNee2thrNXFFRhY94c80RxNSIE,231
-torchx/specs/test/components/c/d.py,sha256=RH07jjo6uvFbzIaNFnAwmD_h24cEsT8kyZDTN-ezFio,531
-torchx/tracker/__init__.py,sha256=3uUMRTQbQ-EcJrAofn3YiYJ00QwzbK4j6bJdgX8_D-k,4350
-torchx/tracker/api.py,sha256=1K7X5mtkoEonICOEXCpPHyefcnILI_VZhqDpPb-yOmc,11254
+torchx/schedulers/ray/ray_driver.py,sha256=Wl-1jldL8veVKzmYDEeR2va3JSlAjZpFE1h8HWE9YVE,12286
+torchx/specs/__init__.py,sha256=vF-WUu_4NZP30lCtNYg0YVenY6wRQ8k7K36fOxqbOKc,5477
+torchx/specs/api.py,sha256=_euy2mCO-P7qQSxt_476GlePXEa_7H4BAzJQHX3fANY,34736
+torchx/specs/builders.py,sha256=UqNvVrJ3ZMbvv6qcfp1ZNxBpjcwQSvle29EVw1TMzUM,10178
+torchx/specs/file_linter.py,sha256=IeiomB1BgHUlT-ZsvGxar3llY63NOupfLBrOrD_---A,11860
+torchx/specs/finder.py,sha256=MnwxG_UC4a-3X2wQ37ANEQR6D1TvriCLyuVYBh_-wuI,16249
+torchx/specs/named_resources_aws.py,sha256=8jsbFZFTa1ZkdpZmBrqhVfSVuZonrXbADhTPmIRdtEQ,7743
+torchx/specs/named_resources_generic.py,sha256=Sg4tAdqiiWDrDz2Lj_pnfsjzGIXKTou73wPseh6j55w,2646
+torchx/specs/test/components/__init__.py,sha256=J8qjUOysmcMAek2KFN13mViOXZxTYc5vCrF02t3VuFU,223
+torchx/specs/test/components/a/__init__.py,sha256=kdxEgnI8QBSBiuTjaB4qDD7JX84hWowyPWU4B2Cqe9A,561
+torchx/specs/test/components/a/b/__init__.py,sha256=J8qjUOysmcMAek2KFN13mViOXZxTYc5vCrF02t3VuFU,223
+torchx/specs/test/components/a/b/c.py,sha256=FhixafzNqpS5zvggtWIWLxRd6HIxsOmct-d1Hs-rDoc,554
+torchx/specs/test/components/c/__init__.py,sha256=5CBMckkpqJUdxBQBYHGSsItqq1gj2V0UiCw02Qfq6MM,246
+torchx/specs/test/components/c/d.py,sha256=2AjE-FmQXJTw3hws66O83ToQPmjOEZLDf-jDAKrrUkQ,546
+torchx/tracker/__init__.py,sha256=u4Fjw4pf49ZLbzQ8mTj5p3Dp_gvnJKxNIwWY7WszZ8E,4365
+torchx/tracker/api.py,sha256=tQk8o8naMfmQVug0BBDiiVz_9SWVRdJpgyUjwytsFYE,11257
 torchx/tracker/mlflow.py,sha256=poeoIXVPzr2sxgi515fMGRH83KAFNL6XFILMh0EQ2Dw,14487
 torchx/tracker/backend/__init__.py,sha256=fE0IHi1JJpxsNVBNzWNee2thrNXFFRhY94c80RxNSIE,231
-torchx/tracker/backend/fsspec.py,sha256=JpSioMgn54mrxqqpY0kw5Gudqx9hhxkgDLaOFSEP2Ko,10425
+torchx/tracker/backend/fsspec.py,sha256=528xKryBE27Rm_OHD7r2R6fmVAclknBtoy1s034Ny6c,10440
 torchx/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-torchx/util/cuda.py,sha256=GiAtP9-T4zJxwHl7r5qGQRuINb59Mzwz8tnzB1MVY74,1084
-torchx/util/datetime.py,sha256=e-sO5Wjx1Utpln14C3qfJHl4v3KM-SMnn11hSyvkqFY,390
-torchx/util/entrypoints.py,sha256=C4A7cF1tPLlfyYWyZ7uZEtsKeuoOoLbMv0sOSxLhXs4,2710
-torchx/util/io.py,sha256=sxb6KI42Lq6n5z6_-YKW_mAhgPdC6CxzexlMyGheWSc,1792
-torchx/util/modules.py,sha256=PjAvkC199EYEQCRIM-Fmrb1DRnySNcnV3xWrfKtqaqQ,1116
-torchx/util/shlex.py,sha256=KzyWektMeU3oXS3Z5mFkNSPLItBTszVcvQ3EYfOMUYA,448
-torchx/util/strings.py,sha256=7CZe5WKHa7IQ6DuJCYeJ5FapUC4Fd1OGeq1yZAmjluw,663
-torchx/util/types.py,sha256=6ASuDKGO91UU3DCSuWhPX_C03341tApLCQEByUz8xpY,7016
-torchx/workspace/__init__.py,sha256=KbGEzJqqXaIxALm_EQO64aw-fE7MeDMFXcpU1mY650I,783
-torchx/workspace/api.py,sha256=Ej6DR__mNWaVyZgoVNAAOloDy1kTD5X1jz7pRtoVf80,5464
-torchx/workspace/dir_workspace.py,sha256=Fz-hKIx0KN8iJf2BsthNj0NvTkWlxP6WFsElPs_BaT0,2253
-torchx/workspace/docker_workspace.py,sha256=cqvxHTtrVza0gkoImbzeRiroWkCjdBXRVgONvaulS0g,9410
-torchx_nightly-2024.2.9.dist-info/LICENSE,sha256=WVHfXhFC0Ia8LTKt_nJVYobdqTJVg_4J3Crrfm2A8KQ,1721
-torchx_nightly-2024.2.9.dist-info/METADATA,sha256=nPIYvZGg5u_GEMJ3zvfpR3K5ZEv1rBt_ziccWHc5Y68,5610
-torchx_nightly-2024.2.9.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-torchx_nightly-2024.2.9.dist-info/entry_points.txt,sha256=3JYZFlX9aWzR-Gs_qsx1zq7mlqbFz6Mi9rQUULW8caI,170
-torchx_nightly-2024.2.9.dist-info/top_level.txt,sha256=pxew3bc2gsiViS0zADs0jb6kC5v8o_Yy_85fhHj_J1A,7
-torchx_nightly-2024.2.9.dist-info/RECORD,,
+torchx/util/cuda.py,sha256=-ZTa1WCLnY2WtSWAdWufLQqZSDCZfZsloBuiS84LIkU,1099
+torchx/util/datetime.py,sha256=hV6Sg0u5KTBe68yrmy_RGCC5su0i4Tb_mAYphWamiXI,405
+torchx/util/entrypoints.py,sha256=4rqmA81XYLj4Kk7GboJi0z78h4NIQxSrcOzDuuTwCkw,2725
+torchx/util/io.py,sha256=HNpWLcFUX0WTAP3CsdamHz--FR5A4kSdLCPfNqa2UkA,1807
+torchx/util/modules.py,sha256=LRTuZRH5bbRr0ZaCtCtvKbgwhMoPsTx-GokWbCLGPdk,1131
+torchx/util/shlex.py,sha256=eXEKu8KC3zIcd8tEy9_s8Ds5oma8BORr-0VGWNpG2dk,463
+torchx/util/strings.py,sha256=GkLWCmYS89Uv6bWc5hH0XwvHy7oQmprv2U7axC4A2e8,678
+torchx/util/types.py,sha256=ynRZWRhmyLboRmz6XcCcu8617YhMh63NPC_6ysysyx8,7379
+torchx/workspace/__init__.py,sha256=FqN8AN4VhR1C_SBY10MggQvNZmyanbbuPuE-JCjkyUY,798
+torchx/workspace/api.py,sha256=1heBmPgB-W5Zf9gwViM7NrqvHpZlVYeMN7jpY8Qkytc,5479
+torchx/workspace/dir_workspace.py,sha256=npNW_IjUZm_yS5r-8hrRkH46ndDd9a_eApT64m1S1T4,2268
+torchx/workspace/docker_workspace.py,sha256=-utUxu1DFVJ5nAXTERDxz4Rr8CzdKO7Tkqfzap1HvHo,9425
+torchx_nightly-2024.4.10.dist-info/LICENSE,sha256=WVHfXhFC0Ia8LTKt_nJVYobdqTJVg_4J3Crrfm2A8KQ,1721
+torchx_nightly-2024.4.10.dist-info/METADATA,sha256=frf8i0_lOSv7Qu9rjsPJQ-qr6MoVSSZnn149ByB7OM4,6053
+torchx_nightly-2024.4.10.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+torchx_nightly-2024.4.10.dist-info/entry_points.txt,sha256=3JYZFlX9aWzR-Gs_qsx1zq7mlqbFz6Mi9rQUULW8caI,170
+torchx_nightly-2024.4.10.dist-info/top_level.txt,sha256=pxew3bc2gsiViS0zADs0jb6kC5v8o_Yy_85fhHj_J1A,7
+torchx_nightly-2024.4.10.dist-info/RECORD,,
```

